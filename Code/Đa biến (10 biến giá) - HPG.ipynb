{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Multivariate-3-RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0' # ƒë·∫£m b·∫£o r·∫±ng c√°c gi√° tr·ªã bƒÉm c·ªßa ƒë·ªëi t∆∞·ª£ng b·∫•t bi·∫øn (dict, set, chu·ªói, tuple...) lu√¥n gi·ªëng nhau gi·ªØa c√°c l·∫ßn ch·∫°y\n",
    "\n",
    "import random as rn\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "rn.seed(3)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, Dropout\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from math import sqrt\n",
    "import datetime as dt\n",
    "import time\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H√†m callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=80,  verbose=1, mode='min')  \n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=\"10Var-hpg-rnn.h5\",   # T√™n file l∆∞u m√¥ h√¨nh\n",
    "    monitor=\"val_loss\",         # Theo d√µi val_loss\n",
    "    save_best_only=True,        # Ch·ªâ l∆∞u khi t·ªët h∆°n m√¥ h√¨nh tr∆∞·ªõc ƒë√≥\n",
    "    mode=\"min\",                 # Gi·∫£m min c·ªßa val_loss l√† t·ªët nh·∫•t\n",
    "    verbose=1\n",
    ")\n",
    "callbacks_list = [earlystop, checkpoint] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ƒê·ªçc d·ªØ li·ªáu t·ª´ file csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = r\"hpg_history.csv\"\n",
    "df = pd.read_csv(url, parse_dates= True, index_col= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of              open   high    low  close    volume\n",
      "time                                            \n",
      "2007-11-16   2.29   2.29   2.29   2.29    248510\n",
      "2007-11-19   2.17   2.17   2.17   2.17    120480\n",
      "2007-11-20   2.08   2.08   2.08   2.08     58710\n",
      "2007-11-21   1.99   2.16   1.99   2.16    728080\n",
      "2007-11-22   2.16   2.16   2.08   2.16    266040\n",
      "...           ...    ...    ...    ...       ...\n",
      "2025-03-14  27.70  27.85  27.55  27.55  18279900\n",
      "2025-03-17  27.90  28.15  27.75  27.85  19719700\n",
      "2025-03-18  27.90  28.05  27.60  27.60  18741700\n",
      "2025-03-19  27.60  27.60  27.10  27.25  37925600\n",
      "2025-03-20  27.30  27.45  27.05  27.15  25022500\n",
      "\n",
      "[4322 rows x 5 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open      0\n",
      "high      0\n",
      "low       0\n",
      "close     0\n",
      "volume    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ti·ªÅn x·ª≠ l√≠ d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X√≥a c√°c d√≤ng c√≥ gi√° tr·ªã Volume b·∫±ng 0\n",
    "df.drop(df[df['volume']==0].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "open      0.999667\n",
       "high      0.999857\n",
       "low       0.999838\n",
       "close     1.000000\n",
       "volume    0.799299\n",
       "Name: close, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ma tr·∫≠n t∆∞∆°ng quan (·ªü ƒë√¢y l√† Pearson t∆∞∆°ng quan tuy·∫øn t√≠nh)\n",
    "df.corr()['close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    4.320000e+03\n",
      "mean     7.877410e+06\n",
      "std      1.133795e+07\n",
      "min      5.000000e+01\n",
      "25%      4.646300e+05\n",
      "50%      2.422785e+06\n",
      "75%      1.233620e+07\n",
      "max      9.967998e+07\n",
      "Name: volume, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#  T√≠nh to√°n c√°c th·ªëng k√™ m√¥ t·∫£ nh∆∞ trung b√¨nh (mean), ƒë·ªô l·ªách chu·∫©n (std), min, max, ph·∫ßn trƒÉm ph√¢n v·ªã (25%, 50%, 75%).\n",
    "print(df.describe().volume) # Gi√∫p ki·ªÉm tra ph√¢n b·ªë c·ªßa Volume, ph√°t hi·ªán c√°c gi√° tr·ªã b·∫•t th∆∞·ªùng (outliers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAGvCAYAAABFKe9kAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAKUlJREFUeJzt3Qt0FPX5//EnJAECyD0Uwv0WoRIgisBBWhAVUOIFsaJAoWKoCEVaSlta0IqCEBSrFigWYgFRUVNTEVCo4h1aK5RbUAIETLiVUEOsXBPI/zzf33+2CQabwGbCk32/ztmzmZ3ZYfZhs/ns9zITVlBQUCAAAACGVCrvAwAAACgtAgwAADCHAAMAAMwhwAAAAHMIMAAAwBwCDAAAMIcAAwAAzCHAAAAAcwgwAADAHAIMAAAwJ0IquJycHMnPzw/qPqOjoyU7Ozuo+0TxqLU/qLN/qLU/qLPdWkdEREidOnX+93ZSwWl4ycvLC9r+wsLCAvvlMlJli1r7gzr7h1r7gzqHRq3pQgIAAOYQYAAAgDkEGAAAYA4BBgAAmEOAAQAA5hBgAACAOQQYAABgDgEGAACYQ4ABAADmEGAAAIA5BBgAAGAOAQYAAJhDgAEAAOYQYAAAgDkR5X0AFmUN6CLWhC9YXt6HAABA0NACAwAAzCHAAAAAcwgwAADAHAIMAAAwhwADAADMIcAAAABzCDAAAKBinwdmzZo17padne2WmzRpInfccYfEx8e75dOnT8uSJUtk3bp1kpeXJ506dZLExESpXbt2YB9HjhyRBQsWSFpamlStWlV69eolQ4YMkfDw8MA2uk73k5WVJfXq1ZNBgwZJ7969g/eqAQBA6ASYunXrurDRqFEjKSgokPfff19mzZrlbk2bNpXFixfLxo0bZcKECVKtWjVJTk6W2bNny6OPPuqef/bsWZkxY4YLNNOmTZOcnByZM2eOCy+6X3X48GGZOXOm3HDDDTJu3DjZtm2bzJ8/3z2nc+fOZVMFAABQcbuQunTpIldeeaULMDExMXL33Xe7VpSdO3fK8ePHZe3atTJixAjp0KGDtGrVSsaMGSM7duyQ9PR09/zNmzfLvn37XDBp0aKFa7kZPHiwrF69WvLz89022sLToEEDGT58uGvh6d+/v3Tv3l1WrlxZNhUAAAChcykBbU1Zv369nDp1SmJjYyUjI0POnDkjcXFxgW0aN24s9evXdwFGt9H7Zs2aFelS0laVhQsXuu6ili1bujBUeB9Ku6IWLVr0rcejXVZ684SFhUlUVFTg52AJ5r78ZPG4vWO2eOyWUGf/UGt/UOfQqHWpA0xmZqZMnjzZhQVtfZk4caJrKdm7d69ERERI9erVi2xfq1YtOXr0qPtZ7wuHF2+9t8679x4rvM2JEyfcGJvKlSsXe1ypqamSkpISWNYwlJSUJNHR0RJsWWKPtppZ1bBhw/I+hJBAnf1Drf1BnSt2rUsdYLTr6PHHH3ddRn/7299k7ty5MnXqVClvAwcOlISEhMCylwZ1wLHXPRUMVhP9wYMHxRqttf5SHDp0yI25Qtmgzv6h1v6gzrZrrY0hJWl8iLiQHXtJS8e57N69W1atWiU9evRwQeHYsWNFWmFyc3MDrS56v2vXriL70/XeOu/ee6zwNtoddL7WFxUZGeluxeENbLsGeuyWj98K6uwfau0P6lyxa33R54HRsTDanaRhRmcTbd26NbDuwIEDbtq0jn9Req9dUIUDypYtW1w40W4o1bZt2yL78Lbx9gEAAFCqAPPiiy/K9u3b3VRnDSLe8ve+9z03bbpPnz7u/C069VkH9c6bN88FDy986GBcDSo6dVrHzGzatEmWLVsm/fr1C7Se9O3b1+1/6dKlsn//fjdDSQcLDxgwoGwqAAAAzClVF5K2nOiYFz1/iwaW5s2buwG9HTt2dOt1CrX2h+m5X7Q7yTuRnadSpUoyadIkN+toypQpUqVKFXciO51K7dEp1LqNnlNGu6b0RHajR4/mHDAAACAgrKCCdxDqIN7C06svlga0/MSbxZrwBcvFGq21zp7SAcgV/G1arqizf6i1P6iz7Vprj0xJBvFyLSQAAGAOAQYAAJhDgAEAAOYQYAAAgDkEGAAAYA4BBgAAmEOAAQAA5hBgAACAOQQYAABgDgEGAACYQ4ABAADmEGAAAIA5BBgAAGAOAQYAAJhDgAEAAOYQYAAAgDkEGAAAYA4BBgAAmEOAAQAA5hBgAACAOQQYAABgDgEGAACYQ4ABAADmEGAAAIA5BBgAAGAOAQYAAJhDgAEAAOYQYAAAgDkEGAAAYA4BBgAAmEOAAQAA5hBgAACAOQQYAABgDgEGAACYQ4ABAADmEGAAAIA5BBgAAGAOAQYAAJhDgAEAAOYQYAAAgDkEGAAAYA4BBgAAmEOAAQAA5hBgAACAOQQYAABgDgEGAACYE1GajVNTU+WTTz6R/fv3S+XKlSU2NlaGDRsmMTExgW0efvhh2b59e5HnXX/99fLjH/84sHzkyBFZsGCBpKWlSdWqVaVXr14yZMgQCQ8PD2yj65YsWSJZWVlSr149GTRokPTu3fviXi0AAAi9AKPBpF+/ftK6dWs5c+aMvPTSSzJt2jR58sknXRDxXHfddTJ48ODAsoYdz9mzZ2XGjBlSu3Zt99ycnByZM2eOCy8aYtThw4dl5syZcsMNN8i4ceNk27ZtMn/+fPeczp07B+eVAwCA0OhCmjx5smsFadq0qbRo0ULGjh3rWlMyMjKKbFelShUXNrxbtWrVAus2b94s+/btc8FE9xEfH+/CzurVqyU/P99ts2bNGmnQoIEMHz5cmjRpIv3795fu3bvLypUrg/W6AQBAqLTAnOv48ePuvkaNGkUe//DDD91Nw8tVV13lun801Kj09HRp1qyZW+fRVpWFCxe67qKWLVvKzp07JS4ursg+O3XqJIsWLTrvseTl5bmbJywsTKKiogI/B0sw9+Uni8ftHbPFY7eEOvuHWvuDOodGrS84wGhXkAaKyy+/3AUST8+ePaV+/fpSt25d+eKLL+SFF16QAwcOyMSJE936o0ePFgkvqlatWoF13r33WOFtTpw4IadPny7SJVV4fE5KSkpgWYNQUlKSREdHS7BliT2NGjUSqxo2bFjehxASqLN/qLU/qHPFrvUFB5jk5GTXYvLII498Y8CuR4NNnTp13DaHDh0q0xc4cOBASUhICCx7aTA7OzvQNRUMVhP9wYMHxRqttb5n9L1TUFBQ3odTYVFn/1Brf1Bn27WOiIgoUeNDxIWGl40bN8rUqVPdDKFv06ZNG3fvBRhtfdm1a1eRbXJzc9291zKj995jhbfRLqHiWl9UZGSkuxWHN7DtGuixWz5+K6izf6i1P6hzxa51qQbx6sFpeNGp1A899JAbaPu/7N27191rS4zSqdeZmZlFAsqWLVtcONEBu6pt27aydevWIvvRbfS5AAAApQowGl50cO748eNd4NCxKnrTcSleK4uOQ9FZSToV+tNPP5W5c+dK+/btpXnz5oHBuBpUdOq0hptNmzbJsmXL3PRsrwWlb9++7vlLly5155zRGUrr16+XAQMGlEUNAACAMaXqQtLpzd7J6gobM2aMm16t/VbacrJq1So5deqU617q1q2b3H777YFtK1WqJJMmTXKzjqZMmeJmJ+mJ7AqfN0ZbdnSbxYsXu33pfkaPHs05YAAAgBNWUME7CHUQb+Hp1cEYsJSfeLNYE75geXkfwgXVWmdP6QDkCv42LVfU2T/U2h/U2XattTemJIN4uRYSAAAwhwADAADMIcAAAABzCDAAAMAcAgwAADCHAAMAAMwhwAAAAHMIMAAAwBwCDAAAMIcAAwAAzCHAAAAAcwgwAADAHAIMAAAwhwADAADMIcAAAABzCDAAAMAcAgwAADCHAAMAAMwhwAAAAHMIMAAAwBwCDAAAMIcAAwAAzCHAAAAAcwgwAADAHAIMAAAwhwADAADMIcAAAABzCDAAAMAcAgwAADCHAAMAAMwhwAAAAHMIMAAAwBwCDAAAMIcAAwAAzCHAAAAAcwgwAADAHAIMAAAwhwADAADMIcAAAABzCDAAAMAcAgwAADCHAAMAAMwhwAAAAHMIMAAAwBwCDAAAMCeiNBunpqbKJ598Ivv375fKlStLbGysDBs2TGJiYgLbnD59WpYsWSLr1q2TvLw86dSpkyQmJkrt2rUD2xw5ckQWLFggaWlpUrVqVenVq5cMGTJEwsPDA9voOt1PVlaW1KtXTwYNGiS9e/cO1usGAACh0gKzfft26devn0yfPl2mTJkiZ86ckWnTpsnJkycD2yxevFg2bNggEyZMkKlTp0pOTo7Mnj07sP7s2bMyY8YMyc/Pd88dO3asvPfee/Lyyy8Htjl8+LDMnDlTrrjiCpk1a5YMGDBA5s+fL5s2bQrW6wYAAKESYCZPnuxaQZo2bSotWrRw4UNbUzIyMtz648ePy9q1a2XEiBHSoUMHadWqlYwZM0Z27Ngh6enpbpvNmzfLvn37ZNy4cW4f8fHxMnjwYFm9erULNWrNmjXSoEEDGT58uDRp0kT69+8v3bt3l5UrV5ZFDQAAQEXuQjqXBhZVo0YNd69BRltl4uLiAts0btxY6tev7wKMdjnpfbNmzYp0KXXu3FkWLlzouotatmwpO3fuLLIPpV1RixYtOu+xaHeV3jxhYWESFRUV+DlYgrkvP1k8bu+YLR67JdTZP9TaH9Q5NGp9wQFGu4I0UFx++eUukKijR49KRESEVK9evci2tWrVcuu8bQqHF2+9t8679x4rvM2JEyfcGBsdf1Pc+JyUlJTAsgahpKQkiY6OlmDLEnsaNWokVjVs2LC8DyEkUGf/UGt/UOeKXesLDjDJycmuxeSRRx6RS8HAgQMlISEhsOylwezs7EDXVDBYTfQHDx4Ua7TW+ktx6NAhKSgoKO/DqbCos3+otT+os+1aa0NISRofIi40vGzcuNEN0tUZQh5tWdGwcOzYsSKtMLm5uYFWF73ftWtXkf3pem+dd+89Vngb7RIqrvVFRUZGultxeAPbroEeu+Xjt4I6+4da+4M6V+xal2oQrx6chhedSv3QQw+5gbaF6aBdnQq9devWwGMHDhxwA311/IvS+8zMzCIBZcuWLS6c6IBd1bZt2yL78Lbx9gEAAEJbqQKMhpcPP/xQxo8f7wKHjlXRm45LUdWqVZM+ffq487ds27bNDeqdN2+eCx5e+NDBuBpU5syZI3v37nVTo5ctW+amZ3stKH379nVTqZcuXerOOaMzlNavX++mUwMAAIQVlKLN58477yz2cZ0q7Z1kzjuR3ccff+y6k4o7kZ2OS9FZR3qyuipVqrgT2Q0dOvQbJ7LTc8rolOuLOZGd/luFZycFo78vP/FmsSZ8wXKxRmutg491/A7NwGWHOvuHWvuDOtuutTZmlGQMTKkCjEUEmP9DgMH5UGf/UGt/UOfQCDBcCwkAAJhDgAEAAOYQYAAAgDkEGAAAYA4BBgAAmEOAAQAA5hBgAACAOQQYAABgDgEGAACYQ4ABAADmEGAAAIA5BBgAAGAOAQYAAJhDgAEAAOYQYAAAgDkEGAAAYA4BBgAAmEOAAQAA5hBgAACAOQQYAABgDgEGAACYQ4ABAADmEGAAAIA5BBgAAGAOAQYAAJhDgAEAAOYQYAAAgDkEGAAAYA4BBgAAmEOAAQAA5hBgAACAOQQYAABgDgEGAACYQ4ABAADmEGAAAIA5BBgAAGAOAQYAAJhDgAEAAOYQYAAAgDkEGAAAYA4BBgAAmEOAAQAA5hBgAACAOQQYAABgDgEGAACYQ4ABAADmRJT2Cdu3b5fly5fLnj17JCcnRyZOnChdu3YNrJ87d668//77RZ7TqVMnmTx5cmD566+/lueee042bNggYWFh0q1bN7nnnnukatWqgW2++OILSU5Olt27d0vNmjWlf//+cuutt174KwUAAKEbYE6dOiUtWrSQPn36yBNPPFHsNp07d5YxY8b89x+JKPrPPPPMMy78TJkyRc6cOSPz5s2TZ599VsaPH+/WHz9+XKZNmyZxcXEyatQoyczMlD/84Q9SvXp1uf7660v/KgEAQGgHmPj4eHf71p1GREjt2rWLXbdv3z7ZtGmTzJgxQ1q3bu0eGzlypFv+4Q9/KHXr1pWPPvpI8vPzXQjSfTVt2lT27t0rK1asIMAAAIDSB5iSdjMlJia6FpMOHTrIXXfdJZdddplbl56e7h73wovSlhbtStq1a5frjtJt2rdvX6TlRruhXn/9ddf9VKNGjW/8m3l5ee7m0f1FRUUFfg6WYO7LTxaP2ztmi8duCXX2D7X2B3UOjVoHPcBo95GOaWnQoIEcOnRIXnrpJXnsscdk+vTpUqlSJTl69Kgb01JYeHi4CyW6Tum9Pr8wr0VH1xUXYFJTUyUlJSWw3LJlS0lKSpLo6Ohgv0TJEnsaNWokVjVs2LC8DyEkUGf/UGt/UOeKXeugB5hrrrkm8HOzZs2kefPmMm7cOElLS3MtLWVl4MCBkpCQEFj20mB2drbrjgoWq4n+4MGDYo3WWn8pNAgXFBSU9+FUWNTZP9TaH9TZdq2196UkjQ9l0oVU2He+8x3XfaQvTgOMtqR89dVXRbbRgbzaNeS1sui91xrj8ZbPN7YmMjLS3YrDG9h2DfTYLR+/FdTZP9TaH9S5Yte6zM8D8+9//9uFkzp16rjl2NhYOXbsmGRkZAS22bZtm3vhbdq0CWzz2WefFWk52bJli8TExBTbfQQAAEJLqQPMyZMn3YwgvanDhw+7n48cOeLWPf/8824Qrj6+detWmTVrlmte0kG4qkmTJm6cjE6b1kG7n3/+uTsnTI8ePdwMJNWzZ0/XhDR//nzJysqSdevWyZtvvlmkiwgAAISuUnch6Ynlpk6dGlhesmSJu+/Vq1fgnC16IjttZdFA0rFjRxk8eHCR7p0HHnjAnaTukUceCZzITqdSe6pVq+bOEaPbTJo0yXVBDRo0iCnUAADACSuo4B2EOoi38PTqi6WBKz/xZrEmfMFysUZrrbOndAByBX+blivq7B9q7Q/qbLvW2uBRkkG8XAsJAACYQ4ABAADmEGAAAIA5BBgAAGAOAQYAAJhDgAEAAOYQYAAAgDkEGAAAYA4BBgAAmEOAAQAA5hBgAACAOQQYAABgDgEGAACYQ4ABAADmEGAAAIA5BBgAAGAOAQYAAJhDgAEAAOYQYAAAgDkEGAAAYA4BBgAAmEOAAQAA5hBgAACAOQQYAABgDgEGAACYQ4ABAADmEGAAAIA5BBgAAGAOAQYAAJhDgAEAAOYQYAAAgDkEGAAAYA4BBgAAmEOAAQAA5hBgAACAOQQYAABgDgEGAACYQ4ABAADmEGAAAIA5BBgAAGAOAQYAAJhDgAEAAOYQYAAAgDkEGAAAYA4BBgAAmEOAAQAA5kSU9gnbt2+X5cuXy549eyQnJ0cmTpwoXbt2DawvKCiQV155Rd555x05duyYtGvXThITE6VRo0aBbb7++mt57rnnZMOGDRIWFibdunWTe+65R6pWrRrY5osvvpDk5GTZvXu31KxZU/r37y+33nprMF4zAAAItRaYU6dOSYsWLeTee+8tdv3rr78ub775powaNUoee+wxqVKlikyfPl1Onz4d2OaZZ56RrKwsmTJlikyaNEk+++wzefbZZwPrjx8/LtOmTZP69evLzJkzZdiwYfLqq6/K22+/faGvEwAAhHKAiY+Pl7vuuqtIq0vh1pdVq1bJ7bffLldffbU0b95cfvKTn7iWmn/84x9um3379smmTZtk9OjR0rZtW9dCM3LkSFm3bp18+eWXbpuPPvpI8vPzZcyYMdK0aVO55ppr5MYbb5QVK1YE4zUDAIBQ60L6NocPH5ajR49Kx44dA49Vq1ZN2rRpI+np6S6I6H316tWldevWgW3i4uJcV9KuXbtcMNJt2rdvLxER/z28Tp06udYd7X6qUaPGN/7tvLw8d/Po/qKiogI/B0sw9+Uni8ftHbPFY7eEOvuHWvuDOodGrYMaYDS8qFq1ahV5XJe9dXqvY1oKCw8Pd6Gk8DYNGjQosk3t2rUD64oLMKmpqZKSkhJYbtmypSQlJUl0dLQEW5bYU3gMkjUNGzYs70MICdTZP9TaH9S5Ytc6qAGmPA0cOFASEhICy14azM7Odt1RwWI10R88eFCs0VrrL8WhQ4dc9yTKBnX2D7X2B3W2XWvtfSlJ40NQA4zXSpKbmyt16tQJPK7LOvDX2+arr74q8rwzZ864riHv+XrvtcZ4vGVvm3NFRka6W3F4A9uugR675eO3gjr7h1r7gzpX7FoH9Tww2u2jAWPr1q1FZhTp2JbY2Fi3rPc6vTojIyOwzbZt29wL17Ey3jY6M6lwy8mWLVskJiam2O4jAAAQWkodYE6ePCl79+51N2/grv585MgR15R00003yWuvvSaffvqpZGZmypw5c1xrjM5KUk2aNJHOnTu7adMabD7//HN3TpgePXpI3bp13TY9e/Z0TUjz58930611hpJOzS7cRQQAAEJXWEEp23zS0tJk6tSp33i8V69eMnbs2MCJ7PScLdr6otOk9Zwx2nri0e4iPUld4RPZ6VTq853I7rLLLnMnsrvttttK/QJ1DEzh2UkXS483P/FmsSZ8wXKxRmutg491/A7NwGWHOvuHWvuDOtuutQ4HKckYmFIHGGsIMP+HAIPzoc7+odb+oM6hEWC4FhIAADCHAAMAAMwhwAAAAHMIMAAAwBwCDAAAMIcAAwAAzCHAAAAAcwgwAADAHAIMAAAwhwADAADMIcAAAABzCDAAAMAcAgwAADCHAAMAAMwhwAAAAHMIMAAAwBwCDAAAMIcAAwAAzCHAAAAAcwgwAADAHAIMAAAwhwADAADMIcAAAABzCDAAAMAcAgwAADCHAAMAAMwhwAAAAHMIMAAAwBwCDAAAMIcAAwAAzCHAAAAAcwgwAADAHAIMAAAwhwADAADMIcAAAABzCDAAAMAcAgwAADCHAAMAAMwhwAAAAHMIMAAAwJyI8j4A+OPMqFvEpJWflvcRAAAuQbTAAAAAcwgwAADAHAIMAAAwhwADAADMCfog3ldeeUVSUlKKPBYTEyNPPfWU+/n06dOyZMkSWbduneTl5UmnTp0kMTFRateuHdj+yJEjsmDBAklLS5OqVatKr169ZMiQIRIeHh7swwUAAAaVySykpk2byoMPPhhYrlTpvw09ixcvlo0bN8qECROkWrVqkpycLLNnz5ZHH33UrT979qzMmDHDBZpp06ZJTk6OzJkzx4UXDTEAAABl0oWkgUUDiHerWbOme/z48eOydu1aGTFihHTo0EFatWolY8aMkR07dkh6errbZvPmzbJv3z4ZN26ctGjRQuLj42Xw4MGyevVqyc/PL4vDBQAAxpRJC8yhQ4fkvvvuk8jISImNjXUtJ/Xr15eMjAw5c+aMxMXFBbZt3LixW6cBRrfV+2bNmhXpUurcubMsXLhQsrKypGXLlsX+m9odpTdPWFiYREVFBX4OlmDuCyVDzf2pL3Uue9TaH9Q5NGod9ADTtm1b16qi4160+0fHwzz00EOum+jo0aMSEREh1atXL/KcWrVquXVK7wuHF2+9t+58UlNTi4y90aCTlJQk0dHRQX6FIllB3yO+TcOGDcv7EEICdfYPtfYHda7YtQ56gNEuH0/z5s0DgWb9+vVSuXJlKSsDBw6UhISEwLKXBrOzs4Pa9USi95+26BUUFJT3YVRY+p7WDx/qXPaotT+os+1aa0NHSRofyvxSAtraoq0x+uI6duzowsSxY8eKtMLk5uYGWl30fteuXUX2oeu9deej3VV6Kw5vYNv0/4//w7JHnf1Drf1BnSt2rcv8PDAnT5504UXDhw7a1dlEW7duDaw/cOCAmzat41+U3mdmZgZCi9qyZYsbz9KkSZOyPlwAAGBA0Ftg9BwvXbp0cQNzdQyMnhdGZyX17NnTTZvu06eP26ZGjRpu+bnnnnOhxQswel4YDSo6dXro0KFu3MuyZcukX79+521hAQAAoSXoAebLL7+Up59+Wv7zn/+46dPt2rWT6dOnB6ZS6xRq7TPTQb3aneSdyM6jYWfSpElu1tGUKVOkSpUq7kR2OpUaAABAhRVU8A5CHcRbeHr1xdLwlZ94c9D2h2/XdOWncvDgQfqxy5C+pxs1akSdfUCt/UGdbddae1tKMoiXayEBAABzCDAAAMAcAgwAADCHAAMAAMwhwAAAAHMIMAAAwBwCDAAAMKfMr4UEXIysAV3EmvAFy8v7EACgwqMFBgAAmEOAAQAA5hBgAACAOQQYAABgDgEGAACYQ4ABAADmEGAAAIA5BBgAAGAOAQYAAJhDgAEAAOYQYAAAgDkEGAAAYA4BBgAAmEOAAQAA5hBgAACAOQQYAABgDgEGAACYQ4ABAADmEGAAAIA5BBgAAGAOAQYAAJhDgAEAAOYQYAAAgDkEGAAAYA4BBgAAmBNR3gcAVDRnRt0i5qz8tLyPAABKhRYYAABgDgEGAACYQ4ABAADmEGAAAIA5BBgAAGAOAQYAAJhDgAEAAOYQYAAAgDkEGAAAYA5n4gUgWQO6iDXhC5aX9yEAKEe0wAAAAHMIMAAAwJxLugvprbfekjfeeEOOHj0qzZs3l5EjR0qbNm3K+7AAAEA5u2RbYNatWydLliyRO+64Q5KSklyAmT59uuTm5pb3oQEAgHJ2ybbArFixQq677jq59tpr3fKoUaNk48aN8u6778ptt91W3ocHoJydGXWLWBOx8I3yPgSgwrgkA0x+fr5kZGQUCSqVKlWSuLg4SU9PL/Y5eXl57uYJCwuTqKgoiYgI7kvU/Ya1vjyo+wQQImZMlEP6OfL/bxaEP/iUWKOf0yoyMlIKCgrK+3AqtLAyqHVJ/25fkgHmq6++krNnz0rt2rWLPK7LBw4cKPY5qampkpKSEli+5pprZPz48VKnTp3gH+AzLwR/nwCAoKpfv355H0LIqF8Otb5kx8CU1sCBA2XRokWBm3Y5FW6RCZYTJ07Ir371K3ePskWt/UGd/UOt/UGdQ6PWl2QLTM2aNV2Xkc4+KkyXz22V8Wjzld7KmjaR7dmzh2ZJH1Brf1Bn/1Brf1Dn0Kj1JdkCo/1frVq1km3btgUe0y4lXY6NjS3XYwMAAOXvkmyBUQkJCTJ37lwXZPTcL6tWrZJTp05J7969y/vQAABAObtkA0yPHj3cYN5XXnnFdR21aNFCfvOb35y3C8kv2k2l56bxo7sq1FFrf1Bn/1Brf1Dn0Kh1WAGdhAAAwJhLcgwMAADAtyHAAAAAcwgwAADAHAIMAAAw55KdhVSe3nrrLXnjjTfc7Ce9CvbIkSPdVO7zWb9+vbz88suSnZ0tDRs2lKFDh8qVV17p6zGHQq3ffvtt+eCDDyQrK8st6xT7u++++1v/b3Bh72nPxx9/LE8//bR06dJFfvnLX/pyrKFW62PHjslLL70kn3zyiXz99dcSHR0tI0aM4DMkyHVeuXKlrFmzRo4cOeJOltqtWzcZMmSIVK5c2dfjtmT79u2yfPlyd6K6nJwcmThxonTt2vVbn5OWliZLlixxn9P16tWTQYMGldnpT2iBOce6detc8XVaWFJSkvvFmD59uuTm5ha7/Y4dO9wHfJ8+fdz2V199tTz++OOSmZnp+7FX9FrrL5Ne4+q3v/2tTJs2zf1y6P2XX37p+7FX5Dp7Dh8+LM8//7y0b9/et2MNtVrrhWv1PaxffiZMmCBPPfWU3HfffVK3bl3fj70i1/mjjz6SF198UX7wgx/I7373Oxk9erT74qnBEeen517TU5jce++9UhL6mTFz5ky54oorZNasWTJgwACZP3++bNq0ScoCAeYcK1askOuuu06uvfZaadKkibumkib0d999t9jt9QR7nTt3lltuucVtf9ddd7mWAf12gODW+oEHHpB+/fq5X6jGjRu7DyE9C8DWrVt9P/aKXGfvzNe///3v5c4775QGDRr4eryhVOu1a9e6Vpdf/OIX0q5dO1fr7373u+49juDVWb9oXn755dKzZ09X406dOrkvQ7t27fL92C2Jj493f9P+V6uLR1u4tL7Dhw93/y/9+/eX7t27u9avskCAOefbUEZGhsTFxQUe02sy6XJ6enqxz9HHC2+v9Jdj586dZX68oVbr4r4d6H5q1KhRhkcamnXWK7trM7u2LKLsar1hwwZp27atJCcnuz/CP//5z+W1115zARLBq7OGF32OF1j+9a9/yT//+U/3BxrBo3/3ivt7WNLP9NJiDEwheuZf/eA492y/unzgwIFin6P9r7Vq1SrymC6feyFKXHytz/XCCy+4pvZzf2FwcXX+/PPPXcuANgGjbGutf0i1+0hbBn7961/LoUOHZOHChXLmzBnX3YHg1Fnrq8978MEH3bLW94YbbpDbb7/dl2MOFUfP8/dQr1R9+vTpoI83IsDApL/85S9ugOnDDz/MILwg0g8a7TrScRjaAoOypV2gWmett7YiaPezjunSgZMEmODRgaWpqamSmJjoWrw0KP7pT39yLY06jgY2EWAK0Q8S/RA5t/VEl893DSZ9/NyBY7pc3tdsqoi19uiHuwYY/Talg/cQvDp7LQI6MNLjXW1E+8J1kKnOtEPwPj8iIiLc8zw6vkufo10lug4XX2edJfr973/fjZtRzZo1k5MnT8of//hH1wpTuP64cOf7exgVFVUmXzT5XytEPyz0G9C2bdsCj2lTpS7HxsYW+xx9/NxBpFu2bHEpH8GttXr99dflz3/+s7uwZ+vWrX062tCpc0xMjDzxxBOu+8i7XXXVVYFZBfXr1/f5FVTs97SOzdDWgMJjXg4ePCh16tQhvASxzjpeLiwsrMhjhJbg0797xf09/LbP9IvB/+A5EhIS5J133pH33ntP9u3b5/qj9c3vzWOfM2eOm47nuemmm2Tz5s3ufAT79+93V8/evXu3G32N4NZaW130m9T999/vRrrrNy696TcpBKfO+i1Jv50WvlWvXl2qVq3qfuaPanDf03379nWzkBYtWuTGb2zcuNF1dehsOwSvzhrC//rXv7puZ53qq39U9bNEHyfInJ9+tu7du9fdlNZOf9Zz6Sitsda68PtZt1m6dKn7e7h69Wo3XV2nU5cFPo3O0aNHDzfYS4OI/nHU6Yz6bd9rmtT/uMJJXr9B6fTeZcuWuXMKNGrUyE2J1A97BLfW+gGkzepPPvlkkf1oH7ZO90Vw6gz/aq0tWpMnT5bFixe7zw0dlH7jjTfKbbfdVo6vouLVWU+mpsv6Oa1jjLQbSsOLnggT56dfxqdOnRpY1nPvqF69esnYsWPdye28MKP0i+WkSZPc+1lPMaLn6tLTXeipRspCWIHXwQ0AAGAEbWcAAMAcAgwAADCHAAMAAMwhwAAAAHMIMAAAwBwCDAAAMIcAAwAAzOFEdgAAoMS2b9/urkm3Z88edzK7iRMnSteuXUu+AxHZtGmTvPrqq5KVlSWRkZHSvn17GT58uDsZXknRAgMAAEpML9ugZz++99575ULo5QYef/zxwDXW9GzU//nPf2T27Nml2g8tMAAAoMTi4+Pd7Xzy8vLcpXX02lPHjx+Xpk2bytChQ11gURkZGe4CnHqFe+9aVDfffLMLNaW5CjstMAAAIGiSk5Nl586d8tOf/tSFku7du8tjjz3mrrSu9Griem0qvRinBhkNOR988IHExcWV6oKxBBgAABAUenFHDSY/+9nP3LiWhg0byi233CLt2rWTd999122j41ymTJniWmmGDBkiP/rRj9xFNvU5pUEXEgAACIrMzEzXqjJ+/Pgij2vXUI0aNdzPegXxZ5991l3V+pprrpETJ064K4s/+eSTLtgUvpL4tyHAAACAoDh58qQb15KUlBQY3+KpWrWqu3/rrbekWrVqMmzYsMC6cePGyf333++6nmJjY0v0bxFgAABAUOjsJG2Byc3NdV1IxTl9+vQ3Wlm8sFNQUFDif4sxMAAAoFStLHv37nU3b1q0/qzjX2JiYqRnz54yZ84c+fvf/+7W7dq1S1JTU2Xjxo1u+yuvvFJ2794tKSkpbmCvzkqaN2+eREdHS8uWLUt8HGEFpYk7AAAgpKWlpcnUqVO/8biOaRk7dqwb7/Laa6/J+++/7wbn1qxZU9q2bSt33nmnNGvWzG2rU6z1ZHgHDhyQKlWquG4jnWrduHHjEh8HAQYAAJhDFxIAADCHAAMAAMwhwAAAAHMIMAAAwBwCDAAAMIcAAwAAzCHAAAAAcwgwAADAHAIMAAAwhwADAADMIcAAAABzCDAAAECs+X/+rGLR9nC3tgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# V·∫Ω bi·ªÉu ƒë·ªì t·∫ßn su·∫•t c·ªßa Volume\n",
    "df['volume'].hist(bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ph√¢n t√≠ch ph√¢n b·ªë d·ªØ li·ªáu Volume b·∫±ng Histogram**  \n",
    "\n",
    "üîπ `.hist(bins=10)`  \n",
    "- V·∫Ω **bi·ªÉu ƒë·ªì histogram** c·ªßa c·ªôt `Volume` v·ªõi **10 bins (nh√≥m d·ªØ li·ªáu)**.  \n",
    "- Gi√∫p tr·ª±c quan h√≥a **ph√¢n b·ªë d·ªØ li·ªáu**, ph√°t hi·ªán s·ª± **l·ªách** (skewness) v√† **gi√° tr·ªã ngo·∫°i lai** (outliers).  \n",
    "\n",
    "**√ù nghƒ©a c·ªßa bi·ªÉu ƒë·ªì histogram**  \n",
    "\n",
    "- **N·∫øu ph√¢n b·ªë l·ªách ph·∫£i (right-skewed)** ‚Üí D·ªØ li·ªáu c√≥ nhi·ªÅu gi√° tr·ªã nh·ªè, m·ªôt s·ªë gi√° tr·ªã r·∫•t l·ªõn.  \n",
    "- **N·∫øu ph√¢n b·ªë l·ªách tr√°i (left-skewed)** ‚Üí D·ªØ li·ªáu c√≥ nhi·ªÅu gi√° tr·ªã l·ªõn, m·ªôt s·ªë gi√° tr·ªã r·∫•t nh·ªè.  \n",
    "- **N·∫øu c√≥ outliers (ƒëi·ªÉm n·∫±m xa t·∫≠p trung ch√≠nh)** ‚Üí C√≥ th·ªÉ c·∫ßn x·ª≠ l√Ω nh∆∞ **lo·∫°i b·ªè** ho·∫∑c **chu·∫©n h√≥a d·ªØ li·ªáu**.  \n",
    "\n",
    "**C√°ch x·ª≠ l√Ω d·ªØ li·ªáu l·ªách/skewed**  \n",
    "\n",
    "‚úÖ **Log Transformation** ‚Üí D√πng `np.log1p(Volume)` ƒë·ªÉ gi·∫£m ƒë·ªô l·ªách.  \n",
    "‚úÖ **Scaling** ‚Üí D√πng `MinMaxScaler()` ho·∫∑c `StandardScaler()` ƒë·ªÉ chu·∫©n h√≥a.  \n",
    "‚úÖ **X·ª≠ l√Ω outliers** ‚Üí Lo·∫°i b·ªè ho·∫∑c thay th·∫ø b·∫±ng **gi√° tr·ªã trung b√¨nh/median**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B·ªï sung c√°c ch·ªâ b√°o kƒ© thu·∫≠t\n",
    "\n",
    "# T√≠nh CMA10\n",
    "df['CMA10'] = df['close'].rolling(window=10, center=True).mean()\n",
    "# T√≠nh SMA10\n",
    "df['SMA10'] = df['close'].rolling(window=10).mean()\n",
    "# T√≠nh SMA50\n",
    "df['SMA50'] = df['close'].rolling(window=50).mean()\n",
    "# T√≠nh EMA12 v√† EMA26\n",
    "df['EMA12'] = df['close'].ewm(span=12, adjust=False).mean()\n",
    "df['EMA26'] = df['close'].ewm(span=26, adjust=False).mean()\n",
    "# T√≠nh MACD\n",
    "df['MACD'] = df['EMA12'] - df['EMA26']\n",
    "#T√≠nh RSI\n",
    "# T√≠nh gi√° tƒÉng/gi·∫£m\n",
    "delta = df['close'].diff()\n",
    "\n",
    "# T√≠nh gi√° tƒÉng\n",
    "gain = delta.where(delta > 0, 0)\n",
    "\n",
    "# T√≠nh gi√° gi·∫£m\n",
    "loss = -delta.where(delta < 0, 0)\n",
    "\n",
    "# T√≠nh trung b√¨nh ƒë·ªông\n",
    "avg_gain = gain.rolling(window=14).mean()\n",
    "avg_loss = loss.rolling(window=14).mean()\n",
    "\n",
    "# T√≠nh RS v√† RSI\n",
    "rs = avg_gain / avg_loss\n",
    "df['RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "#T√≠nh CCI\n",
    "# T√≠nh gi√° trung b√¨nh\n",
    "typical_price = (df['high'] + df['low'] + df['close']) / 3\n",
    "\n",
    "# T√≠nh SMA c·ªßa gi√° trung b√¨nh\n",
    "sma_typical_price = typical_price.rolling(window=20).mean()\n",
    "\n",
    "# T√≠nh ƒë·ªô l·ªách chu·∫©n\n",
    "mean_deviation = typical_price.rolling(window=20).apply(lambda x: np.mean(np.abs(x - x.mean())))\n",
    "\n",
    "# T√≠nh CCI\n",
    "df['CCI'] = (typical_price - sma_typical_price) / (0.015 * mean_deviation)\n",
    "# T√≠nh %K v√† %D\n",
    "low_min = df['low'].rolling(window=14).min()\n",
    "high_max = df['high'].rolling(window=14).max()\n",
    "\n",
    "df['%K'] = 100 * (df['close'] - low_min) / (high_max - low_min)\n",
    "df['%D'] = df['%K'].rolling(window=3).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            open  high   low  close  volume  CMA10  SMA10  SMA50     EMA12  \\\n",
      "time                                                                         \n",
      "2007-11-16  2.29  2.29  2.29   2.29  248510    NaN    NaN    NaN  2.290000   \n",
      "2007-11-19  2.17  2.17  2.17   2.17  120480    NaN    NaN    NaN  2.271538   \n",
      "2007-11-20  2.08  2.08  2.08   2.08   58710    NaN    NaN    NaN  2.242071   \n",
      "2007-11-21  1.99  2.16  1.99   2.16  728080    NaN    NaN    NaN  2.229445   \n",
      "2007-11-22  2.16  2.16  2.08   2.16  266040    NaN    NaN    NaN  2.218761   \n",
      "\n",
      "               EMA26      MACD  RSI  CCI  %K  %D  \n",
      "time                                              \n",
      "2007-11-16  2.290000  0.000000  NaN  NaN NaN NaN  \n",
      "2007-11-19  2.281111 -0.009573  NaN  NaN NaN NaN  \n",
      "2007-11-20  2.266214 -0.024143  NaN  NaN NaN NaN  \n",
      "2007-11-21  2.258346 -0.028902  NaN  NaN NaN NaN  \n",
      "2007-11-22  2.251061 -0.032300  NaN  NaN NaN NaN  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4320, 15)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.head())\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model / H√†m **fit_model_2()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_2(train, val, timesteps, hl, lr, batch, epochs):\n",
    "    np.random.seed(1)\n",
    "    tf.random.set_seed(2)\n",
    "    rn.seed(3)\n",
    "\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    X_val = []\n",
    "    Y_val = []\n",
    "\n",
    "    # Loop for training data\n",
    "    for i in range(timesteps, train.shape[0]):\n",
    "        X_train.append(train[i-timesteps:i])\n",
    "        Y_train.append(train[i][0])\n",
    "    X_train, Y_train = np.array(X_train), np.array(Y_train)\n",
    "\n",
    "    # Loop for val data\n",
    "    for i in range(timesteps, val.shape[0]):\n",
    "        X_val.append(val[i-timesteps:i])\n",
    "        Y_val.append(val[i][0])\n",
    "    X_val, Y_val = np.array(X_val), np.array(Y_val)\n",
    "\n",
    "    # Adding Layers to the model\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(X_train.shape[2], input_shape = (X_train.shape[1], X_train.shape[2]), activation = 'relu', return_sequences = True))\n",
    "    for i in range(len(hl)-1):\n",
    "        model.add(SimpleRNN(hl[i], activation = 'relu', return_sequences = True))\n",
    "    model.add(SimpleRNN(hl[-1], activation = 'relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer = optimizers.Adam(learning_rate= lr), loss = 'mean_squared_error')\n",
    "\n",
    "    # Training the data\n",
    "    history = model.fit(X_train, Y_train, epochs = epochs, batch_size = batch, validation_data = (X_val, Y_val), verbose = 0, shuffle = False, callbacks=callbacks_list)\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, SimpleRNN):\n",
    "            layer.reset_states() #ƒê·∫£m b·∫£o m·ªói l·∫ßn hu·∫•n luy·ªán kh√¥ng b·ªã ·∫£nh h∆∞·ªüng b·ªüi tr·∫°ng th√°i c≈© c·ªßa LSTM.\n",
    "    return model, history.history['loss'], history.history['val_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  B∆∞·ªõc 1: ƒê·∫∑t Seed ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh t√°i l·∫≠p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gi√∫p ƒë·∫£m b·∫£o m·ªói l·∫ßn ch·∫°y ch∆∞∆°ng tr√¨nh, c√°c gi√° tr·ªã ng·∫´u nhi√™n ƒë∆∞·ª£c t·∫°o ra gi·ªëng nhau, tr√°nh k·∫øt qu·∫£ hu·∫•n luy·ªán thay ƒë·ªïi gi·ªØa c√°c l·∫ßn ch·∫°y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnp.random.seed(1)\\ntf.random.set_seed(2)\\nrn.seed(3)\\n'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(2)\n",
    "rn.seed(3)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 2: Chu·∫©n b·ªã d·ªØ li·ªáu hu·∫•n luy·ªán (train) v√† ki·ªÉm ƒë·ªãnh (val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nX_train = []\\nY_train = []\\nX_val = []\\nY_val = []\\n\\nfor i in range(timesteps, train.shape[0]):\\n    X_train.append(train[i-timesteps:i])\\n    Y_train.append(train[i][0])\\nX_train, Y_train = np.array(X_train, Y_train)\\n\\nfor i in range(timesteps, val.shape[0]):\\n    X_val.append(val[i-timesteps:i])\\n    Y_val.append(val[i][0])\\nX_val, Y_val = np.array(X_val, Y_val)  \\n'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "X_train = []\n",
    "Y_train = []\n",
    "X_val = []\n",
    "Y_val = []\n",
    "\n",
    "for i in range(timesteps, train.shape[0]):\n",
    "    X_train.append(train[i-timesteps:i])\n",
    "    Y_train.append(train[i][0])\n",
    "X_train, Y_train = np.array(X_train, Y_train)\n",
    "\n",
    "for i in range(timesteps, val.shape[0]):\n",
    "    X_val.append(val[i-timesteps:i])\n",
    "    Y_val.append(val[i][0])\n",
    "X_val, Y_val = np.array(X_val, Y_val)  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 3: X√¢y d·ª±ng m√¥ h√¨nh RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# X√¢y d·ª±ng m√¥ h√¨nh RNN\\nmodel = Sequential()\\nmodel.add(SimpleRNN(X_train.shape[2], input_shape= (X_train.shape[1], X_train.shape[2]), activation='relu', return_sequences= True))\\nfor i in range(len(hl)-1):\\n    model.add(SimpleRNN(hl[i], activation='relu', return_sequences= True))\\nmodel.add(SimpleRNN(hl[-1], activation='relu'))\\nmodel.add(Dense(1))\\n\""
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# X√¢y d·ª±ng m√¥ h√¨nh RNN\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(X_train.shape[2], input_shape= (X_train.shape[1], X_train.shape[2]), activation='relu', return_sequences= True))\n",
    "for i in range(len(hl)-1):\n",
    "    model.add(SimpleRNN(hl[i], activation='relu', return_sequences= True))\n",
    "model.add(SimpleRNN(hl[-1], activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Th√™m m·ªôt l·ªõp **SimpleRNN ƒë·∫ßu ti√™n**:  \n",
    "-   `X_train.shape[2]`: S·ªë ƒë·∫∑c tr∆∞ng (features).\n",
    "-   `input_shape = (X_train.shape[1], X_train.shape[2])`: ƒê·ªãnh d·∫°ng ƒë·∫ßu v√†o (timesteps, s·ªë ƒë·∫∑c tr∆∞ng).\n",
    "-   `activation = 'relu'`: H√†m k√≠ch ho·∫°t gi√∫p m√¥ h√¨nh h·ªçc phi tuy·∫øn t√≠nh.\n",
    "-   `return_sequences = True`: Gi·ªØ l·∫°i to√†n b·ªô chu·ªói ƒë·∫ßu ra ƒë·ªÉ s·ª≠ d·ª•ng trong c√°c l·ªõp ti·∫øp theo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 4: Bi√™n d·ªãch m√¥ h√¨nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Bi√™n d·ªãch\\nmodel.compile(optimizer= optimizers.Adam(learning_rate= lr), loss= 'mean_squared_error')\\n\""
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Bi√™n d·ªãch\n",
    "model.compile(optimizer= optimizers.Adam(learning_rate= lr), loss= 'mean_squared_error')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 5: Hu·∫•n luy·ªán m√¥ h√¨nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nhistory = model.fit(X_train, Y_train, epochs = epochs, batch_size = batch, validation_data = (X_val, Y_val), verbose = 0, shuffle = False, callbacks=callbacks_list)\\n'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "history = model.fit(X_train, Y_train, epochs = epochs, batch_size = batch, validation_data = (X_val, Y_val), verbose = 0, shuffle = False, callbacks=callbacks_list)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C·∫•u h√¨nh hu·∫•n luy·ªán m√¥ h√¨nh**\n",
    "\n",
    "üîπ Tham s·ªë trong qu√° tr√¨nh hu·∫•n luy·ªán  \n",
    "\n",
    "‚úÖ `epochs = epochs` ‚Üí S·ªë v√≤ng hu·∫•n luy·ªán  \n",
    "‚úÖ `batch_size = batch` ‚Üí K√≠ch th∆∞·ªõc batch  \n",
    "‚úÖ `validation_data = (X_val, Y_val)` ‚Üí D·ªØ li·ªáu ki·ªÉm ƒë·ªãnh ƒë·ªÉ theo d√µi hi·ªáu su·∫•t sau m·ªói epoch  \n",
    "‚úÖ `verbose = 0` ‚Üí Kh√¥ng hi·ªÉn th·ªã log hu·∫•n luy·ªán (c√≥ th·ªÉ ƒë·∫∑t `verbose = 1` ƒë·ªÉ xem ti·∫øn tr√¨nh)  \n",
    "‚úÖ `shuffle = False` ‚Üí Kh√¥ng x√°o tr·ªôn d·ªØ li·ªáu (do chu·ªói th·ªùi gian c√≥ t√≠nh th·ª© t·ª±)  \n",
    "‚úÖ `callbacks = callbacks_list` ‚Üí Danh s√°ch callback h·ªó tr·ª£ hu·∫•n luy·ªán  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 6: ƒê·∫£m b·∫£o tr·∫°ng th√°i kh√¥ng b·ªã ·∫£nh h∆∞·ªüng khi hu·∫•n luy·ªán nhi·ªÅu l·∫ßn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor layer in model.layers:\\n    if isinstance(layer, SimpleRNN):\\n        layer.reset_states()\\n'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, SimpleRNN):\n",
    "        layer.reset_states()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 7: Tr·∫£ v·ªÅ k·∫øt qu·∫£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nreturn model, history.history['train_loss'], history.history['val_loss']\\n\""
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "return model, history.history['train_loss'], history.history['val_loss']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Grid Search**: T√¨m ki·∫øm si√™u tham s·ªë t·ªëi ∆∞u "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'timesteps': [30, 40, 50],\n",
    "    'hl': [[40, 35]],\n",
    "    'lr': [1e-3, 1e-4],\n",
    "    'batch_size': [32, 64],\n",
    "    'num_epochs': [200, 250]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product # T√≠ch ƒë·ªÅ-c√°c\n",
    "import pandas as pd\n",
    "\n",
    "# H√†m Grid Search\n",
    "def grid_search_rnn(train, val, test, param_grid):\n",
    "# Kh·ªüi t·∫°o danh s√°ch l∆∞u k·∫øt qu·∫£\n",
    "    results = []\n",
    "    best_score = float('inf') # Ban ƒë·∫ßu ƒë∆∞·ª£c ƒë·∫∑t l√† v√¥ c√πng l·ªõn\n",
    "    best_params = None # L∆∞u b·ªô si√™u tham s·ªë c√≥ hi·ªáu su·∫•t t·ªët nh·∫•t\n",
    "# T·∫°o t·∫•t c·∫£ c√°c t·ªï h·ª£p tham s·ªë\n",
    "    all_combinations = list(product(*param_grid.values()))\n",
    "    param_names = list(param_grid.keys())\n",
    "# L·∫∑p qua t·ª´ng t·ªï h·ª£p tham s·ªë\n",
    "    for combination in all_combinations:\n",
    "        params = dict(zip(param_names, combination))\n",
    "        timesteps = params['timesteps']\n",
    "        hl = params['hl']\n",
    "        lr = params['lr']\n",
    "        batch_size = params['batch_size']\n",
    "        num_epochs = params['num_epochs']\n",
    "\n",
    "    print(f'Training with param: {params}')\n",
    "# Hu·∫•n luy·ªán v·ªõi fit.model()\n",
    "    model, train_loss, val_loss = fit_model_2(train, val, timesteps, hl, lr,  batch_size, num_epochs)\n",
    "# ƒê√°nh gi√° m√¥ h√¨nh v·ªõi evaluate_model()\n",
    "    mse, rmse, mape, r2, _, _ = evaluate_model_2(model, test, timesteps)\n",
    "# L∆∞u k·∫øt qu·∫£\n",
    "    results.append({\n",
    "        'timesteps': timesteps,\n",
    "        'hl': hl,\n",
    "        'lr': lr,\n",
    "        'batch_size': batch_size,\n",
    "        'num_epochs': num_epochs,\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAPE': mape,\n",
    "        'R¬≤': r2\n",
    "    })\n",
    "# C·∫≠p nh·∫≠t b·ªô si√™u tham s·ªë t·ªët nh·∫•t n·∫øu RMSE c·∫£i thi·ªán\n",
    "    if rmse < best_score:\n",
    "        best_score = rmse\n",
    "        best_params = params\n",
    "\n",
    "# Tr·∫£ v·ªÅ k·∫øt qu·∫£\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return best_params, best_score, results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H√†m **Evaluate_model_2()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# ƒê·ªãnh nghƒ©a h√†m mean_absolute_percentage_error() (MAPE)\\ndef mean_absolute_percentage_error(y_true, y_pred):\\n    y_true, y_pred = np.array(y_true), np.array(y_pred)\\n    return np.mean(np.abs((y_true - y_pred) / y_true))*100\"\\n    '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# ƒê·ªãnh nghƒ©a h√†m mean_absolute_percentage_error() (MAPE)\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true))*100\"\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_2(model, test, timesteps):\n",
    "    np.random.seed(1)\n",
    "    tf.random.set_seed(2)\n",
    "    rn.seed(3)\n",
    "\n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "\n",
    "    # Loop for testing data\n",
    "    for i in range(timesteps, test.shape[0]):\n",
    "        X_test.append(test[i-timesteps:i])\n",
    "        Y_test.append(test[i][0])\n",
    "    X_test, Y_test = np.array(X_test), np.array(Y_test)\n",
    "\n",
    "    # Evaluate model\n",
    "    Y_hat = model.predict(X_test)                         #ch·ª©a d·ª± ƒëo√°n c·ªßa model d·ª±a tr√™n ƒë·∫ßu v√†o x_test\n",
    "    mse = mean_squared_error(Y_test, Y_hat)\n",
    "    rmse = sqrt(mse)\n",
    "    mape = mean_absolute_percentage_error(Y_test, Y_hat)\n",
    "    r2 = r2_score(Y_test, Y_hat)\n",
    "    return mse, rmse, mape, r2, Y_test, Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot chart (v·∫Ω bi·ªÉu ƒë·ªì)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the predictions\n",
    "def plot_data_2(Y_test,Y_hat):\n",
    "    plt.plot(Y_test, c = 'r')\n",
    "    plt.plot(Y_hat, c = 'y')\n",
    "    plt.xlabel('Day')\n",
    "    plt.ylabel('Price')\n",
    "    plt.title(\"Stock Price Prediction using Multivatiate-RNN\")\n",
    "    plt.legend(['Actual','Predicted'], loc = 'lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training errors: tr·ª±c quan loss qua c√°c epoch -> th·∫•y qtr h·ªçc m√¥ h√¨nh, xem c√≥ overfitting ko\n",
    "def plot_error(train_loss, val_loss):\n",
    "    plt.plot(train_loss, c = 'r')\n",
    "    plt.plot(val_loss, c = 'b')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.title('Train Loss and Validation Loss Curve')\n",
    "    plt.legend(['train', 'val'], loc = 'upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model building**: X√¢y d·ª±ng m√¥ h√¨nh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 1: Tr√≠ch xu·∫•t v√† tr·ª±c quan h√≥a d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4267, 10)\n",
      "            close   CMA10   SMA10   SMA50      EMA12        RSI        CCI  \\\n",
      "time                                                                         \n",
      "2025-03-10  27.95  27.860  27.910  26.726  27.664525  66.666667  72.152412   \n",
      "2025-03-11  28.15  27.840  27.975  26.749  27.739213  68.965517  61.493516   \n",
      "2025-03-12  27.80  27.815  27.980  26.771  27.748565  63.440860  54.366686   \n",
      "2025-03-13  27.70  27.785  27.905  26.782  27.741094  63.440860  39.488661   \n",
      "2025-03-14  27.55  27.705  27.860  26.793  27.711695  47.887324  16.260163   \n",
      "\n",
      "                   %K         %D      MACD  \n",
      "time                                        \n",
      "2025-03-10  75.609756  78.723924  0.419877  \n",
      "2025-03-11  85.365854  80.790320  0.427503  \n",
      "2025-03-12  68.292683  76.422764  0.400685  \n",
      "2025-03-13  40.000000  64.552846  0.367130  \n",
      "2025-03-14  28.000000  45.430894  0.324692  \n"
     ]
    }
   ],
   "source": [
    "# Extracting the series\n",
    "series = df[['close', 'CMA10', 'SMA10', 'SMA50', 'EMA12', 'RSI', 'CCI', '%K', '%D', 'MACD']]\n",
    "# Drop rows with NaN values\n",
    "series = series.dropna()\n",
    "\n",
    "# Display the shape and the tail of the cleaned series\n",
    "print(series.shape)\n",
    "print(series.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>CMA10</th>\n",
       "      <th>SMA10</th>\n",
       "      <th>SMA50</th>\n",
       "      <th>EMA12</th>\n",
       "      <th>RSI</th>\n",
       "      <th>CCI</th>\n",
       "      <th>%K</th>\n",
       "      <th>%D</th>\n",
       "      <th>MACD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4267.000000</td>\n",
       "      <td>4267.000000</td>\n",
       "      <td>4267.00000</td>\n",
       "      <td>4267.000000</td>\n",
       "      <td>4267.000000</td>\n",
       "      <td>4267.000000</td>\n",
       "      <td>4267.000000</td>\n",
       "      <td>4267.000000</td>\n",
       "      <td>4267.000000</td>\n",
       "      <td>4267.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.025397</td>\n",
       "      <td>10.022310</td>\n",
       "      <td>9.99800</td>\n",
       "      <td>9.880565</td>\n",
       "      <td>9.992086</td>\n",
       "      <td>52.254060</td>\n",
       "      <td>12.008518</td>\n",
       "      <td>53.007353</td>\n",
       "      <td>53.000609</td>\n",
       "      <td>0.041366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10.126221</td>\n",
       "      <td>10.118056</td>\n",
       "      <td>10.10649</td>\n",
       "      <td>10.019039</td>\n",
       "      <td>10.097556</td>\n",
       "      <td>17.992688</td>\n",
       "      <td>112.438235</td>\n",
       "      <td>30.851704</td>\n",
       "      <td>29.031483</td>\n",
       "      <td>0.373255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>0.71000</td>\n",
       "      <td>0.793800</td>\n",
       "      <td>0.726093</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-486.486486</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.739242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.780000</td>\n",
       "      <td>1.778000</td>\n",
       "      <td>1.77800</td>\n",
       "      <td>1.760400</td>\n",
       "      <td>1.776499</td>\n",
       "      <td>38.724730</td>\n",
       "      <td>-76.041990</td>\n",
       "      <td>25.730277</td>\n",
       "      <td>26.681097</td>\n",
       "      <td>-0.045488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.680000</td>\n",
       "      <td>5.725000</td>\n",
       "      <td>5.70500</td>\n",
       "      <td>5.174200</td>\n",
       "      <td>5.719069</td>\n",
       "      <td>52.577320</td>\n",
       "      <td>21.806854</td>\n",
       "      <td>55.445545</td>\n",
       "      <td>55.238095</td>\n",
       "      <td>0.020672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.415000</td>\n",
       "      <td>15.376000</td>\n",
       "      <td>15.21950</td>\n",
       "      <td>15.065600</td>\n",
       "      <td>15.178271</td>\n",
       "      <td>66.146301</td>\n",
       "      <td>101.156626</td>\n",
       "      <td>80.702355</td>\n",
       "      <td>79.987975</td>\n",
       "      <td>0.119916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>39.900000</td>\n",
       "      <td>39.285000</td>\n",
       "      <td>39.28500</td>\n",
       "      <td>37.489200</td>\n",
       "      <td>39.090578</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>364.285714</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2.247325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             close        CMA10       SMA10        SMA50        EMA12  \\\n",
       "count  4267.000000  4267.000000  4267.00000  4267.000000  4267.000000   \n",
       "mean     10.025397    10.022310     9.99800     9.880565     9.992086   \n",
       "std      10.126221    10.118056    10.10649    10.019039    10.097556   \n",
       "min       0.680000     0.710000     0.71000     0.793800     0.726093   \n",
       "25%       1.780000     1.778000     1.77800     1.760400     1.776499   \n",
       "50%       5.680000     5.725000     5.70500     5.174200     5.719069   \n",
       "75%      15.415000    15.376000    15.21950    15.065600    15.178271   \n",
       "max      39.900000    39.285000    39.28500    37.489200    39.090578   \n",
       "\n",
       "               RSI          CCI           %K           %D         MACD  \n",
       "count  4267.000000  4267.000000  4267.000000  4267.000000  4267.000000  \n",
       "mean     52.254060    12.008518    53.007353    53.000609     0.041366  \n",
       "std      17.992688   112.438235    30.851704    29.031483     0.373255  \n",
       "min       0.000000  -486.486486     0.000000     0.000000    -1.739242  \n",
       "25%      38.724730   -76.041990    25.730277    26.681097    -0.045488  \n",
       "50%      52.577320    21.806854    55.445545    55.238095     0.020672  \n",
       "75%      66.146301   101.156626    80.702355    79.987975     0.119916  \n",
       "max     100.000000   364.285714   100.000000   100.000000     2.247325  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 2: Chia d·ªØ li·ªáu th√†nh c√°c t·∫≠p Train, Validation, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4267, 10)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2987, 10) (640, 10) (640, 10)\n"
     ]
    }
   ],
   "source": [
    "n = series.shape[0]\n",
    "val_size =  test_size = int(n * 0.15)\n",
    "train_size = n - val_size - test_size # ƒê·ªÉ tr√°nh sai s·ªë l√†m m·∫•t d·ªØ li·ªáu\n",
    "\n",
    "# Chia t·∫≠p d·ªØ li·ªáu theo th·ª© t·ª± th·ªùi gian\n",
    "train_data = series.iloc[:train_size].values\n",
    "val_data = series.iloc[train_size:train_size + val_size].values\n",
    "test_data = series.iloc[(train_size + val_size):].values\n",
    "# Ki·ªÉm tra k√≠ch th∆∞·ªõc c·ªßa t·ª´ng t·∫≠p\n",
    "print(train_data.shape, val_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 3: Chu·∫©n h√≥a d·ªØ li·ªáu b·∫±ng MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2987, 10) (640, 10) (640, 10)\n"
     ]
    }
   ],
   "source": [
    "# Normalisation\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(2)\n",
    "rn.seed(3)\n",
    "\n",
    "# Chu·∫©n h√≥a\n",
    "sc = MinMaxScaler() # T·∫°o b·ªô chu·∫©n h√≥a MinMaxScaler\n",
    "train = sc.fit_transform(train_data)\n",
    "val = sc.transform(val_data)\n",
    "test = sc.transform(test_data)\n",
    "\n",
    "print(train.shape, val.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 4: T√¨m si√™u tham s·ªë t·ªët nh·∫•t b·∫±ng Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with param: {'timesteps': 50, 'hl': [40, 35], 'lr': 0.0001, 'batch_size': 64, 'num_epochs': 250}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.55522, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_loss improved from 1.55522 to 0.70718, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: val_loss improved from 0.70718 to 0.29848, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: val_loss improved from 0.29848 to 0.11666, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5: val_loss improved from 0.11666 to 0.05989, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6: val_loss improved from 0.05989 to 0.04296, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7: val_loss improved from 0.04296 to 0.03588, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8: val_loss improved from 0.03588 to 0.03239, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9: val_loss improved from 0.03239 to 0.03031, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10: val_loss improved from 0.03031 to 0.02851, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11: val_loss improved from 0.02851 to 0.02678, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12: val_loss improved from 0.02678 to 0.02519, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13: val_loss improved from 0.02519 to 0.02382, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14: val_loss improved from 0.02382 to 0.02276, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15: val_loss improved from 0.02276 to 0.02185, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16: val_loss improved from 0.02185 to 0.02094, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17: val_loss improved from 0.02094 to 0.01984, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18: val_loss improved from 0.01984 to 0.01898, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19: val_loss improved from 0.01898 to 0.01766, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20: val_loss improved from 0.01766 to 0.01706, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21: val_loss improved from 0.01706 to 0.01650, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22: val_loss did not improve from 0.01650\n",
      "\n",
      "Epoch 23: val_loss improved from 0.01650 to 0.01626, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24: val_loss improved from 0.01626 to 0.01578, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25: val_loss improved from 0.01578 to 0.01485, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26: val_loss improved from 0.01485 to 0.01442, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27: val_loss improved from 0.01442 to 0.01424, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28: val_loss improved from 0.01424 to 0.01421, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29: val_loss did not improve from 0.01421\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.01421\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.01421\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.01421\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.01421\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.01421\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.01421\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.01421\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.01421\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.01421\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.01421\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.01421\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.01421\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.01421\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.01421\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.01421\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.01421\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.01421\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.01421\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.01421\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.01421\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.01421\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.01421\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.01421\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.01421\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.01421\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.01421\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.01421\n",
      "\n",
      "Epoch 57: val_loss improved from 0.01421 to 0.01410, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 58: val_loss improved from 0.01410 to 0.01404, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 59: val_loss improved from 0.01404 to 0.01392, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 60: val_loss improved from 0.01392 to 0.01374, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 61: val_loss improved from 0.01374 to 0.01358, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 62: val_loss improved from 0.01358 to 0.01348, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 63: val_loss improved from 0.01348 to 0.01334, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 64: val_loss improved from 0.01334 to 0.01326, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 65: val_loss improved from 0.01326 to 0.01311, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 66: val_loss improved from 0.01311 to 0.01304, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 67: val_loss improved from 0.01304 to 0.01269, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 68: val_loss improved from 0.01269 to 0.01266, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 69: val_loss improved from 0.01266 to 0.01247, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 70: val_loss improved from 0.01247 to 0.01244, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 71: val_loss improved from 0.01244 to 0.01229, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 72: val_loss improved from 0.01229 to 0.01220, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 73: val_loss improved from 0.01220 to 0.01196, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 74: val_loss improved from 0.01196 to 0.01184, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 75: val_loss improved from 0.01184 to 0.01158, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 76: val_loss improved from 0.01158 to 0.01137, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 77: val_loss improved from 0.01137 to 0.01117, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 78: val_loss improved from 0.01117 to 0.01104, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 79: val_loss improved from 0.01104 to 0.01090, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 80: val_loss did not improve from 0.01090\n",
      "\n",
      "Epoch 81: val_loss improved from 0.01090 to 0.01075, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 82: val_loss improved from 0.01075 to 0.01072, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 83: val_loss improved from 0.01072 to 0.01063, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 84: val_loss improved from 0.01063 to 0.01051, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 85: val_loss improved from 0.01051 to 0.01045, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 86: val_loss improved from 0.01045 to 0.01024, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 87: val_loss improved from 0.01024 to 0.01001, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 88: val_loss improved from 0.01001 to 0.00969, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 89: val_loss improved from 0.00969 to 0.00938, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 90: val_loss improved from 0.00938 to 0.00919, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 91: val_loss improved from 0.00919 to 0.00909, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 92: val_loss did not improve from 0.00909\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.00909\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.00909\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.00909\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.00909\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.00909\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.00909\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.00909\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.00909\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.00909\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.00909\n",
      "\n",
      "Epoch 103: val_loss improved from 0.00909 to 0.00652, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 104: val_loss improved from 0.00652 to 0.00647, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 105: val_loss did not improve from 0.00647\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.00647\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.00647\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.00647\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.00647\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.00647\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.00647\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.00647\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.00647\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.00647\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.00647\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.00647\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.00647\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.00647\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.00647\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.00647\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.00647\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.00647\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.00647\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.00647\n",
      "\n",
      "Epoch 125: val_loss improved from 0.00647 to 0.00645, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 126: val_loss improved from 0.00645 to 0.00572, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 127: val_loss improved from 0.00572 to 0.00485, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 128: val_loss improved from 0.00485 to 0.00448, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 129: val_loss did not improve from 0.00448\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.00448\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.00448\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.00448\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.00448\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.00448\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.00448\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.00448\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.00448\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.00448\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.00448\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.00448\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.00448\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.00448\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.00448\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.00448\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.00448\n",
      "\n",
      "Epoch 146: val_loss improved from 0.00448 to 0.00445, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 147: val_loss improved from 0.00445 to 0.00388, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 148: val_loss improved from 0.00388 to 0.00371, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 149: val_loss improved from 0.00371 to 0.00370, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 150: val_loss did not improve from 0.00370\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.00370\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.00370\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.00370\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.00370\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.00370\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.00370\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.00370\n",
      "\n",
      "Epoch 158: val_loss improved from 0.00370 to 0.00361, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 159: val_loss did not improve from 0.00361\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.00361\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.00361\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.00361\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.00361\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.00361\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.00361\n",
      "\n",
      "Epoch 166: val_loss improved from 0.00361 to 0.00335, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 167: val_loss improved from 0.00335 to 0.00326, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 168: val_loss improved from 0.00326 to 0.00323, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 169: val_loss did not improve from 0.00323\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.00323\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.00323\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.00323\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.00323\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.00323\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.00323\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.00323\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.00323\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.00323\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.00323\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.00323\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.00323\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.00323\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.00323\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.00323\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.00323\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.00323\n",
      "\n",
      "Epoch 187: val_loss improved from 0.00323 to 0.00303, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 188: val_loss improved from 0.00303 to 0.00302, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 189: val_loss did not improve from 0.00302\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.00302\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.00302\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.00302\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.00302\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.00302\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.00302\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.00302\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.00302\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.00302\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.00302\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.00302\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.00302\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.00302\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.00302\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.00302\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.00302\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.00302\n",
      "\n",
      "Epoch 207: val_loss improved from 0.00302 to 0.00292, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 208: val_loss did not improve from 0.00292\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.00292\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.00292\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.00292\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.00292\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.00292\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.00292\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.00292\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.00292\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.00292\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.00292\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.00292\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.00292\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.00292\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.00292\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.00292\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.00292\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.00292\n",
      "\n",
      "Epoch 226: val_loss improved from 0.00292 to 0.00281, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 227: val_loss improved from 0.00281 to 0.00280, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 228: val_loss did not improve from 0.00280\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.00280\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.00280\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.00280\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.00280\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.00280\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.00280\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.00280\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.00280\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.00280\n",
      "\n",
      "Epoch 238: val_loss improved from 0.00280 to 0.00279, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 239: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 247: val_loss improved from 0.00279 to 0.00275, saving model to 10Var-hpg-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 248: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.00275\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "   timesteps        hl      lr  batch_size  num_epochs       MSE      RMSE  \\\n",
      "0         50  [40, 35]  0.0001          64         250  0.002294  0.047899   \n",
      "\n",
      "       MAPE        R¬≤  \n",
      "0  0.023111  0.970069  \n",
      "Best parameters: {'timesteps': 50, 'hl': [40, 35], 'lr': 0.0001, 'batch_size': 64, 'num_epochs': 250}\n",
      "Best RMSE score: 0.0478987837599279\n"
     ]
    }
   ],
   "source": [
    "best_params, best_score, results_df = grid_search_rnn(train, val, test, param_grid)\n",
    "\n",
    "print(results_df)\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best RMSE score:\", best_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 5: Hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi b·ªô tham s·ªë t·ªëi ∆∞u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.00275\n"
     ]
    }
   ],
   "source": [
    "timesteps = 50\n",
    "hl = [40, 35]\n",
    "lr = 0.0001\n",
    "batch_size = 64\n",
    "num_epochs = 250\n",
    "\n",
    "model, train_error, val_error = fit_model_2(train, val, timesteps, hl, lr, batch_size, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 6: ƒê√°nh gi√° m√¥ h√¨nh v√† tr·ª±c quan h√≥a k·∫øt qu·∫£"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1. V·∫Ω bi·ªÉu ƒë·ªì train_loss v√† val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_error(train_error, val_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2. ƒê√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step\n"
     ]
    }
   ],
   "source": [
    "mse, rmse, mape, r2, true, predicted = evaluate_model_2(model, test, timesteps) #ƒë∆∞·ª£c t√≠nh d·ª±a tr√™n b·ªô d·ªØ li·ªáu ƒë√£ chu·∫©n ho√°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.0022942934856803325\n",
      "RMSE = 0.0478987837599279\n",
      "MAPE = 0.023111373786061293\n",
      "R-Squared Score = 0.970069153146253\n"
     ]
    }
   ],
   "source": [
    "print('MSE = {}'.format(mse))\n",
    "print('RMSE = {}'.format(rmse))\n",
    "print('MAPE = {}'.format(mape))\n",
    "print('R-Squared Score = {}'.format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3. V·∫Ω ƒë·ªì th·ªã d·ª± ƒëo√°n vs th·ª±c t·∫ø"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data_2(true, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a model\n",
    "model.save('10VAR-hpg-rnn.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Multivariate-3-GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0' # ƒë·∫£m b·∫£o r·∫±ng c√°c gi√° tr·ªã bƒÉm c·ªßa ƒë·ªëi t∆∞·ª£ng b·∫•t bi·∫øn (dict, set, chu·ªói, tuple...) lu√¥n gi·ªëng nhau gi·ªØa c√°c l·∫ßn ch·∫°y\n",
    "\n",
    "import random as rn\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "rn.seed(3)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, GRU, Dropout\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from math import sqrt\n",
    "import datetime as dt\n",
    "import time\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H√†m callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=80,  verbose=1, mode='min')  \n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=\"10Var-hpg-gru.h5\",   # T√™n file l∆∞u m√¥ h√¨nh\n",
    "    monitor=\"val_loss\",         # Theo d√µi val_loss\n",
    "    save_best_only=True,        # Ch·ªâ l∆∞u khi t·ªët h∆°n m√¥ h√¨nh tr∆∞·ªõc ƒë√≥\n",
    "    mode=\"min\",                 # Gi·∫£m min c·ªßa val_loss l√† t·ªët nh·∫•t\n",
    "    verbose=1\n",
    ")\n",
    "callbacks_list = [earlystop, checkpoint] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ƒê·ªçc d·ªØ li·ªáu t·ª´ file csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = r\"hpg_history.csv\"\n",
    "df = pd.read_csv(url, parse_dates= True, index_col= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-11-16</th>\n",
       "      <td>2.29</td>\n",
       "      <td>2.29</td>\n",
       "      <td>2.29</td>\n",
       "      <td>2.29</td>\n",
       "      <td>248510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-11-19</th>\n",
       "      <td>2.17</td>\n",
       "      <td>2.17</td>\n",
       "      <td>2.17</td>\n",
       "      <td>2.17</td>\n",
       "      <td>120480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-11-20</th>\n",
       "      <td>2.08</td>\n",
       "      <td>2.08</td>\n",
       "      <td>2.08</td>\n",
       "      <td>2.08</td>\n",
       "      <td>58710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-11-21</th>\n",
       "      <td>1.99</td>\n",
       "      <td>2.16</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.16</td>\n",
       "      <td>728080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-11-22</th>\n",
       "      <td>2.16</td>\n",
       "      <td>2.16</td>\n",
       "      <td>2.08</td>\n",
       "      <td>2.16</td>\n",
       "      <td>266040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            open  high   low  close  volume\n",
       "time                                       \n",
       "2007-11-16  2.29  2.29  2.29   2.29  248510\n",
       "2007-11-19  2.17  2.17  2.17   2.17  120480\n",
       "2007-11-20  2.08  2.08  2.08   2.08   58710\n",
       "2007-11-21  1.99  2.16  1.99   2.16  728080\n",
       "2007-11-22  2.16  2.16  2.08   2.16  266040"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "open      0\n",
       "high      0\n",
       "low       0\n",
       "close     0\n",
       "volume    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ti·ªÅn x·ª≠ l√≠ d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X√≥a c√°c d√≤ng c√≥ gi√° tr·ªã Volume b·∫±ng 0\n",
    "df.drop(df[df['volume']==0].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "open      0.999667\n",
       "high      0.999857\n",
       "low       0.999838\n",
       "close     1.000000\n",
       "volume    0.799299\n",
       "Name: close, dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ma tr·∫≠n t∆∞∆°ng quan (·ªü ƒë√¢y l√† Pearson t∆∞∆°ng quan tuy·∫øn t√≠nh)\n",
    "df.corr()['close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    4.320000e+03\n",
      "mean     7.877410e+06\n",
      "std      1.133795e+07\n",
      "min      5.000000e+01\n",
      "25%      4.646300e+05\n",
      "50%      2.422785e+06\n",
      "75%      1.233620e+07\n",
      "max      9.967998e+07\n",
      "Name: volume, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df.describe().volume) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['volume'].hist(bins= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B·ªï sung c√°c ch·ªâ b√°o kƒ© thu·∫≠t\n",
    "\n",
    "# T√≠nh CMA10\n",
    "df['CMA10'] = df['close'].rolling(window=10, center=True).mean()\n",
    "# T√≠nh SMA10\n",
    "df['SMA10'] = df['close'].rolling(window=10).mean()\n",
    "# T√≠nh SMA50\n",
    "df['SMA50'] = df['close'].rolling(window=50).mean()\n",
    "# T√≠nh EMA12 v√† EMA26\n",
    "df['EMA12'] = df['close'].ewm(span=12, adjust=False).mean()\n",
    "df['EMA26'] = df['close'].ewm(span=26, adjust=False).mean()\n",
    "# T√≠nh MACD\n",
    "df['MACD'] = df['EMA12'] - df['EMA26']\n",
    "#T√≠nh RSI\n",
    "# T√≠nh gi√° tƒÉng/gi·∫£m\n",
    "delta = df['close'].diff()\n",
    "\n",
    "# T√≠nh gi√° tƒÉng\n",
    "gain = delta.where(delta > 0, 0)\n",
    "\n",
    "# T√≠nh gi√° gi·∫£m\n",
    "loss = -delta.where(delta < 0, 0)\n",
    "\n",
    "# T√≠nh trung b√¨nh ƒë·ªông\n",
    "avg_gain = gain.rolling(window=14).mean()\n",
    "avg_loss = loss.rolling(window=14).mean()\n",
    "\n",
    "# T√≠nh RS v√† RSI\n",
    "rs = avg_gain / avg_loss\n",
    "df['RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "#T√≠nh CCI\n",
    "# T√≠nh gi√° trung b√¨nh\n",
    "typical_price = (df['high'] + df['low'] + df['close']) / 3\n",
    "\n",
    "# T√≠nh SMA c·ªßa gi√° trung b√¨nh\n",
    "sma_typical_price = typical_price.rolling(window=20).mean()\n",
    "\n",
    "# T√≠nh ƒë·ªô l·ªách chu·∫©n\n",
    "mean_deviation = typical_price.rolling(window=20).apply(lambda x: np.mean(np.abs(x - x.mean())))\n",
    "\n",
    "# T√≠nh CCI\n",
    "df['CCI'] = (typical_price - sma_typical_price) / (0.015 * mean_deviation)\n",
    "# T√≠nh %K v√† %D\n",
    "low_min = df['low'].rolling(window=14).min()\n",
    "high_max = df['high'].rolling(window=14).max()\n",
    "\n",
    "df['%K'] = 100 * (df['close'] - low_min) / (high_max - low_min)\n",
    "df['%D'] = df['%K'].rolling(window=3).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            open  high   low  close  volume  CMA10  SMA10  SMA50     EMA12  \\\n",
      "time                                                                         \n",
      "2007-11-16  2.29  2.29  2.29   2.29  248510    NaN    NaN    NaN  2.290000   \n",
      "2007-11-19  2.17  2.17  2.17   2.17  120480    NaN    NaN    NaN  2.271538   \n",
      "2007-11-20  2.08  2.08  2.08   2.08   58710    NaN    NaN    NaN  2.242071   \n",
      "2007-11-21  1.99  2.16  1.99   2.16  728080    NaN    NaN    NaN  2.229445   \n",
      "2007-11-22  2.16  2.16  2.08   2.16  266040    NaN    NaN    NaN  2.218761   \n",
      "\n",
      "               EMA26      MACD  RSI  CCI  %K  %D  \n",
      "time                                              \n",
      "2007-11-16  2.290000  0.000000  NaN  NaN NaN NaN  \n",
      "2007-11-19  2.281111 -0.009573  NaN  NaN NaN NaN  \n",
      "2007-11-20  2.266214 -0.024143  NaN  NaN NaN NaN  \n",
      "2007-11-21  2.258346 -0.028902  NaN  NaN NaN NaN  \n",
      "2007-11-22  2.251061 -0.032300  NaN  NaN NaN NaN  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4320, 15)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.head())\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model / H√†m **fit_model_3()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_3(train, val, timesteps, hl, lr, batch, epochs):\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    tf.random.set_seed(2)\n",
    "    rn.seed(3)\n",
    "    \"\"\"\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    X_val = []\n",
    "    Y_val = []\n",
    "\n",
    "    for i in range(timesteps, train.shape[0]):\n",
    "        X_train.append(train[i-timesteps:i])\n",
    "        Y_train.append(train[i][0])\n",
    "    X_train, Y_train = np.array(X_train), np.array(Y_train)\n",
    "\n",
    "    for i in range(timesteps, val.shape[0]):\n",
    "        X_val.append(val[i-timesteps:i])\n",
    "        Y_val.append(val[i][0])\n",
    "    X_val, Y_val = np.array(X_val), np.array(Y_val)\n",
    "\n",
    "    # Th√™m c√°c l·ªõp v√†o m√¥ h√¨nh\n",
    "    model = Sequential()\n",
    "    model.add(GRU(X_train.shape[2], input_shape= (X_train.shape[1], X_train.shape[2]), activation= 'relu', return_sequences= True))\n",
    "    for i in range(len(hl)-1):\n",
    "        model.add(GRU(hl[i], activation='relu', return_sequences= True))\n",
    "    model.add(GRU(hl[-1], activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Bi√™n d·ªãch\n",
    "    model.compile(optimizer= optimizers.Adam(learning_rate= lr), loss= 'mean_squared_error')\n",
    "\n",
    "    # Hu·∫•n luy·ªán d·ªØ li·ªáu\n",
    "    history = model.fit(X_train, Y_train, epochs= epochs, batch_size= batch, validation_data= (X_val, Y_val), verbose= 0, shuffle= False, callbacks= callbacks_list)\n",
    "    \n",
    "    # ƒê·∫∑t l·∫°i tr·∫°ng th√°i\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, GRU):\n",
    "            layer.reset_states()\n",
    "    \n",
    "    return model, history.history['loss'], history.history['val_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H√†m **Evaluate_model_3()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_3(model, test, timesteps):\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    tf.random.set_seed(2)\n",
    "    \"\"\"\n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "\n",
    "    for i in range(timesteps, test.shape[0]):\n",
    "        X_test.append(test[i-timesteps:i])\n",
    "        Y_test.append(test[i][0])\n",
    "    X_test, Y_test= np.array(X_test), np.array(Y_test)\n",
    "\n",
    "    # C√°c ch·ªâ s·ªë ƒë√°nh gi√°\n",
    "    Y_hat = model.predict(X_test)\n",
    "    mse = mean_squared_error(Y_test, Y_hat)\n",
    "    rmse = sqrt(mse)\n",
    "    mape = mean_absolute_percentage_error(Y_test, Y_hat)\n",
    "    r2 = r2_score(Y_test, Y_hat)\n",
    "\n",
    "    return mse, rmse, mape, r2, Y_test, Y_hat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Grid Search**: T√¨m ki·∫øm si√™u tham s·ªë t·ªëi ∆∞u "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'timesteps': [30, 40, 50],\n",
    "    'hl': [[40, 35]],\n",
    "    'lr': [1e-4, 1e-3],\n",
    "    'batch_size': [32, 64],\n",
    "    'num_epochs': [200, 250]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H√†m grid search\n",
    "def grid_search_rnn(train, val, test, param_grid):\n",
    "    results = []\n",
    "    best_score = float('inf')\n",
    "    best_params = None\n",
    "\n",
    "    # T·∫°o t·∫•t c·∫£ c√°c t·ªï h·ª£p tham s·ªë\n",
    "    all_combinations = list(product(*(param_grid.values())))\n",
    "    param_names = param_grid.keys()\n",
    "\n",
    "    for combination in all_combinations:\n",
    "        # G√°n gi√° tr·ªã tham s·ªë hi·ªán t·∫°i\n",
    "        params = dict(zip(param_names, combination))\n",
    "        hl = params['hl']\n",
    "        lr = params['lr']\n",
    "        batch_size = params['batch_size']\n",
    "        num_epochs = params['num_epochs']\n",
    "\n",
    "        print(f'Training with params: {params}')\n",
    "\n",
    "        # Hu·∫•n luy·ªán m√¥ h√¨nh\n",
    "        model, train_loss, val_loss = fit_model_3(train, val, timesteps, hl, lr, batch_size, num_epochs)\n",
    "\n",
    "        # ƒê√°nh gi√° m√¥ h√¨nh\n",
    "        mse, rmse, mape, r2, _, _ = evaluate_model_3(model, test, timesteps)\n",
    "\n",
    "        # L∆∞u k·∫øt qu·∫£\n",
    "        results.append({\n",
    "            'timesteps': timesteps,\n",
    "            'hl': hl,\n",
    "            'lr': lr,\n",
    "            'batch_size': batch_size,\n",
    "            'num_epochs': num_epochs,\n",
    "            'mse': mse,\n",
    "            'rmse': rmse,\n",
    "            'mape': mape,\n",
    "            'r2': r2\n",
    "        })\n",
    "\n",
    "        # C·∫≠p nh·∫≠t tham s·ªë t·ªët nh·∫•t n·∫øu RMSE c·∫£i thi·ªán\n",
    "        if rmse < best_score:\n",
    "            best_score = rmse\n",
    "            best_params = params\n",
    "\n",
    "        # Tr·∫£ v·ªÅ k·∫øt qu·∫£\n",
    "        results_df = pd.DataFrame(results)\n",
    "        return best_params, best_score, results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot chart (v·∫Ω bi·ªÉu ƒë·ªì)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the predictions\n",
    "def plot_data_3(Y_test, Y_hat):\n",
    "    plt.plot(Y_test, c = 'r')\n",
    "    plt.plot(Y_hat, c = 'y')\n",
    "    plt.xlabel('Day')\n",
    "    plt.ylabel('Price')\n",
    "    plt.title(\"Stock Price Prediction using Multivariate-GRU\")\n",
    "    plt.legend(['Actual','Predicted'], loc = 'lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training errors: tr·ª±c quan loss qua c√°c epoch -> th·∫•y qtr h·ªçc m√¥ h√¨nh, xem c√≥ overfitting ko\n",
    "def plot_error(train_loss, val_loss):\n",
    "    plt.plot(train_loss, c = 'r')\n",
    "    plt.plot(val_loss, c = 'b')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.title('Train Loss and Validation Loss Curve')\n",
    "    plt.legend(['train', 'val'], loc = 'upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model building**: X√¢y d·ª±ng m√¥ h√¨nh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 1: Tr√≠ch xu·∫•t v√† tr·ª±c quan h√≥a d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4267, 10)\n",
      "            close   CMA10   SMA10   SMA50      EMA12        RSI        CCI  \\\n",
      "time                                                                         \n",
      "2025-03-10  27.95  27.860  27.910  26.726  27.664525  66.666667  72.152412   \n",
      "2025-03-11  28.15  27.840  27.975  26.749  27.739213  68.965517  61.493516   \n",
      "2025-03-12  27.80  27.815  27.980  26.771  27.748565  63.440860  54.366686   \n",
      "2025-03-13  27.70  27.785  27.905  26.782  27.741094  63.440860  39.488661   \n",
      "2025-03-14  27.55  27.705  27.860  26.793  27.711695  47.887324  16.260163   \n",
      "\n",
      "                   %K         %D      MACD  \n",
      "time                                        \n",
      "2025-03-10  75.609756  78.723924  0.419877  \n",
      "2025-03-11  85.365854  80.790320  0.427503  \n",
      "2025-03-12  68.292683  76.422764  0.400685  \n",
      "2025-03-13  40.000000  64.552846  0.367130  \n",
      "2025-03-14  28.000000  45.430894  0.324692  \n"
     ]
    }
   ],
   "source": [
    "# Extracting the series\n",
    "series = df[['close', 'CMA10', 'SMA10', 'SMA50', 'EMA12', 'RSI', 'CCI', '%K', '%D', 'MACD']]\n",
    "# Drop rows with NaN values\n",
    "series = series.dropna()\n",
    "\n",
    "# Display the shape and the tail of the cleaned series\n",
    "print(series.shape)\n",
    "print(series.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 2: Chia d·ªØ li·ªáu th√†nh c√°c t·∫≠p Train, Validation, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4267, 10)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2987, 10) (640, 10) (640, 10)\n"
     ]
    }
   ],
   "source": [
    "n = series.shape[0]\n",
    "val_size =  test_size = int(n * 0.15)\n",
    "train_size = n - val_size - test_size # ƒê·ªÉ tr√°nh sai s·ªë l√†m m·∫•t d·ªØ li·ªáu\n",
    "\n",
    "# Chia t·∫≠p d·ªØ li·ªáu theo th·ª© t·ª± th·ªùi gian\n",
    "train_data = series.iloc[:train_size].values\n",
    "val_data = series.iloc[train_size:train_size + val_size].values\n",
    "test_data = series.iloc[(train_size + val_size):].values\n",
    "# Ki·ªÉm tra k√≠ch th∆∞·ªõc c·ªßa t·ª´ng t·∫≠p\n",
    "print(train_data.shape, val_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 3: Chu·∫©n h√≥a d·ªØ li·ªáu b·∫±ng MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2987, 10) (640, 10) (640, 10)\n"
     ]
    }
   ],
   "source": [
    "# Normalisation\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(2)\n",
    "\n",
    "sc = MinMaxScaler()\n",
    "train = sc.fit_transform(train_data)\n",
    "val = sc.transform(val_data)\n",
    "test = sc.transform(test_data)\n",
    "\n",
    "print(train.shape, val.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 4: T√¨m si√™u tham s·ªë t·ªët nh·∫•t b·∫±ng Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with params: {'timesteps': 30, 'hl': [40, 35], 'lr': 0.0001, 'batch_size': 32, 'num_epochs': 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 2.50771, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_loss improved from 2.50771 to 2.09337, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: val_loss improved from 2.09337 to 1.71586, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: val_loss improved from 1.71586 to 1.39341, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5: val_loss improved from 1.39341 to 1.16265, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6: val_loss improved from 1.16265 to 0.94984, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7: val_loss improved from 0.94984 to 0.74114, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8: val_loss improved from 0.74114 to 0.52979, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9: val_loss improved from 0.52979 to 0.37024, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10: val_loss improved from 0.37024 to 0.28262, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11: val_loss improved from 0.28262 to 0.24039, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12: val_loss improved from 0.24039 to 0.22075, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13: val_loss improved from 0.22075 to 0.20906, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14: val_loss improved from 0.20906 to 0.20132, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15: val_loss improved from 0.20132 to 0.19559, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16: val_loss improved from 0.19559 to 0.19129, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17: val_loss improved from 0.19129 to 0.18773, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18: val_loss improved from 0.18773 to 0.18468, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19: val_loss improved from 0.18468 to 0.18160, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20: val_loss improved from 0.18160 to 0.17886, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21: val_loss improved from 0.17886 to 0.17635, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22: val_loss improved from 0.17635 to 0.17387, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23: val_loss improved from 0.17387 to 0.17168, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24: val_loss improved from 0.17168 to 0.16997, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25: val_loss improved from 0.16997 to 0.16862, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26: val_loss improved from 0.16862 to 0.16756, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27: val_loss improved from 0.16756 to 0.16640, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28: val_loss improved from 0.16640 to 0.16528, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29: val_loss improved from 0.16528 to 0.16445, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30: val_loss improved from 0.16445 to 0.16362, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31: val_loss improved from 0.16362 to 0.16287, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32: val_loss improved from 0.16287 to 0.16214, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33: val_loss improved from 0.16214 to 0.16126, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34: val_loss improved from 0.16126 to 0.16070, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35: val_loss improved from 0.16070 to 0.15985, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36: val_loss improved from 0.15985 to 0.15923, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37: val_loss improved from 0.15923 to 0.15846, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38: val_loss improved from 0.15846 to 0.15773, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39: val_loss improved from 0.15773 to 0.15687, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40: val_loss improved from 0.15687 to 0.15603, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 41: val_loss improved from 0.15603 to 0.15511, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 42: val_loss improved from 0.15511 to 0.15436, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 43: val_loss improved from 0.15436 to 0.15372, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44: val_loss improved from 0.15372 to 0.15265, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 45: val_loss improved from 0.15265 to 0.15253, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46: val_loss improved from 0.15253 to 0.15070, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 47: val_loss did not improve from 0.15070\n",
      "\n",
      "Epoch 48: val_loss improved from 0.15070 to 0.14893, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 49: val_loss did not improve from 0.14893\n",
      "\n",
      "Epoch 50: val_loss improved from 0.14893 to 0.14746, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 51: val_loss did not improve from 0.14746\n",
      "\n",
      "Epoch 52: val_loss improved from 0.14746 to 0.14561, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 53: val_loss did not improve from 0.14561\n",
      "\n",
      "Epoch 54: val_loss improved from 0.14561 to 0.14364, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 55: val_loss did not improve from 0.14364\n",
      "\n",
      "Epoch 56: val_loss improved from 0.14364 to 0.14177, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 57: val_loss did not improve from 0.14177\n",
      "\n",
      "Epoch 58: val_loss improved from 0.14177 to 0.13991, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 59: val_loss improved from 0.13991 to 0.13984, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 60: val_loss improved from 0.13984 to 0.13796, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 61: val_loss improved from 0.13796 to 0.13788, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 62: val_loss improved from 0.13788 to 0.13569, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 63: val_loss improved from 0.13569 to 0.13541, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 64: val_loss improved from 0.13541 to 0.13333, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 65: val_loss improved from 0.13333 to 0.13303, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 66: val_loss improved from 0.13303 to 0.13103, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 67: val_loss improved from 0.13103 to 0.13077, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 68: val_loss improved from 0.13077 to 0.12865, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 69: val_loss improved from 0.12865 to 0.12846, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 70: val_loss improved from 0.12846 to 0.12653, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 71: val_loss improved from 0.12653 to 0.12634, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 72: val_loss improved from 0.12634 to 0.12454, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 73: val_loss did not improve from 0.12454\n",
      "\n",
      "Epoch 74: val_loss improved from 0.12454 to 0.12287, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 75: val_loss improved from 0.12287 to 0.12267, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 76: val_loss improved from 0.12267 to 0.12086, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 77: val_loss improved from 0.12086 to 0.12060, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 78: val_loss improved from 0.12060 to 0.11883, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 79: val_loss improved from 0.11883 to 0.11847, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 80: val_loss improved from 0.11847 to 0.11690, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 81: val_loss improved from 0.11690 to 0.11677, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 82: val_loss improved from 0.11677 to 0.11501, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 83: val_loss improved from 0.11501 to 0.11462, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 84: val_loss improved from 0.11462 to 0.11300, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 85: val_loss improved from 0.11300 to 0.11276, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 86: val_loss improved from 0.11276 to 0.11110, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 87: val_loss improved from 0.11110 to 0.11063, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 88: val_loss improved from 0.11063 to 0.10901, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 89: val_loss improved from 0.10901 to 0.10844, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 90: val_loss improved from 0.10844 to 0.10701, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 91: val_loss improved from 0.10701 to 0.10649, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 92: val_loss improved from 0.10649 to 0.10510, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 93: val_loss improved from 0.10510 to 0.10458, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 94: val_loss improved from 0.10458 to 0.10309, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 95: val_loss improved from 0.10309 to 0.10260, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 96: val_loss improved from 0.10260 to 0.10124, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 97: val_loss improved from 0.10124 to 0.10072, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 98: val_loss improved from 0.10072 to 0.09920, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 99: val_loss improved from 0.09920 to 0.09882, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 100: val_loss improved from 0.09882 to 0.09716, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 101: val_loss improved from 0.09716 to 0.09658, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 102: val_loss improved from 0.09658 to 0.09515, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 103: val_loss improved from 0.09515 to 0.09456, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 104: val_loss improved from 0.09456 to 0.09307, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 105: val_loss improved from 0.09307 to 0.09245, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 106: val_loss improved from 0.09245 to 0.09104, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 107: val_loss improved from 0.09104 to 0.09038, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 108: val_loss improved from 0.09038 to 0.08905, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 109: val_loss improved from 0.08905 to 0.08832, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 110: val_loss improved from 0.08832 to 0.08702, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 111: val_loss improved from 0.08702 to 0.08649, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 112: val_loss improved from 0.08649 to 0.08517, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 113: val_loss improved from 0.08517 to 0.08461, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 114: val_loss improved from 0.08461 to 0.08343, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 115: val_loss improved from 0.08343 to 0.08217, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 116: val_loss improved from 0.08217 to 0.08090, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 117: val_loss improved from 0.08090 to 0.08033, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 118: val_loss improved from 0.08033 to 0.07939, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 119: val_loss improved from 0.07939 to 0.07833, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 120: val_loss improved from 0.07833 to 0.07733, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 121: val_loss improved from 0.07733 to 0.07610, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 122: val_loss improved from 0.07610 to 0.07523, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 123: val_loss improved from 0.07523 to 0.07402, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 124: val_loss improved from 0.07402 to 0.07304, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 125: val_loss improved from 0.07304 to 0.07264, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 126: val_loss improved from 0.07264 to 0.07148, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 127: val_loss improved from 0.07148 to 0.07121, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 128: val_loss improved from 0.07121 to 0.06978, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 129: val_loss improved from 0.06978 to 0.06937, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 130: val_loss improved from 0.06937 to 0.06809, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 131: val_loss improved from 0.06809 to 0.06737, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 132: val_loss improved from 0.06737 to 0.06631, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 133: val_loss improved from 0.06631 to 0.06521, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 134: val_loss improved from 0.06521 to 0.06380, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 135: val_loss improved from 0.06380 to 0.06269, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 136: val_loss improved from 0.06269 to 0.06181, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 137: val_loss improved from 0.06181 to 0.06024, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 138: val_loss improved from 0.06024 to 0.05953, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 139: val_loss improved from 0.05953 to 0.05815, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 140: val_loss improved from 0.05815 to 0.05705, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 141: val_loss improved from 0.05705 to 0.05585, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 142: val_loss improved from 0.05585 to 0.05484, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 143: val_loss improved from 0.05484 to 0.05370, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 144: val_loss improved from 0.05370 to 0.05323, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 145: val_loss improved from 0.05323 to 0.05202, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 146: val_loss did not improve from 0.05202\n",
      "\n",
      "Epoch 147: val_loss improved from 0.05202 to 0.05085, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 148: val_loss improved from 0.05085 to 0.04986, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 149: val_loss improved from 0.04986 to 0.04894, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 150: val_loss improved from 0.04894 to 0.04800, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 151: val_loss improved from 0.04800 to 0.04649, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 152: val_loss improved from 0.04649 to 0.04483, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 153: val_loss improved from 0.04483 to 0.04392, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 154: val_loss improved from 0.04392 to 0.04288, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 155: val_loss improved from 0.04288 to 0.04232, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 156: val_loss improved from 0.04232 to 0.04121, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 157: val_loss improved from 0.04121 to 0.04051, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 158: val_loss improved from 0.04051 to 0.03927, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 159: val_loss improved from 0.03927 to 0.03865, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 160: val_loss improved from 0.03865 to 0.03754, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 161: val_loss improved from 0.03754 to 0.03677, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 162: val_loss improved from 0.03677 to 0.03591, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 163: val_loss improved from 0.03591 to 0.03506, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 164: val_loss improved from 0.03506 to 0.03433, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 165: val_loss improved from 0.03433 to 0.03348, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 166: val_loss improved from 0.03348 to 0.03263, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 167: val_loss improved from 0.03263 to 0.03186, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 168: val_loss improved from 0.03186 to 0.03113, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 169: val_loss improved from 0.03113 to 0.03035, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 170: val_loss improved from 0.03035 to 0.02960, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 171: val_loss improved from 0.02960 to 0.02892, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 172: val_loss improved from 0.02892 to 0.02801, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 173: val_loss improved from 0.02801 to 0.02731, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 174: val_loss improved from 0.02731 to 0.02668, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 175: val_loss improved from 0.02668 to 0.02611, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 176: val_loss improved from 0.02611 to 0.02545, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 177: val_loss improved from 0.02545 to 0.02478, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 178: val_loss improved from 0.02478 to 0.02417, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 179: val_loss improved from 0.02417 to 0.02350, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 180: val_loss improved from 0.02350 to 0.02300, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 181: val_loss improved from 0.02300 to 0.02236, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 182: val_loss improved from 0.02236 to 0.02180, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 183: val_loss improved from 0.02180 to 0.02116, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 184: val_loss improved from 0.02116 to 0.02068, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 185: val_loss improved from 0.02068 to 0.02013, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 186: val_loss improved from 0.02013 to 0.01957, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 187: val_loss improved from 0.01957 to 0.01911, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 188: val_loss improved from 0.01911 to 0.01860, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 189: val_loss improved from 0.01860 to 0.01803, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 190: val_loss improved from 0.01803 to 0.01753, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 191: val_loss improved from 0.01753 to 0.01695, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 192: val_loss improved from 0.01695 to 0.01650, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 193: val_loss improved from 0.01650 to 0.01594, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 194: val_loss improved from 0.01594 to 0.01543, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 195: val_loss improved from 0.01543 to 0.01499, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 196: val_loss improved from 0.01499 to 0.01457, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 197: val_loss improved from 0.01457 to 0.01414, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 198: val_loss improved from 0.01414 to 0.01372, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 199: val_loss improved from 0.01372 to 0.01333, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 200: val_loss improved from 0.01333 to 0.01290, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step\n",
      "   timesteps        hl      lr  batch_size  num_epochs       mse      rmse  \\\n",
      "0         40  [40, 35]  0.0001          32         200  0.003551  0.059593   \n",
      "\n",
      "       mape        r2  \n",
      "0  0.031415  0.956033  \n",
      "Best parameters: {'timesteps': 30, 'hl': [40, 35], 'lr': 0.0001, 'batch_size': 32, 'num_epochs': 200}\n",
      "Best RMSE score: 0.05959297616303428\n"
     ]
    }
   ],
   "source": [
    "best_params, best_score, results_df = grid_search_rnn(train, val, test, param_grid)\n",
    "\n",
    "print(results_df)\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best RMSE score:\", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 5: Hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi b·ªô tham s·ªë t·ªëi ∆∞u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.00731\n",
      "\n",
      "Epoch 105: val_loss improved from 0.00731 to 0.00706, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 106: val_loss improved from 0.00706 to 0.00673, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 107: val_loss improved from 0.00673 to 0.00647, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 108: val_loss improved from 0.00647 to 0.00633, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 109: val_loss improved from 0.00633 to 0.00590, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 110: val_loss improved from 0.00590 to 0.00584, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 111: val_loss improved from 0.00584 to 0.00555, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 112: val_loss improved from 0.00555 to 0.00552, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 113: val_loss improved from 0.00552 to 0.00535, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 114: val_loss improved from 0.00535 to 0.00531, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 115: val_loss improved from 0.00531 to 0.00520, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 116: val_loss improved from 0.00520 to 0.00514, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 117: val_loss improved from 0.00514 to 0.00500, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 118: val_loss improved from 0.00500 to 0.00495, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 119: val_loss improved from 0.00495 to 0.00480, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 120: val_loss improved from 0.00480 to 0.00473, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 121: val_loss improved from 0.00473 to 0.00462, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 122: val_loss improved from 0.00462 to 0.00456, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 123: val_loss improved from 0.00456 to 0.00447, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 124: val_loss improved from 0.00447 to 0.00435, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 125: val_loss improved from 0.00435 to 0.00427, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 126: val_loss improved from 0.00427 to 0.00418, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 127: val_loss improved from 0.00418 to 0.00411, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 128: val_loss improved from 0.00411 to 0.00404, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 129: val_loss improved from 0.00404 to 0.00396, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 130: val_loss improved from 0.00396 to 0.00392, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 131: val_loss improved from 0.00392 to 0.00387, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 132: val_loss improved from 0.00387 to 0.00381, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 133: val_loss improved from 0.00381 to 0.00376, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 134: val_loss improved from 0.00376 to 0.00372, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 135: val_loss improved from 0.00372 to 0.00368, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 136: val_loss improved from 0.00368 to 0.00368, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 137: val_loss improved from 0.00368 to 0.00367, saving model to 10Var-hpg-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 138: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.00367\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.00367\n"
     ]
    }
   ],
   "source": [
    "timesteps = 30\n",
    "hl = [40, 35]\n",
    "lr = 1e-4\n",
    "batch_size = 32\n",
    "num_epochs = 200\n",
    "\n",
    "model, train_loss, val_loss = fit_model_3(train, val, timesteps, hl, lr, batch_size, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 6: ƒê√°nh gi√° m√¥ h√¨nh v√† tr·ª±c quan h√≥a k·∫øt qu·∫£"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1. V·∫Ω bi·ªÉu ƒë·ªì train_loss v√† val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_error(train_error, val_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2. ƒê√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step\n"
     ]
    }
   ],
   "source": [
    "mse, rmse, mape, r2, true, predicted = evaluate_model_3(model, test, timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.009244120548516081\n",
      "RMSE = 0.09614634963697832\n",
      "MAPE = 0.05267980001025357\n",
      "R-Squared Score = 0.8887704466101446\n"
     ]
    }
   ],
   "source": [
    "print('MSE = {}'.format(mse))\n",
    "print('RMSE = {}'.format(rmse))\n",
    "print('MAPE = {}'.format(mape))\n",
    "print('R-Squared Score = {}'.format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3. V·∫Ω ƒë·ªì th·ªã d·ª± ƒëo√°n vs th·ª±c t·∫ø"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAHMCAYAAAAzqWlnAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAvaxJREFUeJzsnQV42+bahh+zHaamTZkHHTN3zIwdnzHDGW//2Tlj5nU74+0MOl7HWzumjrt2XWFlbphjtvVf7yd/suzIiZPYiZ2893UlkmVZlj/L0qMXTYqiKGAYhmEYhumnmPt6BxiGYRiGYdIJix2GYRiGYfo1LHYYhmEYhunXsNhhGIZhGKZfw2KHYRiGYZh+DYsdhmEYhmH6NSx2GIZhGIbp17DYYRiGYRimX8Nih2EYhmGYfg2LHUbj66+/hslkwk033YRMIlP3KxMZPXq0+NPzwgsviPGjabqg7e+5554YKAy0z9sR9Luk8aDfabKsXLlSvOYf//gHMg3+bvsnLHbSTCgUwtNPP43JkyejpKQENpsN5eXl2GKLLXD22Wfj/fff7/ULU7qRJzL9n9VqxeDBg3HIIYfgk08+QTZDYkL/2cxmM4qKirDLLrvgscceQzAYxEAQUUxmig76O+200xKu980332jrpfM7HciioS/FXH19Pe666y4x9nStsdvtyM/Px6RJk3DGGWfggw8+QHyXKHlDqf+ja9XQoUNx9NFH49tvvzV8L/p8nV2v5HHZ1zer1j599wEgdA499FB8+umn4mJIF/rhw4fD7/dj/vz5mDZtGhYtWoTDDz8c/ZHCwkJcfvnlYt7r9WLu3Ln4+OOPxd/DDz+MSy+9NKnt7LDDDli4cCHKysqQSVx22WXie6XvecWKFXj77bfx448/4osvvsA777yDTOGoo47CTjvthIqKirS9B30/OTk5GChk8uelG4u33noLjzzyiDg+46GbL1qnL0X5sGHDxBjSOSLTyOTvtjPo5vn0009HY2OjELIHH3yw+N37/X4sW7YM7777rhAmxx57LN588812rx81apQm0NxuN37//XdMnz5dvO7111/Hcccdh2yFxU4aefXVV4XQ2XLLLcXdVPwPmw6mn3/+Gf0VOtHGq/nnn38eZ555Jm644QZh2UrmpELrbLzxxsg0SMjp74yvv/56bL/99uLkQN83WfMyATru0n1RycTvZ6B+XrrBoovTK6+8gosuuijmuYaGBiHKDzvsMHGc9hVkNcjUMczU/eoMusk65phjhJB95plnhBWHrM56vF4vXn75ZcycOdNwG3Q+iz9nk5WIzm3XXHNNVosddmOlkVmzZokpKWWjiw1dxPfaay/tMZkd6QAlaKo3KZJZVNLU1CQOvo022ghOpxPFxcU44IAD8PnnnyfcFzq46QRHZk2Hw4ERI0bgiCOO6PA1+h8I3QnQftDJMxwOo7vQWOTm5qKtrU1Yt+Qy2vby5cvx6KOPChefy+XSTOAdxeyQyfb//u//sNlmm4nxpHEmcXndddeJ94hfl8Ztk002EdundffZZ5+EP/yuQmZiuc+//PJLO7ckCV96nt6XlknoDvvxxx8X1peCggLxObbeemtMnTrVcKzJBE3P0fvR9093yRdffLE4LozoyDW6du1aYWGbMGGCGBNytZIl7dZbb40Z+1WrVok//TGpN9Encll05VjVf89z5swRllASzDQeJBzl7ykVcV5Gbjm6+yVryDbbbCP2k96X1jH6nRh9Xn3sCllWaBxpGzSmU6ZMwbp16wz35ddff8X+++8vXA30/e+7777CQtidWBjiwAMPFBZkuuDF89JLL4nf8znnnGP42s7c6Mm4puQ24l1m+u/DyM1D+03LyAJsBFkW6PmrrrpKW0aWB7Kw0m+expmOMTqWr7zySiHsOvp8iX6PRp9x/fr1uOWWW7DrrrtiyJAhwjVELp6TTjoJCxYsiFmXPuOYMWPE/P/+97+Yzx8/rjNmzBDWF7Ja03l53LhxuPrqq4VlpiuQdfmCCy4Q5xI6hs8666x2Qoeg8aGbTPIqJAttS35ntbW1yFbYspNGSktLxXTx4sVJrU8/fDq5v/fee+IEu9VWW2nPSXM0/QjoB0c/MLIikHWBDsA33nhDnDD/+9//4rzzzovZ7n/+8x/xQ83Ly8ORRx4phA79eOniQSqfTq6JoBMGudl++OEH3HnnnUJE9BTpL9afYAg6aX333XfiIkcnAIvF0uF2yHVEYpEuwttuu634sZM4oPF+8MEHcf755wthRdA6dAKjH+zuu+8uTqwkhj788EMx/+STTya8AKTis9HFj06uBx10kNgv2h8iEAgIEUonPRIEdPKkE9JXX32FSy65RFj+6AKlh75zOqGRefrcc88Vd8l0zNC6dMGmE3Ey/Pbbb0J4kAjcY489hG+erI10bNEJ+8YbbxQXezp+HnroIe29Jfrj04juHKtyv+655x7svPPO4sS8evVqYY0gYUoiiMYpHdDvj6yxJJwp5oXEH/1Ovv/+e/HddfQ70UPCldwJ9LshkUbfC12o6SJO+08XNQnFQtBY0MWKxp8udvPmzRPH9d57792tz0G/G7Ke0m+exnK77baLcWHRhTjZz9Id6LigY+bmm2+OcYsQHQklcr/Q7+DFF1/E/fff3+55Eg6Efnv0echCReNMn4l+/ySAHnjgAREbSGNPIjKeRL/HRND3RBYO+l7IekLn0iVLlojt0HdN50cSXPIz0rFPrnpaRudc/dhIaHzod0YijaxxdCP6559/4r777hOufhK8JH6TgQQx7Q+d2+m77wyrtXuXfjrXZC0KkzZmz56t2Gw2xWQyKaeccory9ttvKytXruzwNc8//zxdLcXUiHPPPVc8T9NwOKwtX7x4sVJQUKDY7XZlxYoV2vIZM2aI9ceMGaOsXbu23fbWrFmjzX/11Vdi3f/85z/iMe3rJptsIj7Dyy+/nPTnpven7YwaNardc88++6x4Ljc3V3G73WLZ6aefLpYNHTpUWb58ebvXxO+XZOeddxbL77jjjnavqampUTwej/Z48uTJ4nt49dVXY9ZraGhQttxyS8XpdCqVlZVJfT76XPS++nEm/vrrL8Xlconnvv3225jvk977k08+abct+kz0/MUXX6wEg0FtOc2feeaZ4rl3331XW/7DDz+IZePGjVPq6uq05fRZd9ppJ8NxNzqmfD6fMnr0aLH8lVde6fC4kJ/Z6PuU0HZojHtyrMrv2ej4f+KJJ8TyCy64QEmGRMdMos/T2NgovqNtt9025nuQ1NbWdvp55XeZn5+v/PnnnzHPnXjiieK5119/XVsWCoWU8ePHi+Uff/xxzPr//e9/tbGgz5IM8v2ffvpp8ds1m81i7CU//vijeP62225TAoFA0sdKsp87fj+N1o0/R9BvX38MFxYWKoMHDxb7p2fDhg2KxWJRttlmm5jl9DmNvq9nnnlGbP+uu+4y/HyJfo+J9ruqqkppbm5ut+6cOXPEuezAAw/s9PPp+fLLL8XzdA6jc5DRPl5++eVKstx8883iNXSd6Q5fRX4vRt/XrbfeKp7bbLPN2j0nz92Jjhf98ZHot9hbsBsrjZArgiwnlIVEU7ojoDtlsvhQ0ChFxXcFumun7dBdBVlZ9NYDMt2SO4LWoTsjCbmFCLpTIndHPGTuNoLuQOnOmkzvdId08skno6vQ3Q3dudAfWYTIWiNNonfccYe4c9ZDPmFp/u0MunujOx+6U7r22mvbPU9mYbKQEHRHTeZ0Gn9yJ+ghixndYZFpn6wHXYGsHdICcsoppwjrhcfjEd8tWY/0kKWOLEh66C6Uvh8yi5MlSm/Jonn6zug7ptgLfcwTQa47uiOU0GelYyJZ6NgjKxdZH8ialOxxkc5jVULWoPgsFrpbpbtR6R5MNbR/dJ0jq4uR+V9aaZOBPtvmm28es0xaDfX7T5bVpUuXCmsBWRj0kMVu4sSJ6C5kUSGLEVmqpDuXrCB0XElXeaZBx/Dxxx+PqqoqYeHRQ8cSWb/I+hP/OY0swHS8kFUkfjsd/R47gqwuRhYistyQBY4ssWSlTRayzMrvJD6InI59Oq/pf/edUVlZKaZG53jipsh5WP9n5Cqjc4J8ns7H9Nno/EZjSdbvbIbdWGmGfrx08aMfA5nD//jjDzGlAEL6I3O53sfdEX///bdwM9DFQH+hk9CBedttt4n3kPz0009i2135YdP+kRmYftxkvpXm2a5C8RokJAg6IdE+00md4ktI+MRDMQ7JQp+LIDeM0cVJD4kiuT9GMRw1NTVaFkZXIDM1QeNLF3WKNSLRQ2bxZD4budvIhUQXf/rejCBBqN+v2bNni6lR8PNuu+3WqesvfvziL7KpojvHqkTvdtGbz+mmwSgOIxXQyZzciSQC6UJDwpgE64477tjlzByj/Sf3AqHff/nZ6XuLh45pKmWQrAvcCBJY5Kp57bXXRGApudLIRUyxJplaHoEu9CQAyGVF+yqhx3QMxAtzEhh0EabPSO5S+o3r49wSxUl15Vwj+eijj/DEE08I1yC5Y+PHkJYlm/FI5yT6PJQRZZQVRTcCdF6qq6sTQpuuEfq4TekuSza1/+bIedgobEIPufPi16X4tS+//LJTt3Wmw2KnF6CDmu6y6I+gOxSyItDdB93ZkhjS+3UTIQNQE/2g5HK9Yqd5OljjrSgdQSfhlpYWcbLtSWYC3XXF/0A7giwcySI/Y6I7GT10wiA+++wz8ZeI1tZWdAWKGUq2TonRZ5P7Rb52o5OR0X7JY4Au/PGQ5SPZ9PyujF936M6xKjFKl5afj3476YLEwN133y2CNynmRFobKDif4iiMxtwIo/2XMRL6/e/ou+xoebKQeKNtUKAyiQKy8KQiLi2d0DmHLFoUB0PCkM5dJPD/+usvcY6MP75POOEEEbMzduxYYa2h35mMiSLLq8/n6/G5Rt7YUMwZ7c9+++2HkSNHChFMNzp000rW40TvZQT99kksdfS7l799KXbIOh2PFDvy81CMmRGKrq4OiWuKMTKCbqJkQDzdiNF1im5O6ViiQPr4cZM3mh0lrcjnOrspTTcsdvoAuvsmiw8FItLdLanmZMSOzOiSJst4NmzYELOePPHSD4vcK8kKHjq4q6urxV0MuTnox9wVsdRdkrFuxV9QEt256ZHj0ZXaPr3x2eR+kdhNti6PfA2Z+ukEr4dOnnR3mYwLqivj1x26c6ymEnliTWTBIJEVL0roGJcm/DVr1girJl1kyIVCop2C51OJDD6l79KIRMu7cpNFLisKrKWsOzouOrPkdTRuXc0Q6i5k7f7Xv/4lxCdZSWVgcrwLiywsJHQoMJlc7fqgW7rAUpB7Ks41NBZ0TNCFnoRXvICXluOuQMc97SMJimToLCOPLKhyPdpuKoRFSUmJEMdkZaJrwoUXXtjuPCV/v/LGzQiZwZXoJqa34JidPkT6gPWqW7ohjO5gKQuF7iboLsLoxEOuMoJSZyWUzkzbJ3N2stCJgDJl6E6G0rLJnByfxt3X0OciyCffWSq8XDfVF6ueQlYzOgGQSylZf7/8bo3u8sj9mKzlQ45JstWs6bjsilWlO8dqKqE7cIJESzwUJ5MoTV/vdqI4NTq+xo8fL8a2oxN6d2P6CNp2PHRMdyXVPhGUzUa/ZxI7ZEnuzM3Z0biRuOgKdMHtjiWOxA69lkQO/S4o7ogsOnq3lvweCbohi88uotgousFLBXSxpmOYrE7xQocsL9K1rKej87j8/ZHlSpbf6Clk4aHjlL43GdeXKs4//3xR5oKEZbxFSIY4dCT45HPdDYdIFSx20gj9SMltYnQxpjte8k0TlPYbHwhJ6bbxUEoxnYDJxURBY3qoOiYFvdHd3Kmnnqotp/RlgupOGN3Fd3RnT0GzVCOFLkwUG9Pc3IxMgVLN6eRDgdTkeoiHLkwUdCxjKCj+gu5KnnvuOcPtkZWNrFm9CZ2g6fshKwdZnIxOzvScvo6HDNy9/fbbY+4K6bPSd5UsZJYmFxy5C+g4jYcujnrouKQYgmQvIN05VlMtJMlyQin5+u+V9t/IukefjY6BeEjk0wWNvqtkU/qThe7GKdWcfl/xovOpp57qUbyOhLZPNzp0oUrGqkm/FRIa5MqjmCsJHWsUsNoV6JgxEk2dQUKTYrroJoCssfTdUKxOfNqzdCHHWz3o+44vptgTKDiZhDslRehdyiTEqFyGUe0ZEo0kMo3O48Q///lPMSXLiZHriY47GVeXDCSuyBIvzykkeIyuO4FAIOZ7TXbb0t1GiRF6yCpN1h06j1BRw3hoP+gcTcehUWxab8JurDRCNR7ox0rmT/qiZaYRxXpQsBudeMnPTDEBEsqAoh8W+Zvpgi19pHQA00FFJmmyUFBROfKhUiaHrF1CFxZars9oojghMgmTu4yK6ck6O2QipztKusPoqK8JZU1R3ALFMJCvmk6c8u6vryH3At3RUDVm8i3TPFmxKAaGLFLUikOeEOnkTSdQygajCy0FnpJVhS7qVNuCYgLoDoRObL0JCQGyftCJioJjaR8pjoZO2PQ56E6KhM2mm26qXSDpWKAsLqoHQ8eOrLND30uyAZJ04abASDo+6EJCQZ50LJBoooBoOnHpXRlU44aONwp0J3FOcRF0p0aiKRFdPVZTCY0JXYioOCJZUOikTJ+Hbj4oQJf+4kU/rUdZVBRoTr8REvdUh4luTEgoGGXj9AQSFRRPQ2NK1gkKiqaLAh2PtJ/kciIR1FOXhIwVTAY6fkikUm0nCkglawqNA9V9oe/dKKA8EXTMUOAwHSNkwaPvhLahv7lLBLmsqJAj/bbl43go+5F+D3QTQzc+dI6l8xqNGVkW47/j7kLjT98/Hc90fNA5m1w7JFJJBNJxLS2VEkpYoHMMHf80nhSHRKKBvmc6vmhsZGViSlCghA36LZCYoiBhstzS5+mKRZ62SXV/aKxknSWKwaFx8Hq9QlTRmNJ1hfahK24lqgFFxwPtF1k76eaXoGsSXT8oy5WOMzqWadtk0SLrmuwcQJllySZPpI0+TXzv56xevVqZOnWqcuSRRyoTJ04U9TeoZs2QIUOUgw46SHnppZdErY14qP4D1Uyh+g2y1oa+HgnVZbjmmmtEjQ6qVUK1Kfbdd19RUycRH330kXLAAQcoxcXF4jXDhw8X+/XFF18kVZvknnvuEc9tvfXWooZNd+vsGCFrNcTXrUlmv6j+CY0Fja/D4RBjQXVzbrjhBqWtrS1mXaqTcfvtt4taHTS2VFuHas0cfPDBypNPPqm0trb2qM6OEZ3VLSGoBs2LL76o7L333uL7oWOEag7tuuuuYn/pOIpf/9FHH1U23nhj8V1WVFQoF154oagVY1QPp6N9WLVqlahdQ+NA71tSUqLssMMO4n310Nicf/75yrBhw0S9k/gaIolqdHTlWO1qbZzOoHG68847lbFjx4rPNmLECOXqq68Wx0X8tmg/qVbJXnvtJcae9pV+p/SZpk2bFlMnqKv1Zjqru/LTTz+JMcnLyxN/++yzjzJr1izloosuEq/5448/ulxnpzMS1dkhvF6vctVVV4nvmsaNajpRLSv5mmQ/N9WmofpC5eXlouaP/rvtrA4NfUdUiylRfRcJ1Zqi45c+B/3+6bu+/vrrDb/jZH+PRp+RPvv9998v6o7ReYNqAVFNG6rzk+j8tWTJEuXQQw8Vvymq62P0vt99951y3HHHid8wjXVZWZk4f/3zn/9Ufv31V6U70DmRvq/dd99dbM9qtYrjis4Xp512mvLBBx+0u+50VGdH8v7774t1tttuu3bPzZs3T/nHP/4hziP0PVC9MTonU/2wzmrL9RYm+te3cothGIaJh6wWZB2m+CJZCZxhmO7BMTsMwzB9BMVPGAVwk2uAApTJNcBCh2F6Dlt2GIZh+giKK6NYIYqHo2waiiuShUcppoIED8XaMQzTM1jsMAzD9BGUfkxdrimQkwKhqTAdJSVQ7RjKfKGAZYZheg6LHYZhGIZh+jUcs8MwDMMwTL+GxQ7DMAzDMP0aFjsMwzAMw/RrWOwwDMMwDNOv4XYRuqyIRB2Se8KgQYNEbxemY3ickoPHKTl4nJKDxyk5eJwyc5yoF1iy7YtY7EQgoZNs5+lkoUZwctuc9JYYHqfk4HFKDh6n5OBxSg4ep/4xThkldqgzLzUPo6Z81KiQmqedcsopHTZ0o8Zm3377rdZdd+zYsTjxxBNFgS6GYRiGYZiMEjsLFiwQ3VSpkBZ1TX311VdFt+4HHnhAdN5O9BrqIUNdbmX3Z/makpKSXv8MDMMwDMNkFhkldqhiqJ6LLroIZ599NpYvX45NN93U8DWXXnppzOPzzz9fNM+bN2+eaG/PMAzDMMzAJqPEjlGTPCIvLy/p11C5dfIZJnoNxeXoY3PIz+hyubT5VCK3l+rt9jd4nJKDxyk5eJySg8cpOXic+sc4ZazYCYfDovMvuadGjhyZ9OteeeUV4b7afPPNE8YFvfXWW9rjMWPG4O677xZR5OmCet0wncPjlBw8TsnB45QcPE7JweOU3eOUsb2xnn76acyZMwe33HILSktLk3rNu+++K2J2brrpJowaNapLlh1Kl0t16jltm754avCXocOcEfA4JQePU3LwOCUHj1Ny8Dhl7jhR6nmyhoqMtOw8++yzmD17Nm6++eakhc77778vxM6NN96YUOgQFMRMf0ak6wui7fKPpHN4nJKDxyk5eJySg8cpOXicsnucMqqCMg0QCR1KP//3v/+N8vLypF5H1py3334bN9xwg8jkYhiGYRiGyUixQ0Lnu+++w2WXXSZcS42NjeLP7/dr60ydOhXTpk3THpM15/XXX8cFF1wgxJF8jdfr7aNPwTAMwzBMJpFRbqyZM2eKKcXc6Lnwwgux5557ivna2tqYaO/PPvtMxNpQXR09xx57LI4//vhe2W+GYRiGYTKXjBI7b7zxRqfrxAuhxx57LI17xDAMwzBMtpNRbiyGYRiGYZhUw2KHYRiG6TLhsA+hUEtf7wbDJAWLHYZhGKZLeL1/YfnyLbBs2aZoa/u2r3eHYTqFxQ7DMAzTJdzu7xEOt5J9B273V329OwzTKSx2GIZhmC4RDG7Q5n2+JX26LwyTDCx2GIZhmKTxeH5DY+Mz2mOy7FRVXdOn+8QwncFih2EYhkmK1tZPsWbNEe2WNzW9Ar9/aZ/sE8MkA4sdhmEYJql2PrW1dyV8vrn57V7dH4bpCix2GIZhmE7xeGbB74+Nzxk27FUUFByniR1FCffR3jFMx7DYYRiGYWJoa/sSS5ZMREvLJ9qyxsYXxNTp3FpblpOzG8rL74LZXIBgcB08nh/7ZH8ZpjNY7DAMwzAxrFt3KhSlDRs2nC0eBwLrRLwOMXjw/RgyZCqGDZsGk8kMs9mJ3Nx9xHMez+99ut8MkxW9sRiGYZjMi9VpanpZ1NRxuXaGw7GR+NNjt48X00BgeR/tJcN0DFt2GIZhGI1AYH3M42BwPZqbp4v5oqLTDV9jt48RU79/RS/sIcN0HRY7DMMwjEZLiypsJK2tMxAMrgFgQW7u3oavsdnGiSlbdphMhcUOwzAMI6BaOfX1j8csa25+TUydzi1gNud2aNkJheoRCjX2wp4yTNdgscMwDMMIampuQzjcCIdjEoqLzxfLfL75Yupy7ZTwdSSCLJZSMU9ZWQyTabDYYRiGYQR+v+qGKiu7Dna76pqSOJ1bdfhai6VcTIPB2jTuIcN0D87GYhiGYQShUJWY2mwjYTLZY54jN1ZHWK1l8PtpGzVp3UeG6Q4sdhiGYRiEw20Ih1vFvNU6BCaTQ3vObC6C1Tqiw9dbLIPElC07TCbCbiyGYRgGwaBq1TGZcmE258FqHQqHY1OxjLKwTCZTp5YdIhSq7nA9r/cv1Nbeg2CwLmX7zjCdwZYdhmEYRhM7VutgMTWZLBg58hNRZ8dqrej09dGYncRuLKrDs3r1AWI+EFiNioqpKdp7hukYtuwwDMMwCAarY8QOYTJZI/E7tk5fH7XsJHZjud3faPMtLe8jGKzs4V4zTHKw2GEYhmG0YoJ6sdMVZMyO2/0tqqquQUPDM1AUv2G2l0oIXu+fPdhjhkkedmMxDMMMcDye39DW9pmYz83dv1vbsNsnaPNNTa9oNXqGDHlQW+73L4t5TTC4oZt7zDBdgy07DMMwAxwSO0ROzh4oKDiiW9uw2YZh5MiPYpa1tX0d81iKHbt9EzFlscP0Fix2GIZhBjg+n+pOoq7mPYEKDw4efK/I5CKo5o6iBLXU9mBwrZjPydlNTFnsML0Fix2GYZgBjtc7N6kqyclQWHgSxoz5WTQOBRQtO0u18iiw2UbD6dxMLAsEWOwwvQOLHYZhmAEMWVwCgZViXoqQnmIymWG1Doqpu9Pa+qmY5uUdoFl+2LLD9BYcoMwwDDOAkXE01MjTYilJ2XYtliEitZzigVavPlhbnpd3iPY+VMNHUZROCxYyTE9hyw7DMMwAxudb0i6bKhXIFPaampu1ZS7XjnC5toXNNlz03lIUryguyDDphsUOwzBMLxMOe9DU9LpwIfU1fr8UO+NTul2rtVyrp0NQccLBg+8T81Sk0G7fSMz7fH+l9H2ZzCEUakRr6xfCetfXsNhhGIbpZRoankBV1RVYt+60vt4V+P1L02LZsdmijUOpsejo0bNgt4/Vljkcm4spi53+S03Nf7B+/WloaPhvX+8Kix2GYZjeprHxZTH1eH4S/aL6EhmcbLONSel2KRBZ4nBs1i4ux+mcJKYez68pfV8mM1AUBc3Nb4n52trb+9y6w2KHYRimF1FP+gHtcXPz26ivfxShUH2f7EsgsKadJSYVkFvMZhsl5gsKjmn3fG7uvuTQgsfzI9avPxvhsDel78/0LX7/4pjHgUBs9ezehsUOwzBML0IZSKFQnfa4vv5B1NbehbVrp/TBvjQgHG5Ji9ghhg9/ExUVTyEv77B2z1GQsmxN0dr6Caqrr035+zN9R0vLBzGPA4H16EsyKvV8+vTp+OWXX7Bu3TrY7XZMnDgRp5xyCoYOVWsyJOLHH3/E66+/jpqaGgwZMgQnn3wyttlmm17bb4ZhmGTxeKjgXnuoj5TPtwgOx8a9ti9e7wqtiafZ7Er59qmFBP0lYvDgO1FbW4zm5teEy6Ok5LKYuB4mOwmFmtHQ8FTMsr7ucJ9Rlp0FCxbggAMOwO23345//etfCIVCuO222+D1JjZv/v3333j44Yex99574+6778b222+Pe++9F6tXczojwzCZR1vb52JaWHhKu+c8nl8MXyNbLqQar3dl2qw6yaanDxlyP3Jz9xaPGxtf6JP9YFKL2/0dFKVNxIEVFBwrloVCVehLMkrs/N///R/23HNPjBgxAqNHj8ZFF12E2tpaLF++POFrPv74Y2y11VY4/PDDMXz4cEyZMgVjx47Fp5+q1ToZhmEyBYqRaWv7RszTRSAnZ6921h09FMeyevWhWLJkNOrrU5/R4vEs09LC+5KiorPEtLHxeXi9nJ2V7bjd6jFOItZqrRDzwSCLnYS43W4xzcvLS7jO4sWLsfnmagqjZMstt8SSJWrtCIZhmEyBGmOGw40iMNfp3BIVFVNRUHCCqCpM+HwL2ll6vN4/RE8pCmRONW73wrTU2Okqubl7RuJ3wiJ+h8lu3O4fxDQnZw9RdiAT3FgZFbOjJxwO44UXXsBGG22EkSMT33U0NjaisLAwZhk9puVGBAIB8SehdEiXS/VVp7pkudwel0LvGB6n5OBxyv5xCgbVzCfqDWU2O8RfRcWDoopxa+tH8PlIfFD7BPU+1OuNxvf4/X8L14DZnPjmryvQ+LS1qWLH4ZjYpfFyfPUVwoWFCKQwNjI3dze0tc2E378oo767TD6eMglTZHzC4QatnEFOzg5wu4OaZacvxzBjxc6zzz6LNWvW4JZbbkl5EPRbb6m5/8SYMWNErM+gQWrTunRAQdNM5/A4JQePU/aOU1VVs5jm5o5HRYVq3ifC4TKsXGmGonhQWmqGw6E+V1k5W/fqMHJy1qC4WI1vSYVLbckS1ZI0bNiuyM2N7k+HVFYCJ58sPxBQLisl9wync1dUV9NFcYn47jJNXGTi8ZRJKIqCpUuvxNq1D4jHLtd4DB++MZqbm7B+PT1fE3PM9zbWTBU6s2fPxs0334zS0tIO1y0qKkJTU1PMMnpMy4046qijcOihh2qP5Q+KMrmCwdQGAdK26QdSWVnZ5wWVMhkep+Tgccr+caqr+1NMFWUINmzY0C5Yl7qAr1s3W2RkVVf/C01N34rnHI4t4PP9ibVrP4XXu0lK9oXeKxSitHMrmppy4f5hJgqvuQYt118P/x57JHyd7fffURaZb7nrLrRefjkF/aRgf9QbTq93GX77bR8MH/4KMoFMPp4yCZ9voSZ0VMrFMR4M2uF0biOy8uKP+Z5itVqTNlRklNihA+m5554T6ec33XQTypO4Y6D09Hnz5uGQQ1SfN/Hnn39iwgTj0uc2m038JXr/dEDb5R9J5/A4JQePU/aOk9+/SgsIjt83q3W4ECCBwFp4vfPR1PRaZN2xKCg4GjU1f6Ku7l4RxDx06NMp2JfVWr0bwIbi886DdeVKlE6ZgvXr1iV8nbmmRpvPf/BB5Lz0EupfegmBLbbo0f7IrutUXLGt7SuEQl7h5ssUMvF4yiTc7tiSCvn5h4nxslgGY+RIteZOX46fOdMsOt999x0uu+wyEUdDcTf05/f7tXWmTp2KadOmaY8PPvhgzJ07Fx988IGoz/PGG29g2bJlOPDAA/voUzAMw3Tch0pWFtYj69EEAutiUtCLik4Td8aS1taPU5KKLgNGZQCpuS5a6LAjzORr0mGprcWggw5C3uOP0+19j/ZpxIj3tflAIHEWLpNZKIqClpb3xHxx8QUYOvQ5FBaehEwioyw7M2fOFFOy6ui58MILRUo6Qanoel8uBTBfeumleO211/Dqq68Kn+DVV1/dYVAzwzBMb0Np5D7fPDHvdG7V7nmrVRU7weA6LQW9sPBUFBWd2U7cUIsHu71nvaxkKjC5zwjF4aCyt+3Wsyxfjrwnn4TJ64WptRWuBGU9Cm6/HaHSUnhOOKHb+0SfyencFl7v7/D5FsPhSI3LjkkvDQ1PasUy8/L2g8u1IzKNjBI7ZJXpjHghROy8887ij2EYJlMhoaMoflGt2GYbndCyQ7EPsq9Qaek/YTJZxF95+R2orr5BLPf7l6dA7MRadhSn03C9vKefRu7LauPSzrAtWABPj/aKBM9GQuxQyj01SnW5tjPsrTVQcbu/F73UqDZRXp7abqOv8fnmimlR0Z7IydkpI919GeXGYhiG6a94PL+Lqcu1vWGmkd2uxhlSY0zKvKLH0upCFBWdrtXjSYWLp51lRyd2rEuWwNTWJuZt81RrFOHbYYd226EU9Ma771Zft6znzR6lNaex8Wk0Nb2IyspLe7zN/kI47EFl5WVC8Kxff4bmFu1rgkE1jmvo0PORqbDYYRiG6QWktcbh2NTweZdrB5jN0SzSwsIT260j+0aRZSdVYscWKoJ1/nzaMe258j33REkkvdzc0CCm9U8+icYH9Nk2KorFguD48SkTO/n5B7d/DyVaG20g09z8Zkxxvvhmm31FMHIs2e2Zm57PYodhGKYXkHfhiaoVm0xWrY+Q3T4RBQXtu6DLwGaK2ekp8qKZ/9InKN9/f9hI8Ohw/PorlXyGZe1adf+33hqh0e3db7599kFw3Dgxb1mzhnLHe7Rf5FajqtKx+xobFD1QoWapeitga6sa55oJlcEJu73v6uh0BosdhmGYNEMxDH7/sk5bMwwa9H8YNuxVjBz5MSyW2MrwsUHMqgDpLqEQVblVU8+LX/os4XqW9ethCgah2O0IU1E9kwm1r72G5htuQNU336DlssvQdNNNCJeVIVxQAJOiiPR1QxQFRf/8J4ouvph8dR3uX3n5nRg9+lvd501tfZa+pK7uAaxde0qkxlHy+P0rRCwTXbaHDJkqllHtJfou+9q1Fg6rn4UtOwzDMAOYUKhW64lFnaATYTLZkZu7B8xmtYVNPGpNHLLsrO1REGhLy4ckIZCXtxWcHfRntC5U20mEhg+nQjhi3r/77mi96CKExo9HyzXXQKECriYTgqNUq5N18WIR89NuW0uXIueNN5AzfTqKblADrRNB9XXs9nEZ01cpEfQdUH2ZcDj5lPu6uvvhdn8laiZ1BdkbjfpNOZ2baaLZ4/kVmeDCMplcsFjykamw2GEYhkkzslcQFQ40m42znpKBemoR1FaCehB1B0UJobHxOTFfnntEh+va/6AmpEAwiVIeocg6JRdcIGJ+8h56KHZb5BaL4HrrLVhWqQUWOyLTxU5Dw+NYu/bopIWL3prT0jK9S9aTpqZXxbzMTJPp3RSsTGUN+tqFZbWWZ1yLDz0sdhiGYdKMvFjbbD2LaSChRKnr0rrTHairOAVLUzB0RUv7thDh/Px2YkcKmY6Qlh1Jwb33wlxfbyh2TOEwXO+8k5ViJxx2o67uEdTU3Iza2jvEsoaG/yZladN/DqoUHQwmV8iRhE4oVCncenl5B2sB7URj47NYsWJ7sb2+IBhxMZLYyWRY7DAMw6QZeZGj0vk9Re/K6g5u93diWlh4PGz/aW+RqP7mG/i3Uose2mbPNhQyRhgJIsvKlSi86ioUXXIJ7N9/L5b5IjXRrMuXJ23JokKLmQK5oerq7kZDw1Mxy2UhyI6Ijz3y+9VGrJ2hliOg8gP/0CyDLtdO2vMkdNzun9AX+COZgR25ZzMBFjsMwzBpJlrTpucBnFZrRcw2u4rH85uY5tUPAwyqISs5OQiXlIh5c6TWTjKWnZCBICq64grkvvoqct55B9b16xF2ueCOVFi2rlYDpDvCblezv/z+BEHPvQxZbxobX4xZZrWOiBGRXRE7VEAyGXy+v8XU4disneiVyIDz3savBd6rGXmZCosdhmGYLBI71DCTCIWSc4HoCYWo16B64cxfmactl9YWTeyUqu8hSSZmh6xBtJ5ijRbmt8UFKvsmT0Zwo43Uz5GE2JHWAop5yoSqvE1NL0NR3NrjESM+QHHxmWLe7Z4Vs66ihIX1RwYW62stSXy+RR2+H33mDRsuQCCgCgqHQx07SX7+0bpt902BQX8SWYaZAIsdhmGYNBNtzdBzN5bFUqZleHUVtS6LIur4OFc2iWWeI45AcMIE/RuIVHI9yVh2lIICVP/wAzYsWwbP4YfHPNd4zz3w7borWi++WBNOlupqmDpJQafu8EQ43Iz6+ofR19TWqpWiS0ouw8SJ6+BybQOXaxexjHpD6XuY1dTcJOJ6qOIxtQkhi1pDwxMx7jkZ3JuIUKgKLS3R5qgWS2xcDLUQkbWZ+kLsKKKkQsf1ozIFFjsMwzBpJr41Q0+wWsu6bdmRF878/MPVAoC0nREjoOTmxu4vpZpHCBcVCSGTFGYz7SBCOrGkmM1wH3886t54A4Gttxap6lSTh7DpgpaNNxdNwaeMp+667lJBKNSkZcCVlFysLaeK2CZTDhSlLaaytZreTyiiKGJr60fac/n5h0a22dzhewYC67X53Nx92mU7Uap3cfG5mtWot61fAWFxI1enDXZ753FdfQmLHYZhmDRCF6CoZae8T91YalE6unDup1VGJkuLFB9GlpxkgpPjkTE/YlvDhpGJJuZ5GQBdduKJKLjlFpgrE2dbSctJT4KyU0EgoAZJm83FMJtztOUmk1lYygjpIgyHW4VVRkLfv6yHM3jwg0K4qOt1XFgwGIyKncGD27fqkNWUqT5TONzU63E7bhns/lcY+ff1veWtI1jsMAzDpBGyBqh3v2qdnVSJnWCwa24ssiKQO0j22JJihwoGtp16KkIVFWg77TR12zqxQ5afrqJ3gxm5wJruuw9KxEqR9+STKN9tt4QxPOXlt2nzfZmCLjPC4gODCYdDip0lMXEseguI16s2VM3J2QVms1odmwRKMmInL+8wzaIXDwkdu31jMd/Q8Biam6fD6/0TPaGx8QWsWnUA6us7TqmXYqf4lxDyH3wQmQyLHYZhmDQi77Yp7Tz/hWko33VXkZLd85idupjaL50hW0xIywS1ghCvHTYMSnExqn79FU133qluW+/G0tXdSRZ9gHPQoJ8WWXuqfletTGKfPB643n3XcFsUlJuXd2ift42QViXZwkKP3b5RTNZUvNhRrTpBIXLo9WZzfpfcWDabGuOTCKdzczFtanoFlZUXY/Xqg7rcjkLi9c5BdfX/wef7C7W1t6Gl5T0kwuNRv8OiOch4WOwwDMP0gtihYNvCG28UvaPy7+1aqwA98g6f2k9Q4OvatSdi6dIJqKz8Z1IXa2GZUBSYWtSLoebC0seDOLtf5Tle4LSdqWYrxRMePLhdO4lMLi4YtewM68Cys9hQ7Hi9f2kxWxR3Y7GoY06ZXfqg5vbvuT4moDkRLteu7ZY1Nb0oGsZWVl4Jny82CywRJJpra++JWZao0nMwWKO66hQgTybd+f3IVFjsMAzDpBHZoTxnqerKEvPvvgvnR9GA1a5AlY8BtU/VkiVj4HZ/K+ZbWj4S6c6J9yPqhjF5vaKKMaHkRVPQ9Xj33FNM3aee2uV9DG66KepeeAFV332H4Maqi6Uz9BWWE9cW6juxEx2/xJYdClAmASqFkdlcoDXs1Fvl5HJCuhZ7InYo4Lyo6Ox2VZfXrz8Pzc2vCUGcTFmC1asPhtv9jXhcWPgPMaXjKxyOHrsSsvwQjuYCWGW3irjYK/O6dTRwyARY7DAMw6SRQKQgXv6XsdVyS85Vs2i6CgXE6ovLSSguKBBY0akbiy6cptZWuTFRV8eI+mefRdWPPyKw5Zbd2k/ffvshNHZsh+s0PPqoVpeHigxaElh3omInM91YNKZmM4nGoOhOTlYPwumUYxeOCVA3mawig6szV1YgsCEpNxZZiwoLYwUNHQs+39zIe1R2+vlaWj7QYo4AK0pKzheiTBVvG9rt17p1p4j53CpdcHvENUo4Pv8cQ3bYAfn33YdMgMUOwzBMGgm2qQKko+7iXSU3d2/D5dJdYrgfWkaYTuyQVSdR80anM6n6Oj3Bc/TR2LBqFbyTJ4vHrplUB6g9NtsIrQhfR26fdCKtLEYByiQ2KCtKZmTJFHmXK1qskZB9zdT5gg4zshQloGV0dWbZia9g3J2aN8HI58vNPRCjRs0QY2425xrGhLW2Rmv/lP5VHH3i/PNpZSF0Sk8/XSzKnzoVmQCLHYZhmDQS8qkXLLtBn0ZTczNKTj0Vrtdf79I2CwuniLtuSnkePPg+FBaqd9k+37zE+xFSM38slmKYIm0g0I3g43Tg3W8/MbV/Z9xygSwkFkuJiFPyeHq/B1Q4HNCJxfaWndgg5cUIharFfE4OxdJExaS+9EBnGVlUm0cExMCmub86wmSyYejQ51FefidKS6/p0ucjpECjsXY4VNdjVOzEurHa2tTvqaTkCpT9pIvvmjsXznff1YROJsFih2GykLa2r1FTc2eHd/JM3xEKNWgpu6GwqnJsBmIn/6GH4PzySxRfcUWXtk/WhbFjZ2PUqJnCfSHdJR2JnXBA3QGLkguztOxkiNgJyMajCyKuvlAItrlzYa5V0+tNJgtyc/cX821tX/T6/vl8FIOjwGRyaKn/iYKUfb4FWgdym200bLZonSKrNWrZiWZktXRiSaoQrkvC1NgoBHIi8vL2R1HRacjN3UvsazIEg3VoanpNC6TXF740mfLaiR21GrTamDQ//xCY4/aHmr7GY3J3ni2YbljsMEyWQSX/1607GQ0NU0UpeiazcLt/xLJlm6O6+l9qQUGLejELTj4CVd98g8p5UUFiWaGLseli9VuqLkx384TDoaYek/hNWBelSo2HyXv386gbK0PEDgUxU6VlS20tzNXVotDgoIMPRvkuu2hZY1LQ6asUpxMSIZWVV4nv0+dbFXUBRoRHIsuOx0M9sug7sAhrlCw4GN/uIerGauow7dxqqYBj5kyRrVa+115iXDoL+qXSAvr3JcLh9q05aNnatSegqupKTcDEWp/UuCJZJ0qm1yuKVwTK02c2N6n7T+1ACJPB8ZdMH7R0w2KHYbIIupBVV9+iPfb7F/VZA0DGmMbGZ8XFrqnpBeFyUSwhsdxcNh6h8eNFdWHZjkF2FSdi7tg9HtE3qvDqq1F26KGdXizUC5tNuHlkIHI8IajvVfDcmxnnxlJcLgQjwcy2+fOFVUeOj/zsZCUheqtKcGPjM2hufhVr1hyDtrYFCTOxJFJcyBgcis8hYUS9q0wml6izJOvhJFMJW1p2HLUmlJ5xBsonTxb9xKwrVnSYuRbfV0wSilib2gclx3Ze11t2jGJ2vF71u6HPQrFKZG0i2s47L2Y7df/7H/xbbKF+VhY7DMN0hZaW3xEILIfJ5ITLtYNY1tr6WV/vFqMTo/JiQDQ0PC2mllYgPGpCTL8psZxScyNYNqgZLyRyBu+6KyrGj0futGmw//EHiuMuJPGYzQ6dG2Wh4X4FIxnm1hZE3VgJ0s77gsDmqhBwfv45LLoUZukmkRdvEju90QNKFswjliy5UEwdjkkJ16eMMema0ltIyNUzfvzfGDv2N2HpkRY8KSoSpdPLkgXOde0Dsp0zZnS6/+TK0hMyaByrP1YlJMokRjE7Mo3e4diSHsDsVfPO/dtsE9MWhApHSgHr+P579DUsdhgmS6CT39Kll2m9jVyuHcW8rOnB9D10Idb3M2prUy9K9kYgELnLJahiMUEFBiWyorFt9mxYqmJTt+x//qlafkKqlcgIKQb076+9n+KFYlfnra1AnsyQyRDLDuGeMkVMc157DdZIk1JCuklUq4pZfBYZAJwuFCUEr3d2u+V5eYckfI2akaW6smRLjuhzFmHloUD0IZttJhqgWiwdF0qkFhOEPuZH2/bPP3f6GQoKTkBp6VXa46BBE1VZK8fp3FpbRgHsHYsd1cqV/3cYQyNiRnE4RINXUB+0CKGhQ+E5/ngxn/PqqzBFvse+gsUOw2QBlG67atXBaG6eJep5lJZerp2UKBiWyQxkI0hxB6/D1mKJSeMOR8SOHil29AJIT/kee6gxLDrXV7K1aGRwMkKAxa3WtMk0sePfdVcER4wQBQ/1yIskxSfJTKh0u7IofoViaSjjrbyc3MYWkdqtFwVG6OsfxcfMEBSIbm5sROkpp+gsO1Udih1ni5q1pce2eHGncTskrkpL/4n8/KPF4/h+WSTofL75Yn7w4LvFeiSO9DFJshaQ3o0lLU6l974SXbbRRmrHe13NJqWgAL499kBg/HiY3W4RiN+XsNhhmCyACpVJ3/6IEW+K1FDqcUSw2MkcZJuAnJzdYrNwlKKYejaGYifSmNP6tyqYJIFNN1Wfr6mBde1a2H9UA0kTiR1ZiE6P0hjZdqs+ETqzxA6NT3DMmHaLSRxIoq4sNWA4XTQ1TRPT/PwjUVx8NnbaaTlGjvwgYXCyxOncokOxIyE3YkduLMp4kqLC0di+6KPJ54N1WWxLikS4XDuJqQxA1sf7KYoHJlOuaCRaUfGoEEcx+6lZdlq1gGbpDrP7VFes/hjVix1xvJtM8B54YNKut3TCYodhsgC/XzUdFxTspGWlSOsBi53MQVagpQJz+vgOhzuackxQkHI8Mu3atmhRzHLf7rvHPO6OZUepi2QTuc3w7bJLZoqdBB3SpRuLkAIynZYdCuRtbf1EzBcWniymTudIuL6bg5JTTomJs4rH6YxadhyOCe23PSh6HNi8eVo9nfg2H6rQCYvAZnuj2hqEqPr2W/i23z42Tb8TcnKk2PlJBFvLmC7KMiMo9o/cbEZIsSOzsaJtMPKgDN9EW0+Rx9FJJ6nr6b5H7wEHqOPx1VcixqevYLHDMFmAPEHl5kbvHKNuLIMCLkyfIDPjyOXhcGwajfWoV+Mz9MGb8dgoJV1RYI27iMnA3fhA5i6JneaIi8xvj4kdyjixM2pUh2LHbk+/Zae5+U1hWXE4togRL6UnnQTnV1+h6J+JG65SzA597yR0ZfaYHoptkTiXS4tVoN1vuK3tWy3ux+xWU8ZbL7gAoXHjENxkE0MLYCJstrGRfmqq4Fm7VhVwsjhjTk5slWc9MvVcxuxEW2aMgElpH2+FSy5B40MPoe6tt2JqKLWdfjoaH3ggcbXuXoDFDsNkkdjJyzMSO2zZyQQoQ0i6sciyo3dj5TTFihuZeq6H0optv/8OS4P6fSoWC1quuALevfYyjO3pSOzEZyspraoAsgSd8Bx4oBpQSp3N9VaeDIBiduKFgT6wVZ+RlQiKRamu/jcaGp7s8vvTuEkXlrTqxGP/5ZeEr6eeVyNHzhTFHmk+Hn0BPmtVnSaIWlujLh7qq1Vf/3BkH04S2XlEOOIiCo5XW0GQG4vSvqkGT0fxOxQ47YhURCZkCwqv9w8xlVmdHVl2Wls/hsfzh+Zao2BxGVvVfNVV0WavVqsISo4R82Yzmu64A95DDqEfBvoKFjsMkwXIOyqXK2oal24sMjGHw31nHh5o+HxLtNYLeiiWgercUFQM3U1T6q/V40Lxr4DVVBS7ru6iHth4YwQmqvEdOZG2Ef7NNsOGZcvQcuWVIstFL44SW3aG6LKVYoNeg5HHNl8eAttvj8r581H511/AjmpGX6agj9lpufTShG4svz+x2HG7vxO1jmpqbtEuzsni9f4urHMUmFtQcKThOqZAAI5vVcuL4fOJrBehUIzYMVdVoahIbatANZkkNTX/RihUIwSzXuxQLSIiOE7tgUVFBkvOPlvU4Ml76qmk+2bJqskyVqij2CKK55GsX39apGCi+j3I/ZLiK9NhscMwWYB0TTgc0YseZYrIn3A43JDSyrHxjf8YFbf7e6xatRcqKy9O6MIi64PZ7BSWty3fOBqbXw/ViqKD0nIl4dJShCJixvmZWjNJuJp0NUtq338f3j33NLTskKXB/u23otaObAAp04MlAZMaVGrzF0YvnAm6nfclwUmT0Hzttah//HHNWiDaaZx5Jsz19Zplh7p4ywrD8bS2fqrNNzV1reeYtHbk5k6OdDE3hvYHXi+Krrgi6b5m8W0eqJZQXt6huqrEAREATBXSicGDH4TJZNdaLWhiR1p2Vq6EIxKsTun6Hb63Kfb4a239QN0HSzkslvbZXvoq3RJytbW0qA1ACwtP0Cw7cr8yHRY7DJPhkPCQJeUdjqh5mDJDLJailLmyKOOiqelVrFy5G5Yt2zLmosGoLg5qH0DVkdvavmxnTYsGJ0fvdC3uAMxUGkcXqxFZSZsN5+YiNGSIlnEltrXttjGrhwcPRst110WzthQFeQ89hMLrrkPZUUeh9LTTRFsFGRQd3zMtYFWPD1uwfWB0RmEyofXSS+E94giEC9R2CoRrxgzkPfCAsGZKt0tt7V0J+8ZF57vWRyv6HUbr5QgiVgyJ2eNB8WWXCUtcsn3N9BYqgmopkeuRgpCpJgC55tzuH4RljtpSOJ1qv7B4UUFCOex0CguTRBPTXq9h76zi4rNjrDTV1f9naPGJh8RWPPn5h4vjTLM4xQn5TIXFDsNkiVWHTOuyn44kmpFlXHK+K1Avp6qqq4Q7RlHcCS8mAxWKowgGo26ReOtJNDhZZ9aPZJ8YXRCCo9V4Dc9RRwkxE7OtSJ+hmPUjwbvUP8r11lsouPde5L70klhGFz66mMo6L7J+irY9h3oBtKLz7tmZQnzGGll2CNnR2+3+pl1sEglQfbsMKpoXCiVunBkPdSw3dO1ERKh4j8h36frwQ22ZbJnQJbFTWRkpRDhW6/nV1va5mM/N3Vdzh8W7sSgGJkh1bXTI58qOOw7lu+2mNVCV2GwjMX78AhQXXxS3vH1AuB6Xa2cUF58fs6y09Ep1v9iywzBMKpG+dbX7sckgTsM4A6c7WSjxd7mJXAUDDa93Durq7o9bFlthV34HlKki0S4I8ZYdsky8/TbqXngB3sMOQ0gndihIVx/Toy/SFipV+ykZFWij95K9l7ze32KEgD9HrZNCFoNsIVwebUhJKLmqZUK1eJiFKG8XmyTEKHUnz4kE/4bh8XRebTgaYL7EOG08InbIAkeutnhiat4Eg8h79FHYfvutQ7FDMTsExXcRFNwuxU5e3n7aeu3EDq273XaxO0BWHp8Pdqq+XVeHnDfeaLePJpM1pg4QQX27OoKsx4MG3YjBg+8TzWaHDXtJE/Pasc2WHYZhUil2ZLaNnmhFWWNR0tLyIdasOVbrwN1VKNiTISH4jrhwUoxFaem1hmNDWTSE1ToopvhbIrETHjIEvv32E64b6cbSd482IhSxBjm+jrpqNITY2Ua4HuiYCQRWRK0duX4xb7NlkdiJ9A+LFzsURyIvuO3cdZEsLbJYUGFHoq0tud5x1IJCDTA3awJEo7pai6/yHHZYh2LH8eWXKLjrLgw64gjY/lBjgAhZnycUsVhR9h0hLTu1tbcKwUxuLZdLzZJzvvce7HPmqJ9fF2Plj9Ta0bcTGUTZTvp2EhRTdOGFMTFFDke0jMHo0d93mHaup7DwRIwa9Slyc/fuUIRlMhkldhYsWIC77roL5513Ho4//nj80kGKn+S7777D1VdfjVNOOQXnnnsuHn/8cbS0qF1nGSab8fkWYcOGC4RQ0VtxjMSOUX+spqbXsGHD+aJyamPj81r5eSPi63xI6wRd0NU73pVaFdX+QijUKNKUk0G6rHJz90Fe3j7a2FBAaXR7NVq363Zip5O7XxI+Hbmw4rOV9Fk92nt5vUIIOJ1qvM/Klbtjw4ZLRFC1eI0HMOW2F8wZi9ks0u8l+jGMuusSi538fDX4t6XlIxH82xkUJCxfSwHmhpadsjK0nXYaGu+5BxvmzRPz8WLHtjDaiLXgjjvU6U03oeiqq7S6M4SIrVEUFBQcHdNAVH3sFK6xkgsvjH5+vWVn553bCWj9+1Ljzdz//Q85770XE1Nks40UrqySkktht7evVt0V2LLTA3w+H0aPHo2zzjorqfUXLVqEqVOnYq+99sIDDzyAK664AsuWLcOTT3a9vgLDZBpUK4SyH8LhZlgspYYmZ3mnHm/ZCQTWoarqamHSl9B2OotVIEpKLseQIfdpF/S6uruwcuWuWL58ByEQ+gMez69Ytmwr1NTc1Om6JPZkDIzTOQl2+6bCHUSBpHoXiSyjb7WWJeXG0hOqiIqQmArHccgu0kbI93K5onf9LS3viJRhoogaXBckzrzJSKjfUgR9QK4MxI6PTZKilIoPknWEYtrIWiOzrDrC719sHJysEztk2aFgc/fJJ0MpKRFlA8Rr5s41tvLMmgXrokVwvUOWQSBcWIimO+9UP084LKphk5VqxIh34HBsJQoSlpXdYJhhpRc74bIyVH/2Geqefz7hsRDz+pAq6skNPmjQDSgrU62TRlhWrxb1njokGNS+DxY73WDrrbfGlClTsMMOiYsc6Vm8eDHKy8tx8MEHi+nGG2+MfffdF0uXqoGCDJOt0J2ojAkZOvQ5jB37B3Jz9+jAsrO+Xb0QcrvQRUGevDsSOxSTQuTm7o+ysqvhdG4vzOl0Aa+vVztkU0aYvPvNZMiF4/GoLpxEbNhAgZoBNDY+l8T21kXGziZqn9AFQwoKn2+emJKFR1q+Yiw7Sd790sWr6cYb0XTLLTFWnnj8OiFEbp71q1fDv+WWMe9leLEmEfsrEM6wisldEju6BqGysrFe7Hg8s9HUpDandLl2FC0Qon2hfk5a7DgcBnVndG6smNdEYmeoUz0JAMK6fHnMOra5c0UcjdjMt9+KgntKJBtPFkwkkTNq1EcYNeozLcNSHwBt5C6iasq+/fdv5+6LaRYawRzZ/2QoOfVUDDr8cLU1RtxnMfou2I3VC0ycOBG1tbWYPXu2uPtqbGzETz/9JERTIgKBANxut/bn0aUU0kks1X/p2m5/++Nxiv3z+xeKJn1U5j0vb3+YzVbDcZKWHenGksujloitNRM5lXxP9H5e769iHbqI02OLxWnoz6f6Jpl9HIWxfPlO+PnnsUKcGa1DmWZ6tx+58DraprQKOBwbiVo2tEyf9USPpVXHZHKIjDnt9bIXEKUKd7Lv7gsugPvssztcJ6ALTKVaPCarVRNSZp8vsm/Ri3Ve3sHiwk+dzssoxKiwMKt+d/r2AnSBlcul2KG2EeFwi1jm8agxVPR7ycs7UCyL9oX6pdP3kkKevmd618Ibb0TeY4+p+yEtO2VlMa8JbbKJEJDmtjbR08wUKfZH+CPuKkfESqLYbFDo9WazllZvaVH3Pf6P4nnsungfQW6u4brxwcoyiF2PdcOG5MY8GIQtsv/UGqPw5psN1zPrxI7J5eqz46krtK9nnUWQJefSSy/FQw89JERMKBTCtttu26EbbPr06XhL17djzJgxuPvuuzFI16At1Qzp4E6NicLjpLpMli27AmvXPiQeFxXtjKFDhyUcp1CoCCtXqjVyysrsCARqMXfu3vD71cyg8vJdUVdXS7GKKCiwYMiQCsP3XL5cPSGPGHEQCgtl24FDsWxZbNZPTo4bFTqXS6awbNl1qKp6WRRdpL5GRG5uNYqKomXyJc3NsVkyy5Ztjp12Wgmn0zgNt6VFtXoNGrSv9tkdjsmorb0dweBCsaypSe3VZLcPxlBdwUBZxr+MigamatymTgXuvx+O//5X3Z9C1TVVTKKnokIcE6siraMKC8dj/Ii3EC7MgcUPOKlSc5wlIKN/d2QBiVxYc0i8aGNYgTVrRsLnW43c3A0oLt4I9fWqFaK8fD/tO8jNPUi4g/3+Pzs8buk3sHSpmok1bNhuyFtaDTynWv0Kbr5Zs+wUjBuHgvjt7LEH8NFHGERdvakYJMWMms2wU8DwnDnIIasPiYChQ1Ehjw3qel9bi0FUONJovyjAWLzIJOJ6iMEUnB75rmPYe2/gczWLi7BQLzUaN2ojEaGMChN2dvzRjf+a2IrTzu++QwVZA/PiCixKEe9yRT9Thh9PWS121q5dixdeeAHHHnssttxySzQ0NODll1/G008/jQsuuMDwNUcddRQOPVQNXCOkOqypqUEwYoZMFbRt+uIrKyu7lQkzUOBxitLQ8AKqq1WhQzidx2JDpD1AonGi+BFyY61d+6N4vRQ6hM83AoGAWom3vn4NFKV9ijqVjg8E1DvX1tahcLvVdUKhaGE7CsylAm319YthtfY8zT3VLr81aygtPAi/P2qxqaqaA4+nfefp5ub2Lo2ffhotLAJDhjzcrqJsba16IQmHt9C+i2BQvXB4PEuxatUsEUxOmEyl2jpEudsNCrGtaW1FMEGbhy5z9NHqH7FhA6hDGtl2Gisr4Yl7j0BgMKqXLMVgVf9hQ2urViAvG3539ieeEA04CW99PRp0n89m20KInXXrZsLr3QiNjaqo8PuHa99BKKRepOkmYN26ZVpjy3gCgQ2RFiAWNDUVwLfoJ0j7SPW8eSiPWHbqLRb44sbYcdJJKPnoI4SffBKNW2+Nkkj5gJaKCvHdIBI47C8rQ13ktaW5uSBHVv3y5fAZtFvInTMHZPvxHHwwbH//LaxCtfTdRaop6zEddhgGPfqoaCNC69VdcYXIlCrViZ3m+fPR1sHxZ2ptxaCdd9bcbVS4kFxtVKW5/s034TvwwJj1ratXY1Ck5lBVJ+endGK1WpM2VGS12CErzUYbbYTDDz9cPB41ahScTif+/e9/i9ifYlLPcdhsNvFnRLq+INpupp5MMgkeJ7oQR62OZWX/Eub4dk0d48bJZhsjxI7XO69dLyDq3Gwy5WttIIzGNxhUXTDkMiM3jFyHAicrKp4RQZ4+31whduiikGnfEWWKkdAxWm60r36/Gs9DWUsU10EuDoLK9NfVPYhBg/6jrRsMVkeKBVKczo7a9mhMKA2XKimvWXOMcBcSjqYC5N94I3x77SX+ZGxD2G5P3/kl4saiC5x8D/re3O6vUFBwIrBcPSbIdaJQDEwnx1Mm4Zs8GY13342ia68VFh79frpc26C19UN4PNTPaq2Wak8B5HI9aqlCblxydfn9a4zjcWIysUaL1H2zThiYKys1yw65iOLHyjt5sigdQEUCnRGvAfWvCo5UW1tIaB35WunGoowso7G3RExzlH3X8Pjj6nems/LooUDpKrIEUeYaCVmKoVEUtJ10EnKnqU1NzevWdfgdW5cs0YSO3Ff6I7FDn18bz/p65LzyCmzUVy1y7HV2fsoUsjpmh7K34v125khAWyYONsN0BNVDkam0VAOjpMTYOhmPTCGlEvAy9oYoKblEXMwtFvXuNlHquKy+LKsx68nPPwg5OTvqiheqNX8yCao8S1AcDQnE6PKVHa5PQnLEiOkYNepLrUpsU9ObMW0g3G619xAFesvAUUlJyWVaTZvW1k/EsqGPz0Xes8+i9JRTxAWnozo7KRc7ujgK+t4GD75HxBhZIpWH4ysSZwuUwRT/+QinczutgGJVFaVXKyKjSZ8NR1itat8xqqzc1vaNJnb1+P3ReJ34/mNC7OizseKhuJlI3RtK9RbvNW6cVhNJoq+lpEQCxY1aO4j3X62m0IeoarbVGtMnzfgFkRR9GSxsMqHp3nvRdJOabWiOq6is7oQiXHT599zTLoBZCDOdiJbkPv+8qCEkg6cTNaXNRDJK7Hi9XqxcuVL8EdXV1WKegpCJadOmiVRzyXbbbSdq8cycORNVVVUiFf3555/H+PHjUZKlP2xm4EJpsxRvYjYXR6q/JgdZduIpLj4XpaVqXY9ogHJLJ2LH4EQeQRY0zEyxI9s0jBMCcejQp7XgVeP1l8WMG13gysquj6QpN8S0gZBdnqlsfjwu13YYMeJdUa2XoN5DpZ81xlTIpYDPdKfnGokdozYL2Sp2En0+NUjcKgLM1VpCJlRUPNbu9TabGvPW2voZ1q07SfR+o8wto2NCFivUix3rihVUmVALUDYivsgfiR3R4FW3PmVhxQu4+KrK7Sw7BpW0u4IUZxad1UZiXbhQdEvPf/hhFP6f2itLIiw2EeGkH3d6TbaSUW4sqpFzMwWDRXjxxRfFdPLkybjoootETI4UPsSee+4psqk+/fRTsW5ubi4mTZokCgwyTLZBriJZDr8rmQaUkl5bS3d20QJ5JHSoPDwhuzcnsuwo6+aL2x4jy45EWnaoPD9ZTbuaCZFO4htwSqEo3Rp6yGrj9y/S0n0lNFY22zhx4VR7K20dY9nJyTGufeN0bonhw18RFoM88y6weo7XnpNZOZEVkS76vdgxuOgSapf3sVrKODUItdvb3yRIy47s2E2sWXMYhg2bJrqbE4HA2pheUbLasVg2f75mnZNVnOPx7rsv8u+8UzQIpe9D1koKbLopLN9+K+b922wT/UwRN1a74pCBABzffQdbpFaPsOz0ACnOzAZix/5r1Aps1Yk79YVhQ8uOuUFtKOvde2/Yf/sN7mM7bjeRSWSU2CGh8oZBTw8JCZ54DjroIPHHMNlO1B1jXCclEXTRHj/+b2EVWrv2uEgF2OhJuTPLjvOFR4AzAceqJiA28UvDalV7N9F7kCCwWhNbgXo7OJniZgjZJdpuH6V1gqcmkPrmqdJ6RsKOqsnqsdmGCzegvPCpLReWafE6iaCLLP3Ju3EjsdMbbiyZtdTvxE7k81GgLtWzCehEA9UUkmKHWnkYQd8robaCiNLa+pFO7KyJWVcvdhyRTCey1uhT4fWQKKmaOxfmxkZhtVEi2Uv6Fg9UKkASNnJjhcMoPvdcuCKBxcHhwxGiLL4eIFtTmDsRO9o+jh8vxrvl6quR8+qr6j7qjis5LtSZXtR3itQLygYyyo3FMAMZKXba9eVJAmoTQFlEo0bN1Nw40ec6tuwEIqEoziWJC49RbIp0c6Wi6WiqoArPVOOG9i0nZ3ft89ps5YauLFkzx8h6Jt0dUuy43T9p7pL4DC0j4uMiZCVdUUBOVxwv1fR7y47OKjYori+VwyFLC5iQn3+w4evl9xpPW9vXWjCtrLskBHAwCKtOuErrS7yrqt1+5uaqBQN1adoUpK6JG53glYUASRxJ7LNmaUJH7N955/X4uJGWHUt1NQqvuQZOSo+PWGtkfzV9UUL3aaehdsYMEW/U7rgKhbQYnSClm2eR0CFY7DBMhhDNJum62OmIziw7gch13F5lfLGU9GWQMvWwamz8nxafI6EMNCInZ0+YTNEgTpdrvKErK+rCijZEjO8HpnbOjsbrJNssMf7u2bZkSdqtOonEjv2HH5B/773iwt2fxI5Mk5ZIi1tOzh6w+QvoC07oxpIMG/ZKJLB8nTg+qAEotf6gy2HZRTeh7PDDtVYI4Q6abyaD+6ST0HD//aj59NOY5eFIzRt9gK8jUluHXF+Nt92m9d3qCfrvPPeVV1By5pli3vXmm7A0NIiMsab/RLMP9TFGmvsw4sYy19SIGDTqVxYerFp6s4mMcmMxzECFXCuyiWHqxU5eh2LHLy0789djMJnOR41CzQcfiJTWeLFDFYP7Quw0NPwXtbV3ikDpsWOjRQGjd+SxsQ0u1zg0N89qZ9mRVinprtAjl1FfMcLtlsHJiXtVxbw+rk2NbZ4qxBLFeaRT7JQdr8YOkUsl28WOlmGkC5INyOynnJ1FkLgjNAzlu+4qLBk1n8V2OY//rinOikoFUAsJj+cn0QJEbBeDkPuRavkgApMmwXvAAch/4AFRzM+/u2o57BJmMzxTprRbTHVs4t1l0q3UdsopcJ9+OlKC3a4eA01NMWLRHmmy7Z4yBd799tOe0x+r8riiDuqUkSYtQSESOpQhlmWwZYdhMgBV6IRFRo/ForpgUoXFUmzY2Vzij1wD7Y2ASVFEbY28SPXYTLHskFVHfe/4onnrDF0VJHaM0s+pTpA+u0wPZXOpr1kCj+ePiFXILOJxOsP2558ouP129T022STGRSEvbH3hxnJ++WW/s+xQ9pDz44+1x9TixLlgrXDV2BYsEKnUpkggLWGx6DOozOL3IHtmUbPcNWuOVLfjjT0mKEan5YorUEXtHtauRTiFVfZlZpaFUtqpPEE4rPbX6qYFqSPiv3frwoWamy5IMTrFxWi59FL4dttN/MVbdqxr12LIttui8NZbxWNvlsbIsthhmAyK1yGrTqozneTJnmJ2qGGlHiXghzdyLXbpEjJEbZGE6ee9H7MT3+g0utxY7DidqnVMWsui61fGCDc9NtuISIp5CLW1t0WWjUwqXsfxZbStBgWVykaPqUgf7onYoWJxsoaKDFbNdrFDPZtKzjkn5hglF4teDJXvuafWqsNkil7mZJNWKXb0FK+LLTgoOpqbTKrLKb5dQg8JFxeL6sNinzZsgIWK97W1ic8apJYeKSQ+yNm2YIFWxycYyfZqufZa1L3+ekzWYPy4i+DroiI033gjshEWOwyTYWIn1VAVWerYbWTdCbWtQNgBmIKAQ6dvpDXAyPLh8/VurQ0ZMBwPBZbK52T3d4nTqdbQ0VeUVjuTNyS07BD5+UeJKbk39FaxztAaftJrDzlEvVBGCKVb7CRIzSaoqq814ioR2URZSKIaRfaIJYSQF2/tcW0t7D+p36Ge3Ny9tBpJ8a7ekrlRUdt8zTVo+8c/kDZIRElX1vr1QoAQ4rhJsYuo6ZZb0HLxxaIaM0Ep4zQ+RCiuyrMeo27m/m237bzAYYbCYodhMoBAoPuZWJ1BliKZKi4LCGrv26ZWjnWuB8zh6HKjImTUXoGguJ14C1E6aW2NxlEQstEnCRfqDC/7g+lxucZoFiFFCcZYdUwmF8xmY2tNvIWICjwmg8zE8u61FzxHHx3jiujogpIKtK7nMi7DoHo8BZ7Gx2BlDRYL/Lq07Xap0x4PcgxKlkgBQYwY8R4KC0/FoEH/Fo/je2SNGfMLHCtUiyVVHW697DIoRk03U4h0ZZHbWBM7m0ZrP6UKshS1XH892iLByTnvvNOu3o8hBiLTv0PnLt1MhcUOw2SBZcf19tsoOfFE4L33ULbPPig98sh2Jd6TcWVRmraegEdNj3atj3WdWSLdj+mOuezQQ8WfYy3Vp6EsjCB8PjX4tjfQF4MjqGEjxdSsXLmv5pIym2NPzHZ7hejzRS4p6QLTu7ASuQrj3VtaiwgSEPLPAIoX0eIZLBZ4Iv36xP6muQt0cLPNRIaMdflytdaPgYUnOKF9Q9Rsova991A7fXrMMtsfahmB/EcfhW2xWmsnkdghS87gwXfFuCQLC09WX59/hFguSwWkW5xKtBYT1GsqUrgwHWJHEorLoAp04i4zsuxQY9JshcUOw2SB2Cm+9FI4vvkGOPJI2BYuhOPXX5H/4INdFjuy6af2vhGLkqs2VixYqqpEGq/rnXdg/+MP8ef67DO4XNvGZCqlk3DYjdWrDxO9jyhVWBIKNWLDhnNENWex7wYBxBSnEc2uUl0cMm09vpigUfFEvdihLJTB226LoksuQdlhh6HskENESreRZScUCWINbLst/JttJuqrBLZSix2mCwpA9e+spsdT4C5V8Y2nty7gacNub9dYkwJnCaeuNo0e0ayygx6JgwbdjMGDH1B7iK1bJ35XCvW5IldNL9B2+ukitss+Zw6csnBhL4qd5n9F+8gl4z6sfftthMam3vLcW7DYYZg+hgKH5YXbqM9VfDyChCqcSgtMZ8hWEPFuLH9Y3bazrv1dHMXtyPRpwtTWhpwcteKsrFqcbveV16vGZRQWnqKll1MlXH2QtHSvxSPjeOS60hplVGNHonZ+j57kyd1VfNFFQvzlTJ+uCr85c0RAaczrIpYdLWPHZELdO++gatYs4+aRaegOTtD3JeuiUH2f5uuvF0Gl7qPUWKRsJr4vlRhzKnQXiUmSxfEoIJwsXbZFi5D7dGyBzZjXm10oLDxBxOtI145/u+0S9r9KNfQ+8XFUMpMvLe9XEY1T8+y/P3z77JO0Zaf5qqvg36l9UHc2wWKHYfoY2YWZqgAbZf5QZdV487Nv111F4bM8XWPcGIJBmNxu7aHsBN3OjWVSXTzOxjzDInn2uWq/LoK2JwM8SYQEg+3jelKJDIQmAVJW9n9iqk83l+Tmqu6seKzWQZo1y+dbgqamV8Rjp3PzTuKbonfAVp89psptTHNIiaJoAZ/h8vKYmiW9FScTjNxx037J750uVq0XX4zKv/7qXo2YTCMucNcUComUf6pwrNhsqJw9W1gfambM0DKGKDOLRDpBosj6txqjpseydKlIVyco3qo3kdlQYn7kyI5jaHpISJc6L7uuJ2vZ0VdZzlZY7DBMhgcnywur8OfffTcann5aCza0zZlj+JqS009H+U47wfrXX8h58UVYTBH3iq7IHgXu+i2qRcnR0v7kRxWA9RVezW63COBVLSMKWls/RG+InbKya0VMjoyfkZlSgAWjRn1t2PxR77qjjKx1607Ulndk2YmP27G2RE/8lNViJHYoBdoUqdwb6gUrjhEy0yZe7AgyqGlrqpGF7qheDLVjIOsDBRbT7yM4erQQqqLHUziMwTvsgPK9926XaUhZXaZwWFhV3Kee2qv7r3cvpjNeR6Arh5BMVW+9ZSedIqy3YLHDMBkeryOzbLwHHghccw1CEyYgFLEgyC7EMYTDcH79tcioKj/gABRdfz2Kv6vX+j0pSlizkCimEEx+wC7LKOuw6aw6hLxDzs8/0jBwONX4/WqAqcOhmval1aup6SWtRYDDkTjwVoqdpqb/aa4sCkbtKGYn3pVoawxrYqLtrLNECm+82Mn773/V/d1667R2N+/MQkDxJuaWFs21qW910F+oe/55uI8+OiaNmpCPNSwWtJ5/vpjNfeopWHXByjIQWVs10vFbNLbsZWGoj0OScVe9gX+72NT7ziw76W550huw2GGYPsbvX9ah2JGdkansu74oGWHkYtEXWJPkz/WJlGtK1/b7/9YqBROudXQX176lgXXRIkOxk5ur+vq93j9FrZt0QMHEavaUGXb7xoYtIaJNIHV4PKJCLLmWpBtLMnTo/1BR8bhwe1Cwd6JsNumqE+9Z44sJ7pTuotwXX4T9u+/gfP995D37rFjWcuWV6DOcTq1Scwk1kIzruN1f8O2/PxoffVT7HkjUJ6pS7T72WOF+oTpD+org8Q1bpdiRdW96E31VZmpNkW6qZ8xA0803w3PccZ2uy2KHYZiU4vOpd53yoh6P7LqsNyVrXZNJgMQ1P5Qn7xjKK7RMKq9XdX15vWqdkoJF6oWR6ouIbUfehwI89Uj3iN1OosMMRXEjFGovrFJBU9OrmvCwWFQXW0nJ5bDbN9LWkRYfPSXnnotBFHj53nsxbQLy8g5GXp4a20PNEEtPOgmDKI3WQKzJIGzCXuWJicXR1xnJfeEFFNymVlpuPfdcrcN1XxHcKDo2/VXsSOQxKjFsyeFyqdYa+k6pOnAE0flbVwRS/l6Ckbo3vQlZc+gmhuozpbv4pCxT0Hb22cl1U7dY+tWxxGKHYfqQcNirpUQ7HJt26MbSn+ApLoHcFu2sOxQsayR2fD7NPSOrCns8ajPAwj9V/3zbOedgw/z54sKtrx0j31dadigNXKZ1y+DqVNPW9o2YFhRE70Apbqew8KQOLTvUC0rw2GNaULZY7lTTv02NjXBEKutSPBK5AckSVHr88XB+qMYgkbiqqHgagwbditwVgRjLTmjMGDQ89piYd336qbAaUB0dqrjb1/h22aXTOin9BU+k0Wl8gb54qJlnPDlvvon8++7THstsrnT3MDOCMvUosLreoBddJtB24onw7bAD/Duq3eWzmexrXcow/Qi/n4qhhUSl3kQtDKQbS9FnRJjNQvDQxZsu2MLy4PNh0AEHiMBiwnPooeJCnPfMM6KVgIxVIbFDIktaeArnAeEDc7T3iE+VpkBPO2W9RMQOYbONFvVrAgFKwU7tiZBiitQmnO2DifU1dWy28Yk34nDEWHbIakaxLIPj0meHbB7dvnXxYnipho7JhBE3fCIsW7KJor4woDeS5i1peOihdp25+wJ/nNgxt7aiv+LbYw803ncfiq66qkOhkqhuTf7jj4t2EFSrShYk7AuxI+ijOK9kaNKJwmyHLTsM04fIjCOy6pCYyL//fliXqpaeeDdWvOk+Pm6H6r9IoSNP3lqTSGHZkUX21sDno3gbP2ytdhGzoz/Rxwd7hkar2U76VPaolUgNrk4lFEysKFQF2Kbts8Tp3ALl5bejouIJmM26OIJwGLlPPqlfMcay43BsBMf333f4vtSBWsb7UN0VqsArX6N3EVGXaCXiBqDCcJmS1h3YbDNxF54oNqW/EdB9J4mECo1JIihwP5cyteg7tVgSWoeY/gFbdhimD5FBwnQxLrj1VuS+/DJcr72Gatn3Rx+zE9erR4vbkRlZcbE7dAEwtbRoYsdqHaHF6rS1fSHmC+eGQM4w3267aa8LbLedqOwq06llLRC92LHb1Sworzea5ZL6gO1RMJnan6KKiuIaNCoK8h94ILaidCAg3G0UkBwOq0JPXwiQAlypvYJYdeONhXB0/PgjHN9+2y52gqw68Wb8+ldeEev2aVByPBYL6qZPx9DIRbvfi51Jk8R3Q4JeX99IT0fNT51fqL8Bwr/NNhlhnWPSB1t2GCYDxA51FHe9956Yt+pjbrxerZt1IssOubJMDQ2iR1C8+4ncOVHLTvQiXl+vFiMs+iMkRJPe3E+xHuLkHyEkxY7OjRUNdv5NS2Xvq6aoZA1r1zojkpFGqeaFhWp8hzVSibrp3/+O+Xz1zz+viRkSQPHp/GS9iQ/oJDcKldvP5LiYRN3C+w0OB6q/+w411GohUcBt3HK9NagjFyDT/2CxwzB9iAxOJksJ1UiRmOrrUXjddbD//rt4TMHISl6eoWWn+MorRXaR44cfYp4nd5SWMurziQrN+h5TMjhZmPrjLgpUlI1q+VDtGKrWrFl2wmHN7WYy5SAcbtZS2VOFLHwYn2reDrLo3HmncY8wg/R70SRTuuUin0M8HjZMc93lvvIKcv/3PzFPfa0qf/sNrZdeimyi9tVXERg/Hg1PPYX+DmUJdSY43SecIKZUNLDmyy9F6wM9NFatF16Y1v1k+h4WOwzTR1CQsGxS6fDFdsYuuvJK5L70EsoiWSfChRUnSGStEYICiI2qs0qxQ9YhaoUwZszPMevkLostbCahQN2qP/5A7Ycfar2CTIqiWZnIveR0qmm9Pt9fSCWBgFoAkKo1dwS1CsjXtcuI6chsUEPHGhE79Hn1Ljly/+jjlPKeeEJMQ8OHx/QTyhb8e+yBmm++6RcZNKmg8Y470HTjjah/4YVoA05d64m6N99sdyPB9D9Y7DBMH6FmHIVhNhfAsUqNy2mXQh0h3oVFtEUqxCbEbo+KnUhdEau1XOsxRZjD6LS+B7lDZJq73pUlg4fV4n+pQ1Y7TpSdFp+lJml88EHUS2sGPaerpSKy1iKB3OSWk9YaSq01rMCrcxMyWY7TKX4rJF4J6lemDzjvrcafTN/CYodh+jxeZzys62Jr45iCwZjHwYkT272ezPeUXt4ResuOZOjQZ0Ra9sSXJrbrz2MIpblHXAV6sSN7SAUCqRY76lhYrR2nAsf3OKK7c+9BB4mmkPo6Qfp4HXLNkesjsOWWqPzzTzRFGkAaNexksdN/8e6rax6bTIE9JuvhbCyG6SOiWUcTjAsB6pBxM/HIOjB6KEOl7sUX1Qe61HNJTs7OGDduLso/3V48DiZRuZU6eMPtNhQ7oZDaTDQVKEoIQRJPJsC5qg1WZSGCm2wC65IlwvVARf0k1PurHWazaAppW7hQTSP3eGCbPRtOCmKN6zIdX0+obto0EfukPc9ip9/Scsklol2IL0PKBjDph8UOw/S5ZWeCVsU12eq4HYmdpjvuQDBSOVZfZ0ePyeOBpVK1yCRTpl6z7OgsRFLspNKNZfrza8BFvjVg6P5TYAoDVT/8gPI99xSfZQPVIJKVo3VipzHStkF2jyaxY/vrLxT+85+w6LKrOrJi+SZPRusZZyDv+edjAsCZfojL1a8K5jGdw/Y7hukjfL7FWtq5tOzo690QzTfcIFoRSPHSkdjxb7stWs86K8ZEr8/G0mP/9VeYwmEEhw6NaUbYqdjxqL2i0iV2lCWzxNRRCyF0iNxIKX0htHRiS9aRabniCrjPOENbLtPo7T/8ECN09Gn0iYiJ5WDLDsP0G1jsMEwfEArVw+9fpFUFtqxdK+a9e++trRPOyUHrRReh9bLLNGtGR2Kn9eyz0XzLLbEN/OIClCUkBAg/uccSbDt5sVMt3E+pwJenxuE41RhlgWv6dG1evH8gIAovUjYWEYpzR5Flh6AigURYl5oc7iTrxnPssaIdBFVIDmyl9tNiGCb7YTcWw/QBbje1IVBEzybXnLWwz52rxeZQfRequeM94IBOt6MXO2Fd/6bOxI7j66/V90uymJqR2LFYBsFkckBRfPB4ZiEnp/vxD+b165H/yCNo3nq2eJynhjOp76MLRCbrDpX4L7jjDm1Zu15ecVYwqqPTesIJcL73HjzHRRuLJvqcVB2ZMr3iK1YzDJO9sGWHYfoAt1vtvJ2Ts5tWxI5cWHShbpg6FU033YRGajDZCXpXi+zM3ZnYoeJ69r/+Ev2AfPvs022xYzJZtC7kdXUGhf26QOHNN4u6Qt6wWmQxL0HLLXp/vaXHSOyIVGJdfRx6norG1c6YYRjj1P5NTCx0GKafwWKHYfoAasYpe2LJQFv3MceIC61v333Rds455CfqdDthypLqQOwYZWM5Z84UU/9OO7UTConQAp11Ykffp8rr/bNHbSOsK1ZAobo3spXRrqfBu99+7daj95f9vjr83FuqBQ+JZD8jwzD9FxY7DNMH6GvJyF5M3QmIDY0dK8rht15wgSZsEtbZUUhOAJYNakBMYPPNk34fo2ysaEsHGxTFoxUD7A7UsNFfAgQoASoEOCzjDbOh6P3Nra0xywwzrHTxNix2GIbhmB2G6QOCQTXV3GodplX27Vb2j8mExgceSPi0lo1FkHXH6YQpIhY6C9btzI2lvr0NdvtokUav1g0yCZeWx/MTHI5NUFHxpGhT0en2HQ60Rqw6ORSrXVRuOB5UW0eOl0akiGAMurpEXCGXYRi27DBMLxPy1iEcVl0xNlvPLDudoRc70pUlG44q+fndEjv277+H8/33teeoArRsakrd1Jubp4nO5a2tHyEcjk39TgT1qpIuLOrXRVYdI8uOKBSo3y97bGNTDX2xuLjgbIZhBh4sdhiml7G9+ZiYWlsAc8geFR/pqOtis4k0ar3YkTEvlPXVVbFjXboUZSecgJILLtC6iEux4/MthNc7J+Z1fv/KpLZPIkpadigTi4Rf2CBImCopG+1XO3SvDU6YkNQ+MAzTf8koN9aCBQvw/vvvY8WKFWhoaMBVV12FHXbYocPXBAIBvPXWW/juu+/Q2NiI4uJiHHPMMdhbV6+EYTIJZdVvwI6AoxowNzVpy40u7j2GMoscDjWwNxJv0xPLjmvGDG2ZbcECUaTP4VCDgcl1FQyu1WKRKC4pEFgJl2ubznfT7YYn0uQ8Z5U6FqZQtHaPb+edRd0c22K1EKOkoYOMNer8bfvuO3iOOirpz8kwTP8ko8SOz+fD6NGjhVC5L8lS3g8++CCamppw/vnnY8iQIULwhMPdzwphmHTjL1BFh6MGURcWCR1dMcBUEi4vh3nVKlG4kIJ5tZidbogdPdZFi4CDDoLLta14TK4rgrqqU82d5ubXEQio1p/OoJ5bFKAs5is2Q2joUCFScqZNg+fII7UCgiSwiJYLLxSdrDsKPiaLTmC8anViGGZgk1FiZ+uttxZ/yTJnzhxhDZo6dSryIsGW5eXladxDhuk5/nw1yNdZhZ4FJycJdUy3rloF699/iwws6X7qjmVHj+3vv8XUai2H1ToCwaCaTp+TsytsttFiniw7SeGOip22+56DzWIRNXFqvvhCLCu65JKY1amfF2dZMQwzIGJ2fvvtN4wbNw7vvfcezjvvPFx22WV48cUX4ff7+3rXGCYhvnydZSdSYyedTScDkX5PtkWLRENNcySjqqeWHRtVfY64mkpLLxWnE5MpF2Vl/ycytAi/P0F1wDjCljYokaQqq3VQp+9Plh+GYZistOx0laqqKixatAg2mw1XX301mpub8eyzz6K1tRUXXnhhwhgf+pNQWqxLZpokkSLbFeT2Ur3d/sZAGye/FDvVQO7TT2vp0Z19/u6OU3DjjcXU/ttvWqdzQUFB8tvKyYlub+xY0YTTuno1XB9+CO+RR6Ko6GTkmrdH/r33QdllIZr3Vt/T71djbDp7n4DTLaYWFMJs1qXLS3RihwKu/bvvnnCbA+146i48TsnB49Q/ximrxY4SKZJ26aWXIidyMiYh88ADD+Dss8+G3SAtdfr06SKgWTJmzBjcfffdGJRE5+fuQrFETOcMlHFaXRgVO465P1EZZThvuAEVuhYHKR2nAw8Ugcpk2dEzeOLEGBHRIcOGRU8akycDVLvm3ntR/NtvABU0JB5+GHj2Q/FXFPJj1SobwuFWFBf74XKplp5E1OdEXHv2IcbjoPt9mn7+GRVjxnS6ywPleOopPE7JweOU3eOU1WKnqKgIJSUlmtAhhg0bJkRQXV2d4UnzqKOOwqGHHqo9liq0pqYGwWAwpftH26YvvrKyUhNmzMAeJ/p8vrzIhb1GXVb/+OPwkfUlUtk45eOUk4OCf/wDuc8/H7N4AwVHxxfoS4Dd44GMkGncaiuYmppAuWOe2lo0Rva7dOZMyNuLqsoa2Gzj4fcvxNq13yIvz8BaIwkEYCpUf3sKSrHBYBzyAgFIp1uVyYRwB2M1kI6nnsDjlBw8Tpk7TlarNWlDRVaLnY033hg//fQTvF4vnJFS+XSipEEvTRC8SC4v+jMiXV8QbZd/JJ0zEMYpGKxF2K5oMTuElywlXfjc3RknSt2OFztiC0luR/9+3p13hjPSNR1er/qcooj+VhJTfT0cjo2F2KH6O7m5+3aYieWLFDm22oYYfzZdm4oQ/baT2O+BcDylAh6n5OBxyu5xyqgAZRItK1euFH9EdXW1mK+trRWPp02bJjKvJLvtthvy8/Px+OOPY+3atSIz6+WXX8Zee+1l6MJimL5GxrA41wFmGTqWrCupBxg2y+xikHPY5RKp3GGynsrGoBERYmpsjGnjYFmzBna7WiXQ71/VaY2d1kiGuN21ieE6Mf2w+LfNMEwXySjLzrJly3DzzTdrjymzipg8eTIuuugiUWhQCh+CrDn/+te/8Nxzz+G6664TwmfnnXfGlClT+mT/GaYzfD41biY3yYzsVBGO86O3nn9+l15P1Z2rf/lFCB6jLuiWmoiZSid2bGPUBp2d1dohsdMyUZ13OrcwXieu0znDMEzWip1JkybhjTfeSPg8CZ54KEbnxhtvTPOeMUxq8LsX9InYCenqT3kOOwzN3fjNUN2bRF3QzVVVMevaf/kFtn0OE/OBwOqON7xiLryRxuUOh3Endt9uuyHn7bcRzs3t8n4zDMNklBuLYfo7fk9E7ETCW5puvbV33ljv+tGVXugu8W4sS3V1zPMUH+TcoAb/U9sIRUlc+8ryplot3dHggMVi3DLDc8wxqH/qKVTLWCGGYZguwGKHYXoRf0g16ThbilH17bdoO+OMXt8HUwqKbsa7scwRN5b7iCMQqqiAKRyGc1EVTCZaL4xAYJ3xhgIBhFtUN5e5KOLLMsJigfeQQxDmYoIMw3QDFjsM00uEQs0ImdTGn/bgYITGjRP1b3oL/zZqQ0738cf3eFvxbixLxI0VrqhAYNIkdVljE2y2UR26sqg3mD/SKcOSN6LH+8UwDJPxMTsM058JBFTfla0eMOX3LDuqO9RNmwbr4sUIRERPKt1Y0rITGjRIm6daPDbbSPj9fycMUqZ2GbInlsWgTQTDMEwqYMsOw/QSfr/qwspZiz5pYkmNPwPbbpsSa1K7bCxp2Rk8WOvzRanoJHY6tOzU1yNQnLgnFsMwTCpgscMwvYSyYKaYutapvbCyGc2NRY1AAwGYIwHKlPUVK3akGyuBZae+PurGsmT3mDAMk7mw2GGYXkKZ/UH/ETsRy450Zck6O+HyciiFakaVOeLGIvz+aHXldmIn4sayWqPp8QzDMKmExQ7D9BKeEaao2CmOmDOyFYcDSsQdVnzeeULYaJYdKXZ0lh1qG9HYqBYJlZjXr0fRDTewZYdhmLTDYodhegnvULVfjGstsh9d3I/zm280a49SUKC5sShAmVpGWK1qllVzc2zB0PxHHhH9uThmh2GYdMNih2F6Ke08kB8S885qKzyHHIJsxxTX7I8ysUgE6S07JpMFw4Y9GxOgLVGsVgQLgHCkIbrFwm4shmHSA4sdhukFAr5lWtp53Rc/anEt/QmK1yEUGaAccW3ZbGPU58MNCIUaY4KcfZGkNEsoD2ZzNA6IYRgmlbDYYZheINi0IJp2nuXByZ11VtfcWM3N1PkUZnMOLJYh7QKVKW3dFxkKi4sLCjIMkz5Y7DBMLxCIiB1nrT22T1U/IhRp5UBiLlRWJtxc9j/+EMvs9tFiGgisjBE7/kiYjtVa0Re7zDDMAIHFDsP0ohvL0ZyP/khw+HC0nX22+sBkgn+XXcSsfdYsdWqfIKZe7++Glh2rtfcrSjMMM3BgscMwvYC0aDhC/c+C4TnwQNGNPDQi6ory7byzmNp//VVMc3P3FdPW1k+gKGEDsaO6uRiGYdIBix2G6QV8VrXCsLVsK/QXat99F61nnIHGRx4BIhWVJaGRajFBS22tmObk7A6zOQ/BYCV8vnlimZncWFpBQbbsMAyTwY1A3W43Zs6cifnz56OpqQnnnnsuxo8fj9bWVnz99dfYbrvtMGQI37UxAxfKQArm+MS8abTq3ukP+LffXvwZES4oiAYpk7AxO+By7YK2tplwu3+A07mlsOwEIklpFktE9TAMw2SaZaeurg7XXnstXn/9dTG/atUqeCNdkPPy8vDZZ5/hk08+SdW+MkxW4m/9S0wdVUB48x0xEJBixxwRO0ROzq5iSmKHILETjIQwWSxqBhfDMEzGiZ2XXnoJHo8H9957L2666aZ2z2+//faYN081WTPMQMXfov4GcldGa9H0d2QdIVNLCxXYEfMu104xQcrCssNih2GYTBc7f/75Jw466CAMHz4cJl35eMngwYOFxYdhBjJ+70IxzVlnI38OBgKaG0tRVMGjSz8Ph1sQDrcCHrdm2TGbWewwDJM+enTm9fv9KIic1Iwgqw/DDHR8wb/FNKcqNoi3X+NwIBzpjF5y9tnIv+ceEaBMfwQFKithDxSbujpbdhiGyVixQxadhQvVu1Yjfv31V4werd7NMcxAJBSqh9ukFhTMW5vlnc676cpyzJqF/IcfprsfLcWcxE7Q5tXyJEymnD7cU4Zh+js9EjsHH3wwfvjhB7z77rsiK4sIh8OorKzEo48+isWLF+OQftDwkGG6S2vrDMAURt4SwOkZWBlH0pUlsa5aFRU7gUqEHGqGmsVUaOgGZxiGyYjU8z322AO1tbUiG+u1114Ty+644w4oigKz2YwTTzwRO+ywQ6r2lWGyjra2L8W07HtAyVNdOAMFJV7srFgBy1ZqPZ2gZw0CkeGwWPpfU1SGYfpZnZ2jjz5aiJ6ffvpJWHRI6FBg8o477iimDDNQUZSQlmZd/CsQHtM/W0UkIhzX2d3+88+wj7YCFsDn/QvBiBYyWweWxYthmCwUO0RZWRkOPfTQVGyKYfoNXu9chMNNsAQcyP/bB+/meQNa7OQ9/TRKqoG6y4EW/6eoi5Qc4uBkhmEyOmZn+fLlmDFjRsLn6bmVK6NdjhlmIEHVgon8qhEwh4HwAHNjhUvaW2zKvwRsYdXiWxkJ5+PqyQzDZLTYoTidjooG/vXXX1osD8MMJMidK4KTyYW1cpi6bICJneD48dq8f4stxNTWAlQ0TYlZT3ZEZxiGyVjLzsYbb5zw+U022QTLli3ryVswTFbi8cyC378YJpMDxX+rbholf2DF7AQ32kibd0+ZAv+224r53OZRMes5HInPIQzDMKmgR2KHigZaLJaEz1M6qUxJZ5iBREPDU2JaUHAibHV+MR/OzcVAIjBxYvSB3Q4l0hnd3mKDw1+he4rFDsMwGSx2KioqMHfu3ITPz5kzhzOymAGHovjhds8S80VFJ8McaZcw0Cw7SnG0iCIJvXBOjtYTq6xabQpKWK1R4cMwDJNxYmfvvffGH3/8gf/9739oa2vTltP8Cy+8IMQOrcMwAwmvdw4UxQ2LpVRYLcwNDYZF9gYC9U88gbbTToP34IM1y47J7UbZqi0w5ilg4jvbcUFBhmEyO/WcmoBSttXHH3+MTz75BMWRO7mGhgYRoLn77rtzBWVmwNHW9o2Yuly7wBRWYI3ErQXHjcNAw3vYYeKPUHSWHXJ+j3oVaJsyHk19vI8Mw/R/eiR26I7swgsvFEUFf/75Z1RXV4vl22+/vSgqOGnSpFTtJ8NkDa2tn4ppXs7eKL7wQpi8XtEUMzRyJAYymmWHGgSHwwMyQ41hmCwuKrjZZpuJP4YZ6Pj9K+D3L4IpBAx55Fe4PvxQLA8PGkQFZTCQkZYd6+LFCJeVqctY7DAMky1ih2EYFa93tpjmLwCKnpymLQ9zoD4Up1NMXZ+qli8iPMCCthmGyQKxc9FFF4kGnw8++CCsVqt43FlwIT1PHdCTYcGCBXj//fexYsUKEfdz1VVXJd1IdNGiRbjpppswYsQI3HvvvUm9hmFSjdf7p5jmL45d3vTvf2OgI91YMcvYssMwTKaJnU033VSIFxI8+sepwufzYfTo0SKD67777kv6dZT99dhjj2HzzTdHY2NjyvaHYbqKzzevndipe/55BCIF9QYy0o0Vs4zFDsMwmWjZ6ehxT9l6663FX1d5+umnseuuuwoR9uuvv6Z0nximK/V1vN6/xHyeTuwEO6gyPtAtOwOtXxjDMFkWs0NWGHJPUdYVpZj3FV999RWqqqpwySWX4O233+50/UAgIP4kZJlyySyRFNf7kNvjOiIDY5zc7l+hKG2wNQC5q9Rl/u23R3jkyJR8tqwfJ4MK0kpBAf/u+ggep+Tgceof49RtseNwOEQT0K222gp9xYYNGzBt2jTcfPPNHbat0DN9+nS89dZb2uMxY8bg7rvvxiDKlkkTQ4YMSdu2+xOZNE6BALlDFdhs0SrAnbF06Y9iWvIzYFIA3H8/7BdcgAoDi0Z/GacuMUxtiKqnbPRoKsWelrfL2nHqZXickoPHKbvHqUfZWNQEdPHixdh3333R24TDYTzyyCM47rjjMHTo0KRfd9RRR+HQQw/VHksVWlNTg2AwmNJ9pG3TF19ZWSmKLDLZMU6KEsDy5TshFGrC4MG3oaDghKTuVurqvhfT4t+BUEUFqk88EaAYshTFkWXaOHUVu8eDUt3jwKaborawkO5aUvo+2T5OvQWPU3LwOGXuOFGiVLKGih6JnTPPPBO33347XnvtNey3334oLdWfytILNSGljuqUufXcc8+JZTTA9DdlyhT861//Mqz9Y7PZxJ8R6fqC5H4x2TFOPt9iBIPqBbiy8grQLhUWntDp6/yepWKasxpoPftsPp7iUCKJDcSG+fOhFKnd4MUAp+P9snScehsep+TgccruceqR2Ln66qsRCoWEa4j+yJVkJCSod1aqoTib+IytmTNn4q+//sIVV1yB8vLylL8nMzDw+ebHPG5pmY6CguNgMiVuJRcOtyIEtQeWaeO90Xr++Wnfz6xDdwJUyKLDMAyTDWJnp512SnGNEq8wgUmo/QT13srLy0NZWZmIz6mvr8fFF18sMq9GxpXfLygoEGIrfjnDdAWfb4GYulw7w+P5EW73d1i1ai+MGvU5TCZjq6Dft0JMbY2A77QLe3V/swX/dtvBs//+CG60Edm8+3p3GIYZQHRL7Pj9fvz2228iVoaEyLbbbqs1Ae0J5JaiYGPJiy++KKaTJ08Wae5UaLC2trbH78MwiSDzq9v9k5gvKDhaiB3C71+KlpYPxDIjwqtnASbAtd4E/0Hb9Oo+Zw0WCxqef76v94JhmAFIl8VOU1OTiIeRTT+lKKFqx1tssUWPdoYah77xxhsJn++srs/xxx8v/pi+QVFCMJmyu/9TS8t78Pn+hMnkRG7uPigruxG1tbeK5yorL4PdPhZOZ/sMRP/6b4FhgMM3iFIV+2DPGYZhmEQkDkJIANWyocylQw45BNdeey1OP/104Tqiwn7MwIUCeZcv3w7BYB2ylUBgPaqqrhbzxcUXwmodjJKS8zFu3F+wWCgGLIzGxpcMX9uSp8b55DWN79V9ZhiGYdJg2Zk7dy722GMPnHbaadqyoqIiPPzww1i/fn2X0sCZ/kNz8+ti2tT0EkpLL+9w3XDYI7KdyErS1/h8S1BXdx9ycnaFogShKG44HFugtPQybR2LpRiDB9+N9evPgNf7e7ttBINV8BTWkBZCfvPG8PbyZ2AYhmFSbNmhmBmqr6NHPua+VAOTUKhFmw8GowHmidiw4WKsXLk7Fi8ehoaG1FoEVSEVdbF2hKKEsXr1IWht/RC1tXfB7f5KLC9q3AoF99xPgTraui7XdmLq9y9BKKRmXUk8nt/ENHc5YHVwFiDDMEzWix0qvGe322OWyXRzKvTHDDyCwbXaPFl2ZOdvI/z+lWhr+1R7XFNzU0r3ZcOGi7B8+fbCYtMZfv/for0DEQ43oa3tSzE/4pIXkf/II8j773+1dS2WEthso8S8z7coZjte71wxLVgEhPPzU/p5GIZhmD7KxqLg5OXLl2uP3W631r4hx6Cz8dixfe+uYNJHILAm5vGaNUdi2LBXkJOzc7t1W1s/MGygmar9aGubEXmfT+BwTOhwfY/n53bLnM7tkLtMtdQ4v/wSrZdF3VlW6xAEAqsQCtXEvMbrnSOm+QsB5aCClHwWhmEYpo/Fzuuvvy7+4nnmmWcSrs/0XwKBddq8yZQj4l6qqq7A6NHfwGSyt7PsECUl/0R9/VR6dUQsqVaTntDS8r42Hy9IjJDuJz0Um2PCPmLesjZqsRKPLWqFcH0QNrnCKHuLKPgb8B3Plh2GYZisFzsXXHBBevaEyVqUpd8DhUBJ4AiUbHIPli/fAYHAavh8C+F0bmno8rLbR8FuHw+/f2FEAO3W4/2g95P4/Ys7XV/GFxUUHIvm5ndQVnYNnOGo6LJQgctAgPy06mOL2oMlFIrWegoEliMcboHZZ0LOCgXeArbsMAzDZL3Y2XPPPdOzJ0zWosz/FNgFKHr7J5j/nQerdSj8/iaEQo0JrUBW63DY7aOF2AkEVGtPT9Fvh/pbdYTJ4wFqVwG5QH7+kSgvvwdmswPmOGuO6+234ZkyJbLPZe2sRtKFlbfCAnM4yDE7DMMw/SFAmWHiaY2EZOWsVgsKWixq36NwOFbskMsnGFwv5m224bDZRhrG/Bh1IddnfCVCusiIUKi6XdaUnrypU6E0q/tiNhcKoSPm4yp0F195JQpuu43Uk6EbSwYn5y9U+z5xzyeGYZjMg8UO0yNCwWb4hqjzTt9QTTyI50JNMesqdUuhKD4gbBLBvpThpK7XccmCdetOxYoVO4uifwn3I9SIcFgVN2ZzkdbiIRGOb79FIGKEsViiridznSpkQkOGRIXRf/+LoWPHIu+DHyLvVSv2Zd2609DY+JxYVvhnSEzZssMwDJN5sNhJE+GwG62tX6Cq6lX0Z4KVau8oew1gcZbHWXZixU7tkn+KqaNWgQlWTZTEW4Bkj6q6ukewZs2xohEnCZmGhicS7gdlSanvXa61c+jIlRUcWoFQJHHQHFZnnB9/jLzHH1e3F1dLiih4+SNN7GzYcAHa2r7Qniv5FVBMJih5eQnfk2EYhukbWOykiVCoTlgkFi06Q1y4+yOmhgYEqtX07dwVgMnrTWjZCYdb0ViqxrcMmw5Y1q/XRFG8BYjw+eajru5urREn4fGoDTqNaGv7Rkwdjo1ht0/oNEg5MChHO/rtq+thampCyTnnwPGT+h7h0lK0nXpqzGvsDdHYIH0lZRc2h7UNqtAx80+KYRgm0+Azc5pQeymRhcKnuVf6DYqCgn/9CxWbbYbwnHeiYqdNLdAnRYzb/a1WQ0e6lGz1wMjXAOuiRQndXeprVfGih7ZhJBxpWXOz2kCWgo0djo0i6ycuLBgOqtYksxfI+fRL2H+OrbkT2GQTNN94IxoeeAB1L72EcE4OnFSYOSx/MorY/7KyGzB61u7q5xg+vNOhYxiGYXofFjtpggJeZUxKINB5C4Vswvb778h7/nkx781VM5PyVgDm1lYxL91TZJ1Zv14tVeDz/S2muZEYYtvff8NiSezGIqEUTyLhSG0eAoEVMJvzkJ9/KGy2MTGuLSPCEYFlbQXy77sPrvejNXpazzgDbWeeCSU3F54TToBv773FY3MAcDUWxrSQGLJwK5Tf8qz6unPO6WzoGIZhmD6AxU4aoSDcZPtFZRPkgiIUXSYW9YWKt+wQ1Bpiw4ZLtBo4UuzYZ82CtdZnGKBMjTXd7qj7Kva59mNZX6/G8hTmnwTn7wvgWh0QjwOBtVAUNXA4npDSrIkdUyikiZ36J55AM2VfORyx648Yoe7/2uhyh2Mz5D/4IEw+H3y77QbPMcckHDOGYRim72Cxk0as1op+KXZkbI57NBAkXRMCclYBpohlR1psJC0t76CxUW34mRMRO86vvkLF3keKeUXxIhxWhQ9BBf5oo9S6IZ5AYEPMY693HjweypKyYuinOSg78kgM3XcKTAoVAgyK7upGhKHuK8XaiM8UCkFxOODbay/D9TWxszSoLSu0HgT77Nlivun22+kL73DcGIZhmL6BxU4asVoH92uxs07VKij7AbD4o2LHZHJq6+bkRItQ2tpcGKQLxbGKlmomMR8INLQLRM7PP7zde8ePZXPz25F1D0Hur6rbyqQAdm9hh66skFnt52Y2RdPOvXvtlTCbKhiJxxn6ZhuKVg7HpH8Bw/c+XVh1QoMGIThunOHrGIZhmL6HxU4a6a9uLFF9mFLJd1Efl8+O1NfxemFqbNS6gxODBv1LHGYUzLvJEyNgb9ZtRwEsSq6YDwajYkdmUTnd0Vo3kvixlNlaubkHwLomWpzQWae6m6hthREBu/oZLA61KjLhPeSQhJ85NGyYmDoqPdjqjLUYRAKvqkrd3513JoWX8LUMwzBM38JiJ430W7Hj9cJfCPjVVlFwhTfRnivfd1/YrEMxYsQHGDPmRzgcm2DkyA+x+Yt7ouTD9qnglpAqdgKBOq0+kayoPOywSzDx3V1QNBsobtlVLNO7pSiLi4KgiZycnWIad+asCotpY+MLhl3VvYNUt5k1Z7yYKnY7vPvum/hDO50IDVYtdfH4SOwwDMMwGQuLnTTSn8VOq6oR4FoLmPPUNHvCsmGDyNZyubbR2kFQM9DiZ98z3JY14BLTOXN2x/r1FyNQOUuEPtsaAWeVD0MfnoWtrgTyF9vbNeFsbf1EXdc2FtZQEcwRSwsx4iUvzOZi+Hx/oa3ta7Gsqel1Eczsa/kLnqGqGDKP3wueQw5B83XXQemkiadv8mTD5f5dIiYuhmEYJiPhiMo00m/FjseDVrVuH/KWAmFdawVi0BFHoPbVV+HfYw/D1ysWiwgIJhytxXC7VmqBzMG6L4HSaCCzxF6pxtiEQvXasoaGZ8S0sPBkkSFm0tXgyfm7AfnO49DkfhMezy+w2UagquoK8ZyQS5Fm7LaizdDw1GlJfe6Wyy5DzhtqPR+Cau+Ehg7leB2GYZgMhy07vSB2yBpBzSz7k2XHrRpt4GoZhNazzmq3jsxSEgSjGUxE3euvw7/FFurr62J7SXlK1TR0RzjyBhEca5pjxA41BqWO6URh4fGw/fmnmA9MnIhwJMg4x6Oanzyen7VUdrM51npjc0ZMVEkQGj0a/u3UDLFQeTlqvvgCdW+/zfE6DMMwGQ6LnTRCRQVNJlvkek/ld/uP2PFGwlf85/wLSnExGh55BJ6DD46uE7HcGHUSDxcXi7o0RE6ly/g9dj8l5rFjhRQ56jQYVAOPyVVlMRcj/9FHxWPvAQdoTTzza4dG09ObVVdWcfEFKA2oaWSOGktME9BkqH/uObRNmYKGxx5DaORIhMuiAc4MwzBMZsJiJ42YTGbY7eoFN1G9l2x1Y0mxY7WqKdlUUK/h6afReu656hOR9HRCZi0R4aIiBEeNEgKJyF1tfAja8jaPeexcUqM1FyUrGRUMFOvZRsKybBlsCxeKIOPW885DOBJI7FwfgslEYiqAVp/atDOvcSSGLj8Am/4H2OiViV3+7NQzq+n++zlOh2EYJotgsZNmHA4pdqIX/EyBRIO+mF/SeD3wRTKxbLbYflCKU62xk//443B++KGYl4HD1G+q6scfqc+CED2Ea3EThr0FDH8dKK85UNuOPXcSwq6o1cfWRAHFqruopuZ2rX4OxeI4vv9ezPu33VaIKJk1Za2qgd2uto4Q69YDpW8vhHXtOpR/CzjN7TubMwzDMP0PDlBOMw7HsIwMUg6HPVi5cjfhaqM0cbM5WgiwM4K2ZiiUHKWYtbikeLFDlJx3HtavWiUytMTryKITyXgiVxZhXbkKE36IFAj0F8OzHWA1l8E6sVRdN1LTxxSmujw2KCa/Vo2ZKH76QxQ9pYoq365qenqoQq1cXXD77XBuuyV86luJgoYFj0zVXsuNOxmGYQYGbNlJMzabmpYdCql1ZDIFv3+ZEGA+3wI0Nf2vS6/15ajBwrZgEUymWL2s6KwxhHXxYi14ODh2bIw7Szy/bp22LOfrn7HthcCEmWoWF7mlYrZtal8vx7khmhnlPfRQdb38aNBzzndztfkhM4xbQDAMwzD9GxY7acZqLdYK4GUSwaDazJNobf28C6+rwaqD1OBgZyhaKdnIskPY586FY9asdvVopNjRY12+PPY5i6XDfSmYBwz6Dqh9/XVUzpuH4AQ1Hz6w6abaOoXzIjMhoPXjZcKVJmGxwzAMMzBgN1aasdmKtcDaTBU7oVBymWKKEsa6df+Ab5wHZg8wxHdGp2LH9fbbsK5eDcVqhX+HHbTl0o1lhPacOVaL59SWwl1WB5d9B2x51C9wVgMNjz4KfySzS0LNPBvvvRfOTz7B4C+/RNgO5Dp3RdujTjQ8/jjKDj4YpmBQpKkzDMMw/R+27AxQy04gsD7GWpMMra0fweebA7PXhK0vB+z2cZ2KHQcFJNN7jB8PJVdtDZHIshMvdqj4oJ6NXt0U5eX3YLT7ZiF0QsXF8Bx9dPsNmM1wn3QSvPvvL2J9hn4MYHO1FURw4kRU//gjambMaFcMkWEYhumfsGWnl8ROOKwWy8sUgr5o00yyOlHAstlsXPNG0tLyvpgO+8iB/MVeeOLic4xidhK6jJxOkW1ljgQgx2wjgRur8J3v4NwQRGCzJeo2R8YWHoxHurX0wctEeNAg8ccwDMMMDFjsDFDLjunPLwBd8eBQqAZmc2LxoChBuN1qinfpD4qhFSfRMiJoEB8jRI2B2NHcWAYxO2QpktaizmJuAptvLtLQRW0fXawOwzAMM7BgsTMAY3Yo9sZT2hazjOoAycadRni9cxAON8NsLkLBvKakxE64sBDmpqaEad4kamRauh7/9tur2+okQLntlNgqy+32JTcX1d98o8b+xMX/MAzDMAMHvgKkGau1RLPsKLpGlX2J1/sHAsWApRXIqSlJKm6HmmkSOfbtYA5GLDuduLH8W0a6bSYSO/q4neOPF9WXq374QYvt8R5yiLpv1JYhriN5zUcfwb/77p1+VkpD18cKMQzDMAMPFju95MailgWK0t5l0xe0tX4mpiW/AI4GW1JFD73134pp2TQ1l1txOAxFhN6yQ26kjrKvYpZttRVabrpJNNuUUOuH+scfR+2770LJydGW1730EgJbbZXch2UYhmEGPBnlxlqwYAHef/99rFixAg0NDbjqqquwgy5dOZ6ff/4ZM2fOxMqVKxEMBjF8+HAcd9xx2CqDLoQWS25kmIMIhRphNkcv2n2Fu00VLqW/AC2womEi4PNFi+/FQxYpj+93wAYUfx5p/UDBv0ZuJp1lh8QMCRbrkiUxaeeGlp04y43AZoP3iCPUeV1ncd/eeyf5SRmGYRgmwyw7Pp8Po0ePxllnnZXU+gsXLsQWW2yB66+/HnfddRcmTZqEu+++W4ilTMFkMsFiKeybuJ1gELa5c8VUEgq1wOv/S8wX/QEUzVVdUs3Nb0FR2lcoVl9Ti5DNDYSBvKWRTW+0keG6essOzTf/+9+of+klMnG1367etWUkdvTb5ZgbhmEYpptk1BVk6623xpQpUzq05uj5xz/+gSOOOALjx49HRUUFTjrpJDH9/fffkUlYLKWaaOhN8u+9F4MOPhgFt92mLfN4fhblhJ3rIGrVFP1AAkwVIqtWHWS4nUBgpZg6qgFLQF0WGtW+enK7oOVOAozdU6ZEHyRIWZcEN+amnQzDMEw/EDs9JRwOw+PxIC8vD5mEbJYZCLTPPEon+VPVppd5T0cbZ3o8atfN4j/Ux7a6NhQVnCrm/f5FCAarE4odV7QOIXw779y52OnEGhMuLxcxOZ6jjgIifa0S0XjnnfAceCBq33yzw/UYhmEYJqNjdnrKBx98AK/Xi50TXIiJQCAg/vRuJlfEqkDzqURuz2ZTu3CHQlUpf4+u7ovbPUtzYUmG5FwDt/cn+P0LRaZWfv6B2nP2b75BcJjayNO1Dmh48kkRLBzYdVcYfhK9u8pi6fTz+o48Ev6jjoLL6exwXWX4cDQ+95z6WTAwkePTV8dQtsDjlBw8TsnB49Q/xqnfiJ3vv/8eb731Fq6++moUFqoxMkZMnz5drCcZM2aMiPMZlMaKukVFE0DlZuz2ZuFm63WsVvG+fn8N/v57fnuxk5uLJtOu2LBhIazWxaioiPS8ouJ9J56Iqn8B2Ee17BRfeSgwdGhSb1tUUoKiLnzeIdy+ISl4nJKDxyk5eJySg8cpu8epX4idH374AU888QSuuOIKEbDcEUcddRQO1blMpAqtqakRGV2phLZNX7zXq7rVmpuXYoNBEb10IWVGOD8fVRs2oLHxFbKRwOUdCUeD2rmcqFm+HOFytZxyXd1vcLnUfcx9/30omwDVe6rr5TSUQDzTyWeQ71szdCiCSXxeOU6VlZUZU4soE+FxSg4ep+TgcUoOHqfMHSer1Zq0ocLaHyw6//3vf3H55Zdjm2226XR9m80m/oxI1xdktaqX/0CgF38suvchsUPv29LyiXhcWE/1b6JiBy0tsFQM1ooLavsYCKB6H3JHAaXfA668PSHytTr5DNUzZ8K6dq1aZ6cLn5fel08mncPjlBw8TsnB45QcPE7ZPU4ZJXYo3oZUoaS6ulrU0KGA47KyMkybNg319fW4+OKLNaHz2GOPiaysCRMmoLFRbbZpt9uRoytClykBysHgBvXiHwoZpmKnElNrqzYvi//5/QvENL92RLt1rVZV7IRC0QBlauXQtq06X/Y94D9gt6TeOzhpkvhjGIZhmEwgo8TOsmXLcPPNN2uPX3zxRTGdPHkyLrroIlFosLY2mr79+eefIxQK4dlnnxV/Erl+pmCzjdCabRZcdCZcP84RVYCDm22Wtvc01+jaP5hMCId9ov8V4axzxK7b1gaLZbzOshOGyWSGZfUqtB2nrpO3AnDrOoczDMMwTLaQUWKHigK+8cYbCZ+PFzA33XQTsgGLpQgWyyAhdsKLZsJSDQw67DBsWL48pjJwSt9TJ3ZMXi+CQcobV2AyuWBrDBtYdsoij4IIhxthsZQg3LwKASpyHALs4RFoMehvxTAMwzCZTr+qs5PJ2O2q5cQdqcVn8vthWa2Lm0mjG8vk8SAQWKNZmcxt7th129pgMtmFwCHIAmSur4c7b52Wch7Yfd+07SvDMAzDpBMWO72E3T5BTN0jo8scP6gF/tKBya0TNMKys1bM2mzDYVugxu5IzBFhZLGo1p3Vq4+A6atX0LS5GmTmKtgVLddfn7Z9ZRiGYZh0wmInnfh8wG23wbpgQdSyo4sNtv/0U9remqw52jyJnYaF6ns252jv691vP/X5iNhRFLXYoqK0YfmIR9AQSW6zb3yCYYdzhmEYhskGWOykkfwHHgBuvBFl+++vZTuJGJgI1pVqG4Z0W3bMHg/M370j5oue/RAmRYF/yy0RiDTzJDcWUVh4svYaX5EbrRPVeZdrx7TtJ8MwDMOkGxY7acQxY4aYmsJhLR4moCvunNaYHa835nHAqqblO+rUx77Jk6FEeohJN1ZJyQUYP345Skuvju5jKB9W67C07SfDMAzDpBsWO2nEoqsZpBc70qJCGVN6d1Mqid+uX228Dkckc9+3664IR8SOPpjZbHYgL29/7bEzNDJje50wDMMwTDKw2Ekj5ubm9mKnAAiOGoFwpH9Xuqw7MQHKJG4imeX2GkCxWODfdlvRdZxwfvIJrEuXauva7RvrPoQlLfvHMAzDML0Fi510EY6tZWOxFKszZsA/yIngCDVS2bJqVdotO0EnEMqLWnZCw4ZRIA68++6L4NChIoanfPJkmCOWKCooWDHDKebLcHpa9o9hGIZhegsWO2mCWi1Iwi4XTCYbLH61crF/kB2hkWoOujVNlh2zzrLjj1h1LG7A6gFCsjigw4Gmu+7S1rMuWaLNj/uvGTudAORwcDLDMAyT5bDYSRPhnBy0XH65mDdRCrqiwOq1i8fBEgtCo0al142ls+xoLqxIvE64tDT63D77iPidmKrLigJLixfOakBxudKyfwzDMAzTW7DYSRNKcTHazj9fy8aimjvWNrU7R9vgFlRutwHesvRZdvQxO56IIccZiZdW4pqkhgYNiu2n5fer+2ywLsMwDMNkGxnVG6u/oRcKJD5srWqw79qJn4pp1ZPAFvctS7tlR1Ztzl0VtTrpCUfEjrTs6IUSW3YYhmGYbIctO+nEaqXUJk185K6LFQ6BEqBm0irhNko1esHSFunHlbMaUKxWtJ11Vsy6MivLXF2t7StB68JmS/m+MQzDMExvwmIn3UTaLFAV44qvIxlZAPJyDhbTxq1CUfdRCpGCJTh6tGbZCR19OSrnzUNozJiYdUNlalCP3A9N7LALi2EYhukHsNhJNxHBQCnmzhWN2OpyYHjjBRg0+CaxvJlK2qyObczZU/Ieegi2RYvEvHerjeAboi635WwCpaCg3frSsmOJFzvswmIYhmH6ASx2esmyU3raaSIYuWguUGg6ADbbMLiqXYAF8DR+mdK3LLj3Xm2+dVu1J5etETAXjzZcP96yQ1YogsUOwzAM0x9gsZNuDLqFS/dQQaWaJtWm/Ja2t2/dSC0OmLMKCBdH3Wgx+yOrOdfWoviMM2D/+Wd1OYsdhmEYph/AYqcvxE5EROT5thDT5uKlUNIQpEx4Bvu04ORwxIKTaH8I18yZKLjzznbLGYZhGCZbYbHTh5YdV+4uMAUAf0EbAoGVKXvLUKRoILWC8LnUDKvQMf8UFZONSBSIzAHKDMMwTH+AxU4fih0MG4fCeeqs2/1Nyt7S1NYmpnXvvINAcK2Yt5apViQjFKfTeDlbdhiGYZh+AIuddGNgHZFiJ5yfLwKWCa93TmreLxiE2etVt5+bi3C4KbYRqRFmM8IGgofFDsMwDNMfYLGTbiJp3TFQsb6I6JH9qkKh+pRadcT28/IQCqlix2wu6vB18cKGhJL3gANSsk8MwzAM05dwu4h0M9o43VuKHZuqRRAKNqRU7Cg2GxSbBeFws3hssSQhdhrUfaieORPBSZNSsj8MwzAM09ewZSfdxFUrbid2VC2CULAuJW9nlmJHuLBo42qWl8Wippd3tC/avEGcEcMwDMNkKyx2+lLsOJ1RsRNOrWUnrHNhmUwumExqj66O9kWb5ywshmEYph/BYqcP3VgUGGzxqengYaUFihLq8duZWlt1lp2mpKw6cl8kbNlhGIZh+hMsdtJNJ8LBEpJWFEUTJ6lyY0WDkzvIxJLoihpyFhbDMAzTn2Cx0ws0PvRQwudMjlxYWlOXkaUFKAvLTmPylh19BWedlYdhGIZhsh2+qvUCnuOPR93//ifm2048MXFGVtsGuN59F6bmSCBPD9xY+pgds7lzsWNKU7sKhmEYhulrOPW8l/Dtuy8qZ89GeNAgw4ws7zDA8fLjKL7rW3gOOAANzz2XAstOF2J2WOwwDMMw/RS27PQi4cGD27mIKD7GLrPO//5WTFwzZqQoZke1EJnNBZ2/kMUOwzAM009hsdPHkGUnb7k6v/RSoHL/nm1P78YKh1uSFzsMwzAM009hsZMBYid3WfTxoutlGUCkwI2lCh+LJS+JHWHLDsMwDNM/YbHTx5AbK08ndgjf4FSJHWnZyU9qPxiGYRimP8JiJwMsO671gHN9dNkvLwDhYPcysswxbix13mzu3LLTeN99CI4ciYYHH+zW+zIMwzBMpsJip48J5+TApADbXAiUzooscwKt1dO7tT2T262JqK5YdoIbb4zqH38UafIMwzAM05/IqNTzBQsW4P3338eKFSvQ0NCAq666CjvssEOHr5k/fz5efPFFrFmzBqWlpTjmmGOw5557IluQfajsTcDo54G6XdTlgda/etYuoouWHYZhGIbpr2SUZcfn82H06NE466yzklq/uroad911FyZNmoR77rkHhxxyCJ544gnMmTMH2UK4qEibz18KjH9Ynff7l/TIjRUrdjq37DAMwzBMfyWjLDtbb721+EuWmTNnory8HKeddpp4PHz4cCxatAgfffQRttpqK2RN7Z0I/u22g8NBhQCXwIdVPXJjkXuMxQ7DMAzDZJjY6SpLlizB5ptvHrNsyy23xAsvvJDwNYFAQPxJTCYTXJFMJJpPJXJ7HW03PGRIdL64GM4WmxA7flsNFMUHs9nZtffULDtOKH6vmLdY8lP+2Xp7nBgep2ThcUoOHqfk4HHqH+OU1WKnsbERhYWxrRDoscfjgd/vh91ub/ea6dOn46233tIejxkzBnfffTcGxbVxSCVDdIKmHVtuqc06KyrgaGmG2QeEHQpKShS4XBXJv1E4DERSz0tGlwOL1cXDhk2AyWRBptPhODEaPE7JweOUHDxOycHjlN3jlNVipzscddRROPTQQ7XHUoXW1NQgGAym9L1o2/TFV1ZWQklUtE9RIOWMm1xQZotoH+EdCmzY8JdmdUrq/draIA+z9S2VkX3IQWVlNTKZpMaJ4XFKEh6n5OBxSg4ep8wdJ6vVmrShIqvFTlFREZqaIi3DI9BjEghGVh3CZrOJPyPS9QXRdhNu2xr9Ckxer3Bl2etVsRMIVMHpTH6fTC1qqrliNiNsC2jxOtnyA+1wnBgNHqfk4HFKDh6n5OBxyu5xyqhsrK4yYcIEzJs3L2bZn3/+iYkTJyIbMfl8IhWdxA4RDFZ3O+08pNXYyU39jjIMwzBMFpFRYsfr9WLlypXiT6aW03xtba14PG3aNEydOlVbf//99xfrvPzyy1i3bh1mzJiBH3/8UaSgZxOe/dXun61nnSXaPMgu6KFQVZe2Y9YVFAwG14h5q7UHvScYhmEYph+QUW6sZcuW4eabb9YeU7FAYvLkybjoootEoUEpfAhKO7/uuuvwv//9Dx9//LEoKnj++ednTdq5pOGpp9C8fj1Co0bB9tdfcKzomWWHWkX4fEvFvN0+IfU7zDAMwzBZREaJHSoO+MYbbyR8ngSP0WuooGBWY7MJoSMbcqbCjeX3s9hhGIZhmIxzYzFqt3JHjTofCKzu0mvNuo7ngYAUO+NTv5MMwzAMk0Ww2MkwKN4mT9UpCASWIxRSA427YtkJ5efC71fjnuz2cenZUYZhGIbJEljsZBgiQLkRcNRTerwCn+/PLosd/yAqIEg1gyywWjOzwBPDMAzD9BYsdjK0C3recrUWkNc7p8tuLN8gk5aJlQ2VkxmGYRgmnbDYyTCogSdROF/9atzub7tcVNBXGhJTtuowDMMwDIudjLXslM5SK1C63T8lHbdD7SIIf6FaPdlq7UJfLYZhGIbpp7DYyVCxk7PMC5ttjIi98Xh+Suq1ZhmzU+ATUxY7DMMwDMNiJyMDlAlTKASnbQsx7/PN71qAcq5aSZnFDsMwDMOw2MlIy44S6cTuDI8VU59vQZfcWAGnKnpY7DAMwzAMi53Mw2JBuKhIzLo8FV0SO9KNFbCrneBtNhY7DMMwDMNiJwMJl5SIaU5DqZgGAisRDnuSsuxQWHPAovabYMsOwzAMw7DYyWixU/D8OzCb8kRxwUBgbVIxO4FCQDFRQUHueM4wDMMwBIudTLbsfPgRnFVmzbqTjNjxDVLnLZZBMJns6d1RhmEYhskCWOxksNghchY2J9cUNBiE2euFr0x9yAUFGYZhGEaFxU6Gix3XBnUaCKxKKhNLWnY4XodhGIZhVFjsZCBKQYE271yfWOw4Z8xA2aGHwrJ0qVZjxzdE/UpZ7DAMwzCMCoudDERaaWItO+3dWMVnnQX7H3+g7OijtSag3iFWMeW0c4ZhGIZRYbGTgQRHj9bmXZplZzUURe2XJTFFHlvq6mBqUmvr+AbLjuccs8MwDMMwhGoGYDIKzzHHwLJ+PWzz5sHx+QwgDChmL0KhqhgRE9hkE9gWLhTzzs8/F1N/qSqA2I3FMAzDMCps2clErFa0/vOf8B50EMwhwNHkMHRlmTzRQoM5r78uCgr6imWNHRY7DMMwDEOw2MlgwoPU1CpnlcUwSNnUrKalE5aaGoRygbAjLB6z2GEYhmEYFRY7GUwoInZca0Ni6vfrxI6iwNzSErO+TDs3mwthNuf04p4yDMMwTObCYicLLDuuFb72lh2vF6ZAIGZ9rrHDMAzDMO1hsZPBhAsLxdS1Du1iduKtOkS0ejKLHYZhGIaRcDZWJuNwQLHb4drgb2fZkfE64YICUZfHFAqxZYdhGEZHMBiE2+3u8XY8Hg/8fvU8zPTeOFG5FavVitzc3B5vi8VOhhPOzYVzvXrwhEI1CIfdIh5HWnbC+fkwm0yizo4UO1xQkGGYgQ4Jnba2NuTTOdLcMyeGzWZDIC5sgOmdcaLv0OfzweFQs5K7C7uxMhwlPx+2VsCs5Ma4sqTYodYSJIgIbgLKMAyjQhadVAgdpm/JyckRYqen8FGQ4Sh5eWLqCJTHuLI0N1Z+PsJ5uWiZCHiGqq9hNxbDMAxlpvIlLtsxmdSuAD2F3VgZTjgiduy+EnjsKzSxo1l28vNRu1kVlpwafQ1bdhiGYRgmCsveLLHs2N2FMW4sfYDyqsMj3UJpOWyw2cb0yb4yDMMw/Zdhw4bh008/RTbCYidLxI6zKTemsKDesmMORg10VsswmM3OPtlXhmEYJjX89ttvGDFiBE49VWe2T4Idd9wRTz/9dNr2K1thsZPhUEwO4WhwGcbshApz4SuI9shy5WzXJ/vJMAzDpI7XXnsNZ5xxBn7++WdUVlb29e5kPSx2ssSy46q1iWkwuAaKEtIsO57BASg2tdP5oK+BQeX/6cO9ZRiGYVKRbv3+++/jtNNOwz777IM33ngj5vmZM2fi4IMPxtixY7HZZpvhrLPOEsuPPfZYrF27FjfddJNwOdEfcf/992O//faL2QZZf8gKJJkzZw6mTJkitrfxxhvjmGOOwbx589BfYLGTJQHKjioSNFYoih/BYCVMUuyUtYlp/gJg0s2AxVLSp/vLMAyTkSgKTG53t/5AhVu7+1pFvRntCh988AHGjx8v/o4++mi8/vrrosAe8fnnn+Pss8/G3nvvjRkzZojnttpqK03AVFRU4KqrrsIff/wh/pKltbUVxx13HN59913x/mPGjBEuNFreH+BsrCyx7JhbPbDZRiAQoIyslTBH3FjeIlX05EQ7STAMwzBxmDweVEyY0Ovvu2HJEig5XWvM/OqrrwqRQ+y111644oor8OOPP2KXXXbBI488giOOOEIIGsmkSZPEtLi4GBaLBXl5eSgvV8uVJMtuu+0W8/iee+7BJptsIt433iqUjWSk2KFob1KWjY2NGDVqFM4880yhcBPx0UcfCbNebW0tCgoKhGnupJNOgt1uR78ROy0tsNlGC7Hj96/ULDvevDoxzVlnQdNNN/bpvjIMwzA9Y+nSpcKl9Oyzz4rH1C7h8MMPFwKIxM78+fNx8sknp/x9a2pqhMCZNWsW6urqEAqFRPuHdesizRmznIwTOzTQL774Is455xxMmDBBCJnbb78dDz30EAojjTH1fP/995g2bRouuOACTJw4ERs2bMDjjz8uChGdfvrp6C9uLBI3dvumIKsoBSlrlh1HtZj6rnsMbcWH9em+MgzDZCqKyyWsLN2BBAe1n+ju+3Y1MJnea5tttoluQ1HEzTtdC51OZ7eKKypx7rT4z3P55ZejoaEBt9xyC4YPHy7ej0RWf2mTkXFi58MPPxQBWWS6I0j0zJ49G1999RWOPPLIduv//fff2GijjTQTHJnudt11Vyzp5kGdaYRLS8XUXFcnLDsEWXekZcdvUaP0bTkb9eFeMgzDZDgmU5fdSRo2G5ReuOiTAHnrrbfw73//G5MnT455joKQKZ6GXEt0k3/CCSck2FWbsMroKSkpEZYbEjyyIjFZiPT8+uuvuOOOO8T1lyCLTn19PfoLGRWgTF/08uXLsfnmm8coUnq8ePFiw9eQ0KHXkOmPqKqqEkFZW2+9NfoD4Yjf1VJTA5ttlJgXlp2WFgRdQNikBijbbJFeEQzDMExWQsHHTU1NOPHEE0VGlP6Psq/I6kPxOyR67rvvPnFTv3DhQjz22GPaNqg2D6Wrk5dDipVddtlFuKbI67Fy5Uq88MILwoCghwKS3377bbFNMjBccskl3bIiZSoZZdlpbm5GOBxGUVFRzHJ6vH79esPXkEWHXnfjjWq8CilaCqaSwV3xkElOb5YjleuKmBlT1YNDv+2ebleKHXNTE+xQ0wgpZgd+P3wj1HXM5nxYLGo9nmwkFeM0EOBxSg4ep+Tgcco8KC6HrmkUexoPiR0SK3Q9fPLJJ0VoB4kcCkbeaaedtPUocPnaa68VHg5qoEkWmgkTJgirzaOPPipeR9s677zz8Morr2ivo/T0a665BgceeKDI6Lruuutw6623IlPo6XFqUuIdeX0IqdDzzz8ft912m4i/kbz88stYsGCB+LLiIVMcfXlUH4C+UCq+9PzzzwtTHNUciIfqFZCZUK9m7777bmQs9PWQuvb7EV65GN+uIHeVgl2OAlrHAX/eR11hN8UOO8SaJBmGYQYyZPGnrudM9tPS0iJqCvUbyw6pWXJbURaWHnocb+2RUI2BPfbYQ/Mzjhw5El6vF0899ZSw7sR3vT3qqKNw6KGHtlOL5M/sbgBaImjbQ4YMEQKsJ5qyvKwMlvXrUT9/CawFQxEMroNnGOAdTiZGL5UTFCbLbCVV49Tf4XFKDh6n5Ojv4+T3+1MWXEtxMP0lUDedpGuc6Ls0usZR4PigQYOyT+zQjpN6++uvv7DDDjuIZeTWosdkWjOCzHTx5q14gRP/ZdCfEen6wdN2e7Lt0KBBQuyYqqthKx2lip2hgG8ofQ4vrNaKfnGy6uk4DRR4nJKDxyk5eJyYbKCnx2hGBSgTZHX54osv8PXXX4uy188884wQNHvuuad4furUqSLVXLLtttvis88+ww8//IDq6mr8+eefwtpDyzsSPdlEOKJcKUjZblc7mrtHAb4hFjFvtQ7p0/1jGIZhmEwmoyw7MmqcAo4ptobcV6NHj8YNN9ygubGocKDekkP9O+gxRalTzA+5wkjoUDR7fyEkM7JWr4bDsamYp3idcKGqdMmywzAMwzBMlogdglxWidxW1OBMD5XGpn4e9Ndf8e+0E3KnTYPrww/huOwBTexYoMYY2Wwj+3gPGYZhGCZzyUixw8TiPfBAhF0uWFeuRM5Ks/jW/MKzJWvsDO/rXWQYhmGYjKV/BLX0c5TcXATHjRPz9qpWONpiM9OsVrX+DsMwDMMw7WGxkyUokXoRpuZm5FVGu9laLENgNjv6cM8YhmEYJrNhsZMlhCNix9zaioJVxdpydmExDMMwTMew2MkSFF3386JFuTD71OUu1459u2MMwzBM1nH55ZfjzDPP1B5TxwFqQNrbzJo1C8OGDRM9wdIJi50sc2NRA1BnZQDbnQNMWHYlysqu7+tdYxiGYVIoQujiT39UeoV6XD344IMpr/Afz9NPPy16Y2WSQEklnI2VZW4ssuzQX84awGPZDD5u4scwDNOv2GuvvfDAAw+INglUZPf//u//RIcB6kSuh5632+0pec/i4mh4RH+ELTtZaNkxNzeryww64zIMwzDZDQmY8vJyDB8+HKeffjp23313zJw5U3M9Pfzww9hmm21EX0iCOptTF/NNNtkEkyZNwhlnnIE1a9Zo2wuFQqJGnXyemm3Ht1+Id2NR54Lbb78d2223nWiYTRYm6spO25V17TbddFNh4aH9ku2dqLM6dWEfN24c9t13X3z44Ycx70PijTq70/P0nvr9TCds2ckSwjJmp7kZ5spKMR8qK+vjvWIYhsmmHmCebr02HLYiHO6eG8lkcrXr39hVnE4nGhoaxPz333+PvLw8ITwIarx58skni84B77zzjrAAkRiiZZ9//rkQTk8++STefPNN3H///ZgwYYJ4/OmnnwoBk4jLLrsMv//+O2699VYhalavXi26FAwdOlS4vM455xx8++23orM87R9B7/vWW2/hrrvuEgLpp59+wqWXXorS0lLsvPPOQpTR60jA0f5Re6dbbrkFvQGLnSxBWnFsCxfC7HZDsdsRGj26r3eLYRgmKyChs3TphF5/3/Hjl8Bkyum2QPvuu+/wzTffCGtNXV0dcnJycN9992nuq7fffltYVGiZFFXkAiMrzo8//ojJkyeLHpMXX3wxDj74YPE8iRHqP5mIZcuW4YMPPhCCSlqPRo0apT0v2zeVlZWhsLBQswSR2KHXkDVIvubXX3/Fyy+/LMTOiy++KJb95z//iYzNeCxatAiPPfYY0g2LnSyz7FhXrRLT4PjxVE2wj/eKYRiGSTVkkSELDAUlk5A58sgjceWVV4o+kRtvvHFMnM6CBQuwcuVKTJw4MWYbJD5o+dZbb42qqioxlZD1Z8stt0zYSXz+/PmiFRMJlGSh93K73e36UpLlabPNNhPzS5cujdkPgixSvQFfLbOE+PgcGbDMMAzDJOdOIitLd7DZrAgEuu/G6k5D7DvvvFOImsGDBwtxIiHLjp62tjZsscUWIlYmHnIfdQdnxC3VFWg/CLLeDBkyJOa5VAVR9wQWO1lm2ZF499+/z/aFYRgm2yAXT3fdSWazDWZzAL0FCRqKeUmGzTffXLicyKVE8TNGDB48GH/88YcIHCbIYkTxMvRaI8gFRhYlcoNJN5Yem82mBT5LyLLkcDhEXE4iixC5rT777LOYZbNnz0ZvwNlYWdQfS9J6/vloO/30Pt0fhmEYpu85+uijRdo4xfT8/PPPIpCY6uDceOONWL9+vVjnrLPOwtSpU0VQMrmSyB3WHMnqNWLEiBEi44pcZ/Qauc33339fPE9ZYiQeyd1GcURk1aGg6QsvvFBkfb3xxhvCrTVv3jw899xz4jFx2mmnYcWKFSLomfZj+vTp2nPphsVOlhAaORK+nXaC5/DD0XzjjVQ6ua93iWEYhuljXC6XyMKiFPCzzz4be+65J6666ioRsyMtPeeddx6OOeYYkSJ++OGHIzc3FwceeGCH2yU32iGHHCKEEQU5X3311fB41Gy2iooKIYRoHYr9oTpAxHXXXSfeg4QV7QdlXFGq+ciRI8XztI9PPfWUEFD7778/XnrpJfGa3sCkJIpQGmDU1NSIQKpUQsqXDooNGzYkDARjeJyShccpOXickqO/jxNZLgpSVIuM3Dapvj70R2xpGqdE3yW936BBg5LaBlt2GIZhGIbp17DYYRiGYRimX8Nih2EYhmGYfg2LHYZhGIZh+jUsdhiGYRiG6dew2GEYhmEYpl/DYodhGIbpl1AVYCa7SVVZBBY7DMMwTL+DWi60tLSw4Mly3G63aEPRU7g3FsMwDNPvoOaZVCm4tbW1x9uiRpZ+vz8l+9Wfsad4nMiqQ98jix2GYRiGSQBdKHtaRbm/V5pOFZk+TuzGYhiGYRimX8Nih2EYhmGYfg2LHYZhGIZh+jUsdhiGYRiG6ddwgLIukC0bt92f4HFKDh6n5OBxSg4ep+Tgccq8cerKe5mUTAybZhiGYRiGSRHsxkojHo8H1157rZgyieFxSg4ep+TgcUoOHqfk4HHqH+PEYieNkNFsxYoVGVlzIJPgcUoOHqfk4HFKDh6n5OBx6h/jxGKHYRiGYZh+DYsdhmEYhmH6NSx20ojNZsOxxx4rpkxieJySg8cpOXickoPHKTl4nPrHOHE2FsMwDMMw/Rq27DAMwzAM069hscMwDMMwTL+GxQ7DMAzDMP0aFjsMwzAMw/RruNlHmvj000/xwQcfoLGxEaNGjcKZZ56J8ePHY6CwYMECvP/++6LIVENDA6666irssMMO2vMUF//GG2/giy++QFtbGzbeeGOcffbZqKio0NZpbW3Fc889h99//x0mkwk77rgjzjjjDDidTvQXpk+fjl9++QXr1q2D3W7HxIkTccopp2Do0KHaOn6/Hy+++CJmzZqFQCCALbfcUoxVUVGRtk5tbS2efvppzJ8/X4zP5MmTcdJJJ8FisaA/MHPmTPFXU1MjHg8fPlxkfmy99dbiMY+RMe+++y6mTZuGgw8+GP/4xz/EMh4riHPPW2+9FbOMfnMPPfSQmOcxilJfX4+XX34Zc+bMgc/nw5AhQ3DhhRdi3LhxWXUu52ysNEA/kKlTp+Kcc87BhAkT8NFHH+Gnn34SP6TCwkIMBP744w/8/fffGDt2LO677752YodOwvR30UUXoby8HK+//jpWr16NBx54QFz0iTvuuEMIpXPPPRehUAiPP/64+IFddtll6C/cfvvt2HXXXcXnos/46quvYs2aNWIc5ImATqizZ88WY5WTk4Nnn30WZrMZt956q3g+HA7j6quvFifiU089VYwZHX/77LOPOPn2B3777TfxmekESqesb775Rojpe+65ByNGjOAxMmDp0qV48MEHxXhMmjRJEzs8VqrY+fnnn3HjjTdqy2gMCgoKxDyPUVSkUAsIOn72339/MT4bNmzA4MGDhejJqnM5iR0mtVx//fXKM888oz0OhULKueeeq0yfPl0ZiBx33HHKzz//rD0Oh8PKOeeco7z33nvasra2NuWkk05Svv/+e/F4zZo14nVLly7V1vnjjz+U448/Xqmrq1P6K01NTeJzz58/XxuXKVOmKD/++KO2ztq1a8U6f//9t3g8e/ZsMS4NDQ3aOjNmzFBOO+00JRAIKP2Vf/zjH8oXX3zBY2SAx+NRLr30UmXu3LnKf/7zH+X5558Xy3msVF5//XXlqquuMnyOxyjKyy+/rNx4441KIrLpXM4xOykmGAxi+fLl2HzzzbVldEdAjxcvXtyn+5YpVFdXC/feFltsoS2juydy88kxomlubq5mKiVoDMkESnes/RW32y2meXl5YkrHEt0J6Y+nYcOGoaysLGasRo4cGWNi32qrrURDPrIS9TforvqHH34QJnVy+/EYteeZZ54RLj79b4zgsYpSWVmJ8847DxdffDEeeeQR4ZYieIxiLapknScrDbmmrrnmGnz++edZeS7nmJ0U09zcLE7G+h8BQY/Xr1/fZ/uVSdCPg4h36dFj+RxNpUlZQr5wEgFynf4GHTcvvPACNtpoI3EiJeizWq1WcbLoaKzijzc5tv1prMg0/n//938ihoJcfOQapdidlStX8hjpICFIsXJ33nlnu+f4eFKh8AKKO6E4HXKvUPzOv//9b9x///08RjpIzHz22Wc45JBDcNRRR2HZsmV4/vnnxfjsueeeWXUuZ7HDMP/f3t2ERPlFcRw/UqAZKb5EQRtdJFgRQbYxKxTBTS9EUUGLqBZRBO5DUFtUEBQtLNoo7sKoRVBRq+jFohaRJvayiBYRBS2sMKno/+d34D6OM9Mf/jC+PNfvB2RmHqdh5vbM9Tz3nnvuHKG8AF0Vnjx5crbfypykP0xnz5710S/lwPX09Fh3d/dsv605RaMTCpg7OjqSfAnkContogUkIfh5/Pgx7ZZ1AaYRmZCHVFtb6xcdCoAU7KQJ01gFpghW01bZEWu+K4H5KrTD2NjYlON6HH6nW42SZdLQshLmYmxHBTpKiOzs7LSqqqrkuD6rpka1yuG/2ir7fAttG1Nb6WpSSZEaVlfnW1NTY7du3aKNMmgKRp9LSaX79u3zH62MvH37tt/XFTdtlUujOAqmNbXF+TSpoqLCR08z6XGY8ktTX06wMw0dsjrjly9fTomO9Vj5BTDP2NdJPjw8nBzT1brmb0Mb6VadjTrvQG2olTgxLeHX51Ggo+XnGkZX22TSuaQh38y20nSoOpvMttLVVmaHMzQ0ZIsWLcrpqGKi75WmtGgjm5ILodWPWqUWfnRl3tTUlNynrXJNTEwkgQ7n0yRNqWenX+jx0qVLU9eXM401DbZu3epD7PrS6D9TV59KpkzbsF8hOo/MuV/lVmieVol+qvtx/fp1X0qsL8yVK1f8KmLDhg3+fHUYSvi7fPmyL+HXlZbqNDQ2NlplZaXFQoHOw4cPPfFPHWW4WlSSn4bTddvS0uI1P9R2eqx2UAcSOhPVAFF7aenr/v37/TXUnm1tbXN2B+L/S7VidD7o3NG5pTbTiIVyeGijSTqHQr5XUFxcbEuWLEmO01bmn7+hocHPJ+XsaCm6RuQVFHI+TVKujpbnq69W36sgRvV0tIRclGSclr6cOjvTWFRQdUD0JdBwuwooaV54vlChrXz5FCq8pXoMoRCVMvt1JaBCVIcPH55STE/DnAoGMgtRqThjTEUF9+zZk/e48gdCcBwKnCnxVB1FvgJnKranFThqd/1xUzurE46lwNmlS5f8alB/mPTHR3kWO3bsSFaB0EZ/19XV5X1QdlHB+dxWqnk2Ojpq375989QD9T+a5gu1Y2ijSep/dbGhi1cFMwqAWltbk9+npS8n2AEAAFEjZwcAAESNYAcAAESNYAcAAESNYAcAAESNYAcAAESNYAcAAESNYAcAAESNYAcAAESN7SIApMK9e/fs4sWLyWOV5Vc5f22DoF2sm5ubfbsEAMhGsAMgdVtsqGy9dk7WdizaI6u/v99u3rzpe4xpKwkAyESwAyBVNIqj3buDnTt3+r5ZZ86c8Z29z58/75uoAkBAzg6A1FuzZo3t2rXLN2e8f/++H3v//r319PTY8ePHfYNG7bisaTBt/hgoSNJI0dOnT3NeUzur63dv3ryZ0c8CoPAIdgBEYfPmzX47NDSU3H7+/Nl3jz948KBt3LjRBgcH7fTp075Ts6xevdqqqqrswYMHOa+nY8uWLbO6uroZ/iQACo1pLABRUNBSWlpqnz598sdtbW22bdu2Kc9ZuXKlXbhwwV69emX19fVWVFRkmzZt8nyf8fFx//fy9etXD5Y0RQYg/RjZARCNkpIS+/Hjh9/PzNv5+fOnBzAKduTdu3fJ77Zs2WK/fv2yJ0+eJMc0AqQE6DBaBCDdGNkBEI2JiQkrLy/3+9+/f7erV6964DI2NjbleRrFCVasWOEJz5q2amlp8WO6r8Bo+fLlM/wJAEwHgh0AUfjy5YsHMcqzEa3Kev36tW3fvt1qamp81OfPnz926tQpv82k0Z2+vj5/DY3yvH371g4dOjRLnwRAoRHsAIhCWIW1bt06H9UZHh721VS7d+9OnvPx48e8/7axsdFr9Tx69MinvBYsWODHAMSBYAdA6mkJ+bVr17zYYFNTk/3+/duPh1VXgRKR8ykrK/P6PZq+UrCjgEnHAMSBYAdAqjx//tw+fPjgU1GqoDwyMuIrp6qrq72CshKT9aPVVjdu3PBE48rKSnvx4oUvRf8bJSOfO3fO7+/du3cGPxGA6UawAyBVBgYG/HbhwoXJ3lgHDhzI2Rurvb3dent77c6dOz7Cs3btWjtx4oQdOXIk7+s2NDTY4sWL/bm6DyAeRf9kj/MCwDykESAFQuvXr7ejR4/O9tsBUEDU2QEAM3v27JnX4tHKLABxYRoLwLymZebaR0sJzrW1tbZq1arZfksACoxgB8C8dvfuXV+FpVo8x44dm+23A2AakLMDAACiRs4OAACIGsEOAACIGsEOAACIGsEOAACIGsEOAACIGsEOAACIGsEOAACIGsEOAACIGsEOAACwmP0LnKIK/xreJXMAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data_3(true, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a model\n",
    "model.save('10VAR-hpg-gru.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: Multivariate-3-LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0' # ƒë·∫£m b·∫£o r·∫±ng c√°c gi√° tr·ªã bƒÉm c·ªßa ƒë·ªëi t∆∞·ª£ng b·∫•t bi·∫øn (dict, set, chu·ªói, tuple...) lu√¥n gi·ªëng nhau gi·ªØa c√°c l·∫ßn ch·∫°y\n",
    "\n",
    "import random as rn\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "rn.seed(3)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from math import sqrt\n",
    "import datetime as dt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H√†m callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=80,  verbose=1, mode='min')  \n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=\"10Var_hpg_lstm.h5\",   # T√™n file l∆∞u m√¥ h√¨nh\n",
    "    monitor=\"val_loss\",         # Theo d√µi val_loss\n",
    "    save_best_only=True,        # Ch·ªâ l∆∞u khi t·ªët h∆°n m√¥ h√¨nh tr∆∞·ªõc ƒë√≥\n",
    "    mode=\"min\",                 # Gi·∫£m min c·ªßa val_loss l√† t·ªët nh·∫•t\n",
    "    verbose=1\n",
    ")\n",
    "callbacks_list = [earlystop, checkpoint] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ƒê·ªçc d·ªØ li·ªáu t·ª´ file csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = r\"hpg_history.csv\"\n",
    "df = pd.read_csv(url, parse_dates= True, index_col= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-11-16</th>\n",
       "      <td>2.29</td>\n",
       "      <td>2.29</td>\n",
       "      <td>2.29</td>\n",
       "      <td>2.29</td>\n",
       "      <td>248510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-11-19</th>\n",
       "      <td>2.17</td>\n",
       "      <td>2.17</td>\n",
       "      <td>2.17</td>\n",
       "      <td>2.17</td>\n",
       "      <td>120480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-11-20</th>\n",
       "      <td>2.08</td>\n",
       "      <td>2.08</td>\n",
       "      <td>2.08</td>\n",
       "      <td>2.08</td>\n",
       "      <td>58710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-11-21</th>\n",
       "      <td>1.99</td>\n",
       "      <td>2.16</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.16</td>\n",
       "      <td>728080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-11-22</th>\n",
       "      <td>2.16</td>\n",
       "      <td>2.16</td>\n",
       "      <td>2.08</td>\n",
       "      <td>2.16</td>\n",
       "      <td>266040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            open  high   low  close  volume\n",
       "time                                       \n",
       "2007-11-16  2.29  2.29  2.29   2.29  248510\n",
       "2007-11-19  2.17  2.17  2.17   2.17  120480\n",
       "2007-11-20  2.08  2.08  2.08   2.08   58710\n",
       "2007-11-21  1.99  2.16  1.99   2.16  728080\n",
       "2007-11-22  2.16  2.16  2.08   2.16  266040"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "open      0\n",
       "high      0\n",
       "low       0\n",
       "close     0\n",
       "volume    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ti·ªÅn x·ª≠ l√≠ d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X√≥a c√°c d√≤ng c√≥ gi√° tr·ªã Volume b·∫±ng 0\n",
    "df.drop(df[df['volume']==0].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "open      0.999667\n",
       "high      0.999857\n",
       "low       0.999838\n",
       "close     1.000000\n",
       "volume    0.799299\n",
       "Name: close, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ma tr·∫≠n t∆∞∆°ng quan (·ªü ƒë√¢y l√† Pearson t∆∞∆°ng quan tuy·∫øn t√≠nh)\n",
    "df.corr()['close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    4.320000e+03\n",
      "mean     7.877410e+06\n",
      "std      1.133795e+07\n",
      "min      5.000000e+01\n",
      "25%      4.646300e+05\n",
      "50%      2.422785e+06\n",
      "75%      1.233620e+07\n",
      "max      9.967998e+07\n",
      "Name: volume, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(df.describe().volume) \n",
    "df['volume'].hist(bins= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B·ªï sung c√°c ch·ªâ b√°o kƒ© thu·∫≠t\n",
    "\n",
    "# T√≠nh CMA10\n",
    "df['CMA10'] = df['close'].rolling(window=10, center=True).mean()\n",
    "# T√≠nh SMA10\n",
    "df['SMA10'] = df['close'].rolling(window=10).mean()\n",
    "# T√≠nh SMA50\n",
    "df['SMA50'] = df['close'].rolling(window=50).mean()\n",
    "# T√≠nh EMA12 v√† EMA26\n",
    "df['EMA12'] = df['close'].ewm(span=12, adjust=False).mean()\n",
    "df['EMA26'] = df['close'].ewm(span=26, adjust=False).mean()\n",
    "# T√≠nh MACD\n",
    "df['MACD'] = df['EMA12'] - df['EMA26']\n",
    "#T√≠nh RSI\n",
    "# T√≠nh gi√° tƒÉng/gi·∫£m\n",
    "delta = df['close'].diff()\n",
    "\n",
    "# T√≠nh gi√° tƒÉng\n",
    "gain = delta.where(delta > 0, 0)\n",
    "\n",
    "# T√≠nh gi√° gi·∫£m\n",
    "loss = -delta.where(delta < 0, 0)\n",
    "\n",
    "# T√≠nh trung b√¨nh ƒë·ªông\n",
    "avg_gain = gain.rolling(window=14).mean()\n",
    "avg_loss = loss.rolling(window=14).mean()\n",
    "\n",
    "# T√≠nh RS v√† RSI\n",
    "rs = avg_gain / avg_loss\n",
    "df['RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "#T√≠nh CCI\n",
    "# T√≠nh gi√° trung b√¨nh\n",
    "typical_price = (df['high'] + df['low'] + df['close']) / 3\n",
    "\n",
    "# T√≠nh SMA c·ªßa gi√° trung b√¨nh\n",
    "sma_typical_price = typical_price.rolling(window=20).mean()\n",
    "\n",
    "# T√≠nh ƒë·ªô l·ªách chu·∫©n\n",
    "mean_deviation = typical_price.rolling(window=20).apply(lambda x: np.mean(np.abs(x - x.mean())))\n",
    "\n",
    "# T√≠nh CCI\n",
    "df['CCI'] = (typical_price - sma_typical_price) / (0.015 * mean_deviation)\n",
    "# T√≠nh %K v√† %D\n",
    "low_min = df['low'].rolling(window=14).min()\n",
    "high_max = df['high'].rolling(window=14).max()\n",
    "\n",
    "df['%K'] = 100 * (df['close'] - low_min) / (high_max - low_min)\n",
    "df['%D'] = df['%K'].rolling(window=3).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            open  high   low  close  volume  CMA10  SMA10  SMA50     EMA12  \\\n",
      "time                                                                         \n",
      "2007-11-16  2.29  2.29  2.29   2.29  248510    NaN    NaN    NaN  2.290000   \n",
      "2007-11-19  2.17  2.17  2.17   2.17  120480    NaN    NaN    NaN  2.271538   \n",
      "2007-11-20  2.08  2.08  2.08   2.08   58710    NaN    NaN    NaN  2.242071   \n",
      "2007-11-21  1.99  2.16  1.99   2.16  728080    NaN    NaN    NaN  2.229445   \n",
      "2007-11-22  2.16  2.16  2.08   2.16  266040    NaN    NaN    NaN  2.218761   \n",
      "\n",
      "               EMA26      MACD  RSI  CCI  %K  %D  \n",
      "time                                              \n",
      "2007-11-16  2.290000  0.000000  NaN  NaN NaN NaN  \n",
      "2007-11-19  2.281111 -0.009573  NaN  NaN NaN NaN  \n",
      "2007-11-20  2.266214 -0.024143  NaN  NaN NaN NaN  \n",
      "2007-11-21  2.258346 -0.028902  NaN  NaN NaN NaN  \n",
      "2007-11-22  2.251061 -0.032300  NaN  NaN NaN NaN  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4320, 15)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.head())\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model / H√†m **fit_model_4()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_4(train, val, timesteps, hl, lr, batch, epochs):\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    X_val = []\n",
    "    Y_val = []\n",
    "\n",
    "    for i in range(timesteps, train.shape[0]):\n",
    "        X_train.append(train[i-timesteps:i])\n",
    "        Y_train.append(train[i][0])\n",
    "    X_train, Y_train = np.array(X_train), np.array(Y_train)\n",
    "\n",
    "    for i in range(timesteps, val.shape[0]):\n",
    "        X_val.append(val[i-timesteps:i])\n",
    "        Y_val.append(val[i][0])\n",
    "    X_val, Y_val = np.array(X_val), np.array(Y_val)\n",
    "\n",
    "    # X√¢y d·ª±ng m√¥ h√¨nh\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(X_train.shape[2], input_shape= (X_train.shape[1], X_train.shape[2]), activation='relu', return_sequences= True))\n",
    "    for i in range(len(hl)-1):\n",
    "        model.add(LSTM(hl[i], activation='relu', return_sequences= True))\n",
    "    model.add(LSTM(hl[-1], activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Bi√™n d·ªãch\n",
    "    model.compile(optimizer= optimizers.Adam(learning_rate= lr), loss= 'mean_squared_error')\n",
    "\n",
    "    # Hu·∫•n luy·ªán m√¥ h√¨nh\n",
    "    history = model.fit(X_train, Y_train, batch_size= batch, epochs= epochs, validation_data= (X_val, Y_val), verbose= 0, shuffle= False, callbacks= callbacks_list)\n",
    "\n",
    "    # ƒê·∫∑t l·∫°i tr·∫°ng th√°i\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, LSTM):\n",
    "            layer.reset_state()\n",
    "\n",
    "    return model, history.history['loss'], history.history['val_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H√†m **Evaluate_model_4()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_4(model, test, timesteps):\n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "\n",
    "    for i in range(timesteps, test.shape[0]):\n",
    "        X_test.append(test[i-timesteps:i])\n",
    "        Y_test.append(test[i][0])\n",
    "    X_test, Y_test = np.array(X_test), np.array(Y_test)\n",
    "\n",
    "    # C√°c ch·ªâ s·ªë\n",
    "    Y_hat = model.predict(X_test)\n",
    "    mse = mean_squared_error(Y_test, Y_hat)\n",
    "    rmse = sqrt(mse)\n",
    "    mape = mean_absolute_percentage_error(Y_test, Y_hat)\n",
    "    r2 = r2_score(Y_test, Y_hat)\n",
    "\n",
    "    return mse, rmse, mape, r2, Y_test, Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Grid Search**: T√¨m ki·∫øm si√™u tham s·ªë t·ªëi ∆∞u "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'timesteps': [30, 40, 50],  # S·ªë gi√° tr·ªã tr∆∞·ªõc ƒë√≥ ƒë·ªÉ d·ª± ƒëo√°n\n",
    "    'hl': [ [40, 35]], # C·∫•u tr√∫c l·ªõp ·∫©n\n",
    "    'lr': [1e-3, 1e-4],  # T·ªëc ƒë·ªô h·ªçc\n",
    "    'batch_size': [32, 64],  # K√≠ch th∆∞·ªõc batch\n",
    "    'num_epochs': [200, 250],  # S·ªë epoch\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import pandas as pd\n",
    "\n",
    "# H√†m Grid Search\n",
    "def grid_search_rnn(train, val, test, param_grid):\n",
    "    results = []  # L∆∞u k·∫øt qu·∫£ c·ªßa t·ª´ng t·ªï h·ª£p tham s·ªë\n",
    "    best_score = float('inf')  # L∆∞u RMSE t·ªët nh·∫•t\n",
    "    best_params = None  # L∆∞u b·ªô tham s·ªë t·ªët nh·∫•t\n",
    "\n",
    "    # T·∫°o t·∫•t c·∫£ c√°c t·ªï h·ª£p tham s·ªë\n",
    "    all_combinations = list(product(*param_grid.values()))\n",
    "    param_names = list(param_grid.keys())\n",
    "\n",
    "    for combination in all_combinations:\n",
    "        # G√°n gi√° tr·ªã tham s·ªë hi·ªán t·∫°i\n",
    "        params = dict(zip(param_names, combination))\n",
    "        timesteps = params['timesteps']\n",
    "        hl = params['hl']\n",
    "        lr = params['lr']\n",
    "        batch_size = params['batch_size']\n",
    "        num_epochs = params['num_epochs']\n",
    "\n",
    "        print(f\"Training with params: {params}\")\n",
    "\n",
    "        # Hu·∫•n luy·ªán m√¥ h√¨nh\n",
    "        model, train_loss, val_loss = fit_model_4(\n",
    "            train, val, timesteps, hl, lr, batch_size, num_epochs\n",
    "        )\n",
    "\n",
    "        # ƒê√°nh gi√° m√¥ h√¨nh\n",
    "        mse, rmse, mape, r2, _, _ = evaluate_model_4(model, test, timesteps)\n",
    "\n",
    "        # L∆∞u k·∫øt qu·∫£\n",
    "        results.append({\n",
    "            'timesteps': timesteps,\n",
    "            'hl': hl,\n",
    "            'lr': lr,\n",
    "            'batch_size': batch_size,\n",
    "            'num_epochs': num_epochs,\n",
    "            'MSE': mse,\n",
    "            'RMSE': rmse,\n",
    "            'MAPE': mape,\n",
    "            'R¬≤': r2\n",
    "        })\n",
    "\n",
    "        # C·∫≠p nh·∫≠t tham s·ªë t·ªët nh·∫•t n·∫øu RMSE c·∫£i thi·ªán\n",
    "        if rmse < best_score:\n",
    "            best_score = rmse\n",
    "            best_params = params\n",
    "\n",
    "    # Tr·∫£ v·ªÅ k·∫øt qu·∫£\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return best_params, best_score, results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot chart (v·∫Ω bi·ªÉu ƒë·ªì)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the predictions\n",
    "def plot_data_4(Y_test, Y_hat):\n",
    "    plt.plot(Y_test, c = 'r')\n",
    "    plt.plot(Y_hat, c = 'y')\n",
    "    plt.xlabel('Day')\n",
    "    plt.ylabel('Price')\n",
    "    plt.title('Stock Prediction Graph using Multivariate-LSTM model')\n",
    "    plt.legend(['Actual', 'Predicted'], loc = 'lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training errors: tr·ª±c quan loss qua c√°c epoch -> th·∫•y qtr h·ªçc m√¥ h√¨nh, xem c√≥ overfitting ko\n",
    "def plot_error(train_loss, val_loss):\n",
    "    plt.plot(train_loss, c = 'r')\n",
    "    plt.plot(val_loss, c = 'b')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.title('Train Loss and Validation Loss Curve')\n",
    "    plt.legend(['train', 'val'], loc = 'upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model building**: X√¢y d·ª±ng m√¥ h√¨nh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 1: Tr√≠ch xu·∫•t v√† tr·ª±c quan h√≥a d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4267, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>CMA10</th>\n",
       "      <th>SMA10</th>\n",
       "      <th>SMA50</th>\n",
       "      <th>EMA12</th>\n",
       "      <th>RSI</th>\n",
       "      <th>CCI</th>\n",
       "      <th>%K</th>\n",
       "      <th>%D</th>\n",
       "      <th>MACD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4267.000000</td>\n",
       "      <td>4267.000000</td>\n",
       "      <td>4267.00000</td>\n",
       "      <td>4267.000000</td>\n",
       "      <td>4267.000000</td>\n",
       "      <td>4267.000000</td>\n",
       "      <td>4267.000000</td>\n",
       "      <td>4267.000000</td>\n",
       "      <td>4267.000000</td>\n",
       "      <td>4267.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.025397</td>\n",
       "      <td>10.022310</td>\n",
       "      <td>9.99800</td>\n",
       "      <td>9.880565</td>\n",
       "      <td>9.992086</td>\n",
       "      <td>52.254060</td>\n",
       "      <td>12.008518</td>\n",
       "      <td>53.007353</td>\n",
       "      <td>53.000609</td>\n",
       "      <td>0.041366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10.126221</td>\n",
       "      <td>10.118056</td>\n",
       "      <td>10.10649</td>\n",
       "      <td>10.019039</td>\n",
       "      <td>10.097556</td>\n",
       "      <td>17.992688</td>\n",
       "      <td>112.438235</td>\n",
       "      <td>30.851704</td>\n",
       "      <td>29.031483</td>\n",
       "      <td>0.373255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>0.71000</td>\n",
       "      <td>0.793800</td>\n",
       "      <td>0.726093</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-486.486486</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.739242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.780000</td>\n",
       "      <td>1.778000</td>\n",
       "      <td>1.77800</td>\n",
       "      <td>1.760400</td>\n",
       "      <td>1.776499</td>\n",
       "      <td>38.724730</td>\n",
       "      <td>-76.041990</td>\n",
       "      <td>25.730277</td>\n",
       "      <td>26.681097</td>\n",
       "      <td>-0.045488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.680000</td>\n",
       "      <td>5.725000</td>\n",
       "      <td>5.70500</td>\n",
       "      <td>5.174200</td>\n",
       "      <td>5.719069</td>\n",
       "      <td>52.577320</td>\n",
       "      <td>21.806854</td>\n",
       "      <td>55.445545</td>\n",
       "      <td>55.238095</td>\n",
       "      <td>0.020672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.415000</td>\n",
       "      <td>15.376000</td>\n",
       "      <td>15.21950</td>\n",
       "      <td>15.065600</td>\n",
       "      <td>15.178271</td>\n",
       "      <td>66.146301</td>\n",
       "      <td>101.156626</td>\n",
       "      <td>80.702355</td>\n",
       "      <td>79.987975</td>\n",
       "      <td>0.119916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>39.900000</td>\n",
       "      <td>39.285000</td>\n",
       "      <td>39.28500</td>\n",
       "      <td>37.489200</td>\n",
       "      <td>39.090578</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>364.285714</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2.247325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             close        CMA10       SMA10        SMA50        EMA12  \\\n",
       "count  4267.000000  4267.000000  4267.00000  4267.000000  4267.000000   \n",
       "mean     10.025397    10.022310     9.99800     9.880565     9.992086   \n",
       "std      10.126221    10.118056    10.10649    10.019039    10.097556   \n",
       "min       0.680000     0.710000     0.71000     0.793800     0.726093   \n",
       "25%       1.780000     1.778000     1.77800     1.760400     1.776499   \n",
       "50%       5.680000     5.725000     5.70500     5.174200     5.719069   \n",
       "75%      15.415000    15.376000    15.21950    15.065600    15.178271   \n",
       "max      39.900000    39.285000    39.28500    37.489200    39.090578   \n",
       "\n",
       "               RSI          CCI           %K           %D         MACD  \n",
       "count  4267.000000  4267.000000  4267.000000  4267.000000  4267.000000  \n",
       "mean     52.254060    12.008518    53.007353    53.000609     0.041366  \n",
       "std      17.992688   112.438235    30.851704    29.031483     0.373255  \n",
       "min       0.000000  -486.486486     0.000000     0.000000    -1.739242  \n",
       "25%      38.724730   -76.041990    25.730277    26.681097    -0.045488  \n",
       "50%      52.577320    21.806854    55.445545    55.238095     0.020672  \n",
       "75%      66.146301   101.156626    80.702355    79.987975     0.119916  \n",
       "max     100.000000   364.285714   100.000000   100.000000     2.247325  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting the series\n",
    "series = df[['close', 'CMA10', 'SMA10', 'SMA50', 'EMA12', 'RSI', 'CCI', '%K', '%D', 'MACD']]\n",
    "# Drop rows with NaN values\n",
    "series = series.dropna()\n",
    "\n",
    "# Display the shape and the tail of the cleaned series\n",
    "print(series.shape)\n",
    "series.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 2: Chia d·ªØ li·ªáu th√†nh c√°c t·∫≠p Train, Validation, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4267, 10)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2987, 10) (640, 10) (640, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n = series.shape[0]\n",
    "val_size =  test_size = int(n * 0.15)\n",
    "train_size = n - val_size - test_size # ƒê·ªÉ tr√°nh sai s·ªë l√†m m·∫•t d·ªØ li·ªáu\n",
    "\n",
    "# Chia t·∫≠p d·ªØ li·ªáu theo th·ª© t·ª± th·ªùi gian\n",
    "train_data = series.iloc[:train_size].values\n",
    "val_data = series.iloc[train_size:train_size + val_size].values\n",
    "test_data = series.iloc[(train_size + val_size):].values\n",
    "# Ki·ªÉm tra k√≠ch th∆∞·ªõc c·ªßa t·ª´ng t·∫≠p\n",
    "print(train_data.shape, val_data.shape, test_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 3: Chu·∫©n h√≥a d·ªØ li·ªáu b·∫±ng MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2987, 10) (640, 10) (640, 10)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "tf.random.set_seed(2)\n",
    "\n",
    "sc = MinMaxScaler()\n",
    "train = sc.fit_transform(train_data)\n",
    "val = sc.transform(val_data)\n",
    "test = sc.transform(test_data)\n",
    "\n",
    "print(train.shape, val.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 4: T√¨m si√™u tham s·ªë t·ªët nh·∫•t b·∫±ng Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with params: {'timesteps': 30, 'hl': [40, 35], 'lr': 0.001, 'batch_size': 32, 'num_epochs': 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 0.34059, saving model to 10Var_hpg_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_loss improved from 0.34059 to 0.10461, saving model to 10Var_hpg_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: val_loss did not improve from 0.10461\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.10461\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.10461\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.10461\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.10461\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.10461\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.10461\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.10461\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.10461\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.10461\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.10461\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.10461\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.10461\n",
      "\n",
      "Epoch 16: val_loss improved from 0.10461 to 0.07478, saving model to 10Var_hpg_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17: val_loss improved from 0.07478 to 0.06678, saving model to 10Var_hpg_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18: val_loss did not improve from 0.06678\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.06678\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.06678\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.06678\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.06678\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.06678\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.06678\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.06678\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.06678\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.06678\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.06678\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.06678\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.06678\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.06678\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.06678\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.06678\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.06678\n",
      "\n",
      "Epoch 35: val_loss improved from 0.06678 to 0.06629, saving model to 10Var_hpg_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36: val_loss did not improve from 0.06629\n",
      "\n",
      "Epoch 37: val_loss improved from 0.06629 to 0.06588, saving model to 10Var_hpg_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38: val_loss did not improve from 0.06588\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.06588\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.06588\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.06588\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.06588\n",
      "\n",
      "Epoch 43: val_loss improved from 0.06588 to 0.06147, saving model to 10Var_hpg_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44: val_loss did not improve from 0.06147\n",
      "\n",
      "Epoch 45: val_loss improved from 0.06147 to 0.04038, saving model to 10Var_hpg_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46: val_loss did not improve from 0.04038\n",
      "\n",
      "Epoch 47: val_loss improved from 0.04038 to 0.03638, saving model to 10Var_hpg_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48: val_loss improved from 0.03638 to 0.03284, saving model to 10Var_hpg_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 49: val_loss improved from 0.03284 to 0.02921, saving model to 10Var_hpg_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 50: val_loss improved from 0.02921 to 0.02606, saving model to 10Var_hpg_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 51: val_loss improved from 0.02606 to 0.02315, saving model to 10Var_hpg_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 52: val_loss improved from 0.02315 to 0.02061, saving model to 10Var_hpg_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 53: val_loss improved from 0.02061 to 0.01845, saving model to 10Var_hpg_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 54: val_loss improved from 0.01845 to 0.01664, saving model to 10Var_hpg_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 55: val_loss improved from 0.01664 to 0.01513, saving model to 10Var_hpg_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 56: val_loss improved from 0.01513 to 0.01392, saving model to 10Var_hpg_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 57: val_loss improved from 0.01392 to 0.01295, saving model to 10Var_hpg_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 58: val_loss improved from 0.01295 to 0.01222, saving model to 10Var_hpg_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 59: val_loss improved from 0.01222 to 0.01173, saving model to 10Var_hpg_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 60: val_loss improved from 0.01173 to 0.01145, saving model to 10Var_hpg_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 61: val_loss improved from 0.01145 to 0.01143, saving model to 10Var_hpg_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 62: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.01143\n",
      "Epoch 140: early stopping\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
      "Training with params: {'timesteps': 30, 'hl': [40, 35], 'lr': 0.001, 'batch_size': 32, 'num_epochs': 250}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.01143\n",
      "Epoch 86: early stopping\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
      "Training with params: {'timesteps': 30, 'hl': [40, 35], 'lr': 0.001, 'batch_size': 64, 'num_epochs': 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.01143\n",
      "\n",
      "Epoch 64: val_loss improved from 0.01143 to 0.01082, saving model to 10Var_hpg_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 65: val_loss improved from 0.01082 to 0.00574, saving model to 10Var_hpg_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 66: val_loss improved from 0.00574 to 0.00485, saving model to 10Var_hpg_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 67: val_loss did not improve from 0.00485\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00485\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00485\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00485\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00485\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00485\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00485\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00485\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00485\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00485\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00485\n",
      "\n",
      "Epoch 78: val_loss improved from 0.00485 to 0.00450, saving model to 10Var_hpg_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 79: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.00450\n",
      "Epoch 158: early stopping\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step\n",
      "Training with params: {'timesteps': 30, 'hl': [40, 35], 'lr': 0.001, 'batch_size': 64, 'num_epochs': 250}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00450\n",
      "Epoch 81: early stopping\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step\n",
      "Training with params: {'timesteps': 30, 'hl': [40, 35], 'lr': 0.0001, 'batch_size': 32, 'num_epochs': 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00450\n",
      "Epoch 81: early stopping\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step\n",
      "Training with params: {'timesteps': 30, 'hl': [40, 35], 'lr': 0.0001, 'batch_size': 32, 'num_epochs': 250}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.00450\n",
      "Epoch 120: early stopping\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
      "Training with params: {'timesteps': 30, 'hl': [40, 35], 'lr': 0.0001, 'batch_size': 64, 'num_epochs': 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.00450\n",
      "Epoch 109: early stopping\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
      "Training with params: {'timesteps': 30, 'hl': [40, 35], 'lr': 0.0001, 'batch_size': 64, 'num_epochs': 250}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.00450\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
      "Training with params: {'timesteps': 40, 'hl': [40, 35], 'lr': 0.001, 'batch_size': 32, 'num_epochs': 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.00450\n",
      "Epoch 156: early stopping\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
      "Training with params: {'timesteps': 40, 'hl': [40, 35], 'lr': 0.001, 'batch_size': 32, 'num_epochs': 250}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.00450\n",
      "Epoch 130: early stopping\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
      "Training with params: {'timesteps': 40, 'hl': [40, 35], 'lr': 0.001, 'batch_size': 64, 'num_epochs': 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.00450\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 95ms/step\n",
      "Training with params: {'timesteps': 40, 'hl': [40, 35], 'lr': 0.001, 'batch_size': 64, 'num_epochs': 250}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00450\n",
      "Epoch 85: early stopping\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 83ms/step\n",
      "Training with params: {'timesteps': 40, 'hl': [40, 35], 'lr': 0.0001, 'batch_size': 32, 'num_epochs': 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.00450\n",
      "Epoch 159: early stopping\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step\n",
      "Training with params: {'timesteps': 40, 'hl': [40, 35], 'lr': 0.0001, 'batch_size': 32, 'num_epochs': 250}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00450\n",
      "Epoch 82: early stopping\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step\n",
      "Training with params: {'timesteps': 40, 'hl': [40, 35], 'lr': 0.0001, 'batch_size': 64, 'num_epochs': 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00450\n",
      "Epoch 86: early stopping\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step\n",
      "Training with params: {'timesteps': 40, 'hl': [40, 35], 'lr': 0.0001, 'batch_size': 64, 'num_epochs': 250}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.00450\n",
      "Epoch 88: early stopping\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step\n",
      "Training with params: {'timesteps': 50, 'hl': [40, 35], 'lr': 0.001, 'batch_size': 32, 'num_epochs': 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00450\n",
      "Epoch 85: early stopping\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step\n",
      "Training with params: {'timesteps': 50, 'hl': [40, 35], 'lr': 0.001, 'batch_size': 32, 'num_epochs': 250}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00450\n",
      "Epoch 85: early stopping\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 390ms/step\n",
      "Training with params: {'timesteps': 50, 'hl': [40, 35], 'lr': 0.001, 'batch_size': 64, 'num_epochs': 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00450\n",
      "Epoch 83: early stopping\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step\n",
      "Training with params: {'timesteps': 50, 'hl': [40, 35], 'lr': 0.001, 'batch_size': 64, 'num_epochs': 250}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.00450\n",
      "Epoch 155: early stopping\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step\n",
      "Training with params: {'timesteps': 50, 'hl': [40, 35], 'lr': 0.0001, 'batch_size': 32, 'num_epochs': 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00450\n",
      "Epoch 84: early stopping\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step\n",
      "Training with params: {'timesteps': 50, 'hl': [40, 35], 'lr': 0.0001, 'batch_size': 32, 'num_epochs': 250}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00450\n",
      "Epoch 81: early stopping\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step\n",
      "Training with params: {'timesteps': 50, 'hl': [40, 35], 'lr': 0.0001, 'batch_size': 64, 'num_epochs': 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.00450\n",
      "Epoch 200: early stopping\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 234ms/step\n",
      "Training with params: {'timesteps': 50, 'hl': [40, 35], 'lr': 0.0001, 'batch_size': 64, 'num_epochs': 250}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.00450\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.00450\n",
      "Epoch 165: early stopping\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
      "    timesteps        hl      lr  batch_size  num_epochs            MSE  \\\n",
      "0          30  [40, 35]  0.0010          32         200       0.018166   \n",
      "1          30  [40, 35]  0.0010          32         250       0.502617   \n",
      "2          30  [40, 35]  0.0010          64         200       0.082128   \n",
      "3          30  [40, 35]  0.0010          64         250      19.323725   \n",
      "4          30  [40, 35]  0.0001          32         200      92.451043   \n",
      "5          30  [40, 35]  0.0001          32         250       1.263663   \n",
      "6          30  [40, 35]  0.0001          64         200       0.038086   \n",
      "7          30  [40, 35]  0.0001          64         250       0.040460   \n",
      "8          40  [40, 35]  0.0010          32         200       0.168978   \n",
      "9          40  [40, 35]  0.0010          32         250       0.017970   \n",
      "10         40  [40, 35]  0.0010          64         200       0.082201   \n",
      "11         40  [40, 35]  0.0010          64         250       0.299000   \n",
      "12         40  [40, 35]  0.0001          32         200       1.790999   \n",
      "13         40  [40, 35]  0.0001          32         250     435.573039   \n",
      "14         40  [40, 35]  0.0001          64         200       2.655032   \n",
      "15         40  [40, 35]  0.0001          64         250       4.052959   \n",
      "16         50  [40, 35]  0.0010          32         200       1.981392   \n",
      "17         50  [40, 35]  0.0010          32         250    1052.803603   \n",
      "18         50  [40, 35]  0.0010          64         200  250667.292907   \n",
      "19         50  [40, 35]  0.0010          64         250       0.033993   \n",
      "20         50  [40, 35]  0.0001          32         200       0.385948   \n",
      "21         50  [40, 35]  0.0001          32         250       0.117378   \n",
      "22         50  [40, 35]  0.0001          64         200       0.163681   \n",
      "23         50  [40, 35]  0.0001          64         250       0.309909   \n",
      "\n",
      "          RMSE       MAPE            R¬≤  \n",
      "0     0.134782   0.063126  7.814172e-01  \n",
      "1     0.708955   0.392959 -5.047723e+00  \n",
      "2     0.286581   0.163733  1.179283e-02  \n",
      "3     4.395876   2.413806 -2.315120e+02  \n",
      "4     9.615147   4.612546 -1.111414e+03  \n",
      "5     1.124128   0.533471 -1.420498e+01  \n",
      "6     0.195155   0.101154  5.417374e-01  \n",
      "7     0.201147   0.114375  5.131663e-01  \n",
      "8     0.411070   0.216199 -1.092007e+00  \n",
      "9     0.134053   0.083376  7.775234e-01  \n",
      "10    0.286707   0.153126 -1.767662e-02  \n",
      "11    0.546809   0.296422 -2.701726e+00  \n",
      "12    1.338282   0.684410 -2.117316e+01  \n",
      "13   20.870387   8.975707 -5.391540e+03  \n",
      "14    1.629427   0.688026 -3.187018e+01  \n",
      "15    2.013196   0.904592 -4.917699e+01  \n",
      "16    1.407619   0.673821 -2.484881e+01  \n",
      "17   32.446935   2.561094 -1.373364e+04  \n",
      "18  500.666848  56.852633 -3.270149e+06  \n",
      "19    0.184373   0.095950  5.565289e-01  \n",
      "20    0.621247   0.297732 -4.034988e+00  \n",
      "21    0.342605   0.160287 -5.312868e-01  \n",
      "22    0.404575   0.157158 -1.135344e+00  \n",
      "23    0.556695   0.270997 -3.043007e+00  \n",
      "Best parameters: {'timesteps': 40, 'hl': [40, 35], 'lr': 0.001, 'batch_size': 32, 'num_epochs': 250}\n",
      "Best RMSE score: 0.13405281686277914\n"
     ]
    }
   ],
   "source": [
    "best_params, best_score, results_df = grid_search_rnn(train, val, test, param_grid)\n",
    "\n",
    "print(results_df)\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best RMSE score:\", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 5: Hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi b·ªô tham s·ªë t·ªëi ∆∞u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 0.39539, saving model to 10Var_hpg_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_loss did not improve from 0.39539\n",
      "\n",
      "Epoch 3: val_loss improved from 0.39539 to 0.36036, saving model to 10Var_hpg_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: val_loss improved from 0.36036 to 0.29609, saving model to 10Var_hpg_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5: val_loss improved from 0.29609 to 0.15478, saving model to 10Var_hpg_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6: val_loss improved from 0.15478 to 0.09551, saving model to 10Var_hpg_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7: val_loss did not improve from 0.09551\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.09551\n",
      "\n",
      "Epoch 9: val_loss improved from 0.09551 to 0.07028, saving model to 10Var_hpg_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10: val_loss improved from 0.07028 to 0.05181, saving model to 10Var_hpg_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11: val_loss did not improve from 0.05181\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.05181\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.05181\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.05181\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.05181\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.05181\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.05181\n",
      "\n",
      "Epoch 18: val_loss improved from 0.05181 to 0.04220, saving model to 10Var_hpg_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19: val_loss improved from 0.04220 to 0.02960, saving model to 10Var_hpg_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.02960\n",
      "\n",
      "Epoch 81: val_loss improved from 0.02960 to 0.01642, saving model to 10Var_hpg_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 82: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.01642\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.01642\n",
      "Epoch 161: early stopping\n"
     ]
    }
   ],
   "source": [
    "timesteps = 40\n",
    "hl = [40, 35]\n",
    "lr = 1e-3\n",
    "batch_size = 32\n",
    "num_epochs = 250\n",
    "\n",
    "model, train_error, val_error = fit_model_4(train, val, timesteps, hl, lr, batch_size, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 6: ƒê√°nh gi√° m√¥ h√¨nh v√† tr·ª±c quan h√≥a k·∫øt qu·∫£"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1. V·∫Ω bi·ªÉu ƒë·ªì train_loss v√† val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_error(train_error, val_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2. ƒê√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step\n"
     ]
    }
   ],
   "source": [
    "mse, rmse, mape, r2, true, predicted = evaluate_model_4(model, test, timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.3025234036292122\n",
      "RMSE = 0.5500212756150549\n",
      "MAPE = 0.2892431515809715\n",
      "R-Squared Score = -2.7453408595001494\n"
     ]
    }
   ],
   "source": [
    "print('MSE = {}'.format(mse))\n",
    "print('RMSE = {}'.format(rmse))\n",
    "print('MAPE = {}'.format(mape))\n",
    "print('R-Squared Score = {}'.format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3. V·∫Ω ƒë·ªì th·ªã d·ª± ƒëo√°n vs th·ª±c t·∫ø"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data_4(true, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a model\n",
    "model.save('10VAR-hpg-lstm.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
