{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Multivariate-3-RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0' # ƒë·∫£m b·∫£o r·∫±ng c√°c gi√° tr·ªã bƒÉm c·ªßa ƒë·ªëi t∆∞·ª£ng b·∫•t bi·∫øn (dict, set, chu·ªói, tuple...) lu√¥n gi·ªëng nhau gi·ªØa c√°c l·∫ßn ch·∫°y\n",
    "\n",
    "import random as rn\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "rn.seed(3)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, Dropout\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from math import sqrt\n",
    "import datetime as dt\n",
    "import time\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H√†m callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=80,  verbose=1, mode='min')  \n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=\"10Var-vn30-rnn.h5\",   # T√™n file l∆∞u m√¥ h√¨nh\n",
    "    monitor=\"val_loss\",         # Theo d√µi val_loss\n",
    "    save_best_only=True,        # Ch·ªâ l∆∞u khi t·ªët h∆°n m√¥ h√¨nh tr∆∞·ªõc ƒë√≥\n",
    "    mode=\"min\",                 # Gi·∫£m min c·ªßa val_loss l√† t·ªët nh·∫•t\n",
    "    verbose=1\n",
    ")\n",
    "callbacks_list = [earlystop, checkpoint] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ƒê·ªçc d·ªØ li·ªáu t·ª´ file csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = r\"VN30 Index.csv\"\n",
    "df = pd.read_csv(url, parse_dates= True, index_col= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                open     high      low    close       volume\n",
      "time                                                       \n",
      "2009-01-05   311.23   311.23   311.23   311.23          NaN\n",
      "2009-01-06   314.21   314.21   314.21   314.21          NaN\n",
      "2009-01-07   320.53   320.53   320.53   320.53          NaN\n",
      "2009-01-08   314.14   314.14   314.14   314.14          NaN\n",
      "2009-01-09   312.90   312.90   312.90   312.90          NaN\n",
      "...             ...      ...      ...      ...          ...\n",
      "2025-03-14  1387.08  1394.39  1385.20  1387.03  374490000.0\n",
      "2025-03-17  1392.75  1397.28  1389.42  1394.90  352180000.0\n",
      "2025-03-18  1398.44  1400.38  1388.64  1388.64  291590000.0\n",
      "2025-03-19  1385.92  1386.57  1374.83  1377.63  363360000.0\n",
      "2025-03-20  1382.23  1388.37  1371.98  1378.95  359980000.0\n",
      "\n",
      "[4044 rows x 5 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open        0\n",
      "high        0\n",
      "low         0\n",
      "close       0\n",
      "volume    859\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ti·ªÅn x·ª≠ l√≠ d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X√≥a c√°c d√≤ng c√≥ gi√° tr·ªã Volume b·∫±ng 0\n",
    "df.drop(df[df['volume']==0].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "open      0.999562\n",
       "high      0.999783\n",
       "low       0.999789\n",
       "close     1.000000\n",
       "volume    0.169492\n",
       "Name: close, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ma tr·∫≠n t∆∞∆°ng quan (·ªü ƒë√¢y l√† Pearson t∆∞∆°ng quan tuy·∫øn t√≠nh)\n",
    "df.corr()['close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    3.185000e+03\n",
      "mean     3.279171e+06\n",
      "std      2.776577e+07\n",
      "min      4.540000e+03\n",
      "25%      3.809000e+04\n",
      "50%      6.036000e+04\n",
      "75%      1.560100e+05\n",
      "max      3.744900e+08\n",
      "Name: volume, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#  T√≠nh to√°n c√°c th·ªëng k√™ m√¥ t·∫£ nh∆∞ trung b√¨nh (mean), ƒë·ªô l·ªách chu·∫©n (std), min, max, ph·∫ßn trƒÉm ph√¢n v·ªã (25%, 50%, 75%).\n",
    "print(df.describe().volume) # Gi√∫p ki·ªÉm tra ph√¢n b·ªë c·ªßa Volume, ph√°t hi·ªán c√°c gi√° tr·ªã b·∫•t th∆∞·ªùng (outliers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# V·∫Ω bi·ªÉu ƒë·ªì t·∫ßn su·∫•t c·ªßa Volume\n",
    "df['volume'].hist(bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ph√¢n t√≠ch ph√¢n b·ªë d·ªØ li·ªáu Volume b·∫±ng Histogram**  \n",
    "\n",
    "üîπ `.hist(bins=10)`  \n",
    "- V·∫Ω **bi·ªÉu ƒë·ªì histogram** c·ªßa c·ªôt `Volume` v·ªõi **10 bins (nh√≥m d·ªØ li·ªáu)**.  \n",
    "- Gi√∫p tr·ª±c quan h√≥a **ph√¢n b·ªë d·ªØ li·ªáu**, ph√°t hi·ªán s·ª± **l·ªách** (skewness) v√† **gi√° tr·ªã ngo·∫°i lai** (outliers).  \n",
    "\n",
    "**√ù nghƒ©a c·ªßa bi·ªÉu ƒë·ªì histogram**  \n",
    "\n",
    "- **N·∫øu ph√¢n b·ªë l·ªách ph·∫£i (right-skewed)** ‚Üí D·ªØ li·ªáu c√≥ nhi·ªÅu gi√° tr·ªã nh·ªè, m·ªôt s·ªë gi√° tr·ªã r·∫•t l·ªõn.  \n",
    "- **N·∫øu ph√¢n b·ªë l·ªách tr√°i (left-skewed)** ‚Üí D·ªØ li·ªáu c√≥ nhi·ªÅu gi√° tr·ªã l·ªõn, m·ªôt s·ªë gi√° tr·ªã r·∫•t nh·ªè.  \n",
    "- **N·∫øu c√≥ outliers (ƒëi·ªÉm n·∫±m xa t·∫≠p trung ch√≠nh)** ‚Üí C√≥ th·ªÉ c·∫ßn x·ª≠ l√Ω nh∆∞ **lo·∫°i b·ªè** ho·∫∑c **chu·∫©n h√≥a d·ªØ li·ªáu**.  \n",
    "\n",
    "**C√°ch x·ª≠ l√Ω d·ªØ li·ªáu l·ªách/skewed**  \n",
    "\n",
    "‚úÖ **Log Transformation** ‚Üí D√πng `np.log1p(Volume)` ƒë·ªÉ gi·∫£m ƒë·ªô l·ªách.  \n",
    "‚úÖ **Scaling** ‚Üí D√πng `MinMaxScaler()` ho·∫∑c `StandardScaler()` ƒë·ªÉ chu·∫©n h√≥a.  \n",
    "‚úÖ **X·ª≠ l√Ω outliers** ‚Üí Lo·∫°i b·ªè ho·∫∑c thay th·∫ø b·∫±ng **gi√° tr·ªã trung b√¨nh/median**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B·ªï sung c√°c ch·ªâ b√°o kƒ© thu·∫≠t\n",
    "\n",
    "# T√≠nh CMA10\n",
    "df['CMA10'] = df['close'].rolling(window=10, center=True).mean()\n",
    "# T√≠nh SMA10\n",
    "df['SMA10'] = df['close'].rolling(window=10).mean()\n",
    "# T√≠nh SMA50\n",
    "df['SMA50'] = df['close'].rolling(window=50).mean()\n",
    "# T√≠nh EMA12 v√† EMA26\n",
    "df['EMA12'] = df['close'].ewm(span=12, adjust=False).mean()\n",
    "df['EMA26'] = df['close'].ewm(span=26, adjust=False).mean()\n",
    "# T√≠nh MACD\n",
    "df['MACD'] = df['EMA12'] - df['EMA26']\n",
    "#T√≠nh RSI\n",
    "# T√≠nh gi√° tƒÉng/gi·∫£m\n",
    "delta = df['close'].diff()\n",
    "\n",
    "# T√≠nh gi√° tƒÉng\n",
    "gain = delta.where(delta > 0, 0)\n",
    "\n",
    "# T√≠nh gi√° gi·∫£m\n",
    "loss = -delta.where(delta < 0, 0)\n",
    "\n",
    "# T√≠nh trung b√¨nh ƒë·ªông\n",
    "avg_gain = gain.rolling(window=14).mean()\n",
    "avg_loss = loss.rolling(window=14).mean()\n",
    "\n",
    "# T√≠nh RS v√† RSI\n",
    "rs = avg_gain / avg_loss\n",
    "df['RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "#T√≠nh CCI\n",
    "# T√≠nh gi√° trung b√¨nh\n",
    "typical_price = (df['high'] + df['low'] + df['close']) / 3\n",
    "\n",
    "# T√≠nh SMA c·ªßa gi√° trung b√¨nh\n",
    "sma_typical_price = typical_price.rolling(window=20).mean()\n",
    "\n",
    "# T√≠nh ƒë·ªô l·ªách chu·∫©n\n",
    "mean_deviation = typical_price.rolling(window=20).apply(lambda x: np.mean(np.abs(x - x.mean())))\n",
    "\n",
    "# T√≠nh CCI\n",
    "df['CCI'] = (typical_price - sma_typical_price) / (0.015 * mean_deviation)\n",
    "# T√≠nh %K v√† %D\n",
    "low_min = df['low'].rolling(window=14).min()\n",
    "high_max = df['high'].rolling(window=14).max()\n",
    "\n",
    "df['%K'] = 100 * (df['close'] - low_min) / (high_max - low_min)\n",
    "df['%D'] = df['%K'].rolling(window=3).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              open    high     low   close  volume  CMA10  SMA10  SMA50  \\\n",
      "time                                                                      \n",
      "2009-01-05  311.23  311.23  311.23  311.23     NaN    NaN    NaN    NaN   \n",
      "2009-01-06  314.21  314.21  314.21  314.21     NaN    NaN    NaN    NaN   \n",
      "2009-01-07  320.53  320.53  320.53  320.53     NaN    NaN    NaN    NaN   \n",
      "2009-01-08  314.14  314.14  314.14  314.14     NaN    NaN    NaN    NaN   \n",
      "2009-01-09  312.90  312.90  312.90  312.90     NaN    NaN    NaN    NaN   \n",
      "\n",
      "                 EMA12       EMA26      MACD  RSI  CCI  %K  %D  \n",
      "time                                                            \n",
      "2009-01-05  311.230000  311.230000  0.000000  NaN  NaN NaN NaN  \n",
      "2009-01-06  311.688462  311.450741  0.237721  NaN  NaN NaN NaN  \n",
      "2009-01-07  313.048698  312.123278  0.925420  NaN  NaN NaN NaN  \n",
      "2009-01-08  313.216591  312.272665  0.943926  NaN  NaN NaN NaN  \n",
      "2009-01-09  313.167885  312.319134  0.848750  NaN  NaN NaN NaN  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4044, 15)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.head())\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model / H√†m **fit_model_2()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_2(train, val, timesteps, hl, lr, batch, epochs):\n",
    "    np.random.seed(1)\n",
    "    tf.random.set_seed(2)\n",
    "    rn.seed(3)\n",
    "\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    X_val = []\n",
    "    Y_val = []\n",
    "\n",
    "    # Loop for training data\n",
    "    for i in range(timesteps, train.shape[0]):\n",
    "        X_train.append(train[i-timesteps:i])\n",
    "        Y_train.append(train[i][0])\n",
    "    X_train, Y_train = np.array(X_train), np.array(Y_train)\n",
    "\n",
    "    # Loop for val data\n",
    "    for i in range(timesteps, val.shape[0]):\n",
    "        X_val.append(val[i-timesteps:i])\n",
    "        Y_val.append(val[i][0])\n",
    "    X_val, Y_val = np.array(X_val), np.array(Y_val)\n",
    "\n",
    "    # Adding Layers to the model\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(X_train.shape[2], input_shape = (X_train.shape[1], X_train.shape[2]), activation = 'relu', return_sequences = True))\n",
    "    for i in range(len(hl)-1):\n",
    "        model.add(SimpleRNN(hl[i], activation = 'relu', return_sequences = True))\n",
    "    model.add(SimpleRNN(hl[-1], activation = 'relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer = optimizers.Adam(learning_rate= lr), loss = 'mean_squared_error')\n",
    "\n",
    "    # Training the data\n",
    "    history = model.fit(X_train, Y_train, epochs = epochs, batch_size = batch, validation_data = (X_val, Y_val), verbose = 0, shuffle = False, callbacks=callbacks_list)\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, SimpleRNN):\n",
    "            layer.reset_states() #ƒê·∫£m b·∫£o m·ªói l·∫ßn hu·∫•n luy·ªán kh√¥ng b·ªã ·∫£nh h∆∞·ªüng b·ªüi tr·∫°ng th√°i c≈© c·ªßa LSTM.\n",
    "    return model, history.history['loss'], history.history['val_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  B∆∞·ªõc 1: ƒê·∫∑t Seed ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh t√°i l·∫≠p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gi√∫p ƒë·∫£m b·∫£o m·ªói l·∫ßn ch·∫°y ch∆∞∆°ng tr√¨nh, c√°c gi√° tr·ªã ng·∫´u nhi√™n ƒë∆∞·ª£c t·∫°o ra gi·ªëng nhau, tr√°nh k·∫øt qu·∫£ hu·∫•n luy·ªán thay ƒë·ªïi gi·ªØa c√°c l·∫ßn ch·∫°y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnp.random.seed(1)\\ntf.random.set_seed(2)\\nrn.seed(3)\\n'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(2)\n",
    "rn.seed(3)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 2: Chu·∫©n b·ªã d·ªØ li·ªáu hu·∫•n luy·ªán (train) v√† ki·ªÉm ƒë·ªãnh (val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nX_train = []\\nY_train = []\\nX_val = []\\nY_val = []\\n\\nfor i in range(timesteps, train.shape[0]):\\n    X_train.append(train[i-timesteps:i])\\n    Y_train.append(train[i][0])\\nX_train, Y_train = np.array(X_train, Y_train)\\n\\nfor i in range(timesteps, val.shape[0]):\\n    X_val.append(val[i-timesteps:i])\\n    Y_val.append(val[i][0])\\nX_val, Y_val = np.array(X_val, Y_val)  \\n'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "X_train = []\n",
    "Y_train = []\n",
    "X_val = []\n",
    "Y_val = []\n",
    "\n",
    "for i in range(timesteps, train.shape[0]):\n",
    "    X_train.append(train[i-timesteps:i])\n",
    "    Y_train.append(train[i][0])\n",
    "X_train, Y_train = np.array(X_train, Y_train)\n",
    "\n",
    "for i in range(timesteps, val.shape[0]):\n",
    "    X_val.append(val[i-timesteps:i])\n",
    "    Y_val.append(val[i][0])\n",
    "X_val, Y_val = np.array(X_val, Y_val)  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 3: X√¢y d·ª±ng m√¥ h√¨nh RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# X√¢y d·ª±ng m√¥ h√¨nh RNN\\nmodel = Sequential()\\nmodel.add(SimpleRNN(X_train.shape[2], input_shape= (X_train.shape[1], X_train.shape[2]), activation='relu', return_sequences= True))\\nfor i in range(len(hl)-1):\\n    model.add(SimpleRNN(hl[i], activation='relu', return_sequences= True))\\nmodel.add(SimpleRNN(hl[-1], activation='relu'))\\nmodel.add(Dense(1))\\n\""
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# X√¢y d·ª±ng m√¥ h√¨nh RNN\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(X_train.shape[2], input_shape= (X_train.shape[1], X_train.shape[2]), activation='relu', return_sequences= True))\n",
    "for i in range(len(hl)-1):\n",
    "    model.add(SimpleRNN(hl[i], activation='relu', return_sequences= True))\n",
    "model.add(SimpleRNN(hl[-1], activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Th√™m m·ªôt l·ªõp **SimpleRNN ƒë·∫ßu ti√™n**:  \n",
    "-   `X_train.shape[2]`: S·ªë ƒë·∫∑c tr∆∞ng (features).\n",
    "-   `input_shape = (X_train.shape[1], X_train.shape[2])`: ƒê·ªãnh d·∫°ng ƒë·∫ßu v√†o (timesteps, s·ªë ƒë·∫∑c tr∆∞ng).\n",
    "-   `activation = 'relu'`: H√†m k√≠ch ho·∫°t gi√∫p m√¥ h√¨nh h·ªçc phi tuy·∫øn t√≠nh.\n",
    "-   `return_sequences = True`: Gi·ªØ l·∫°i to√†n b·ªô chu·ªói ƒë·∫ßu ra ƒë·ªÉ s·ª≠ d·ª•ng trong c√°c l·ªõp ti·∫øp theo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 4: Bi√™n d·ªãch m√¥ h√¨nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Bi√™n d·ªãch\\nmodel.compile(optimizer= optimizers.Adam(learning_rate= lr), loss= 'mean_squared_error')\\n\""
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Bi√™n d·ªãch\n",
    "model.compile(optimizer= optimizers.Adam(learning_rate= lr), loss= 'mean_squared_error')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 5: Hu·∫•n luy·ªán m√¥ h√¨nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nhistory = model.fit(X_train, Y_train, epochs = epochs, batch_size = batch, validation_data = (X_val, Y_val), verbose = 0, shuffle = False, callbacks=callbacks_list)\\n'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "history = model.fit(X_train, Y_train, epochs = epochs, batch_size = batch, validation_data = (X_val, Y_val), verbose = 0, shuffle = False, callbacks=callbacks_list)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C·∫•u h√¨nh hu·∫•n luy·ªán m√¥ h√¨nh**\n",
    "\n",
    "üîπ Tham s·ªë trong qu√° tr√¨nh hu·∫•n luy·ªán  \n",
    "\n",
    "‚úÖ `epochs = epochs` ‚Üí S·ªë v√≤ng hu·∫•n luy·ªán  \n",
    "‚úÖ `batch_size = batch` ‚Üí K√≠ch th∆∞·ªõc batch  \n",
    "‚úÖ `validation_data = (X_val, Y_val)` ‚Üí D·ªØ li·ªáu ki·ªÉm ƒë·ªãnh ƒë·ªÉ theo d√µi hi·ªáu su·∫•t sau m·ªói epoch  \n",
    "‚úÖ `verbose = 0` ‚Üí Kh√¥ng hi·ªÉn th·ªã log hu·∫•n luy·ªán (c√≥ th·ªÉ ƒë·∫∑t `verbose = 1` ƒë·ªÉ xem ti·∫øn tr√¨nh)  \n",
    "‚úÖ `shuffle = False` ‚Üí Kh√¥ng x√°o tr·ªôn d·ªØ li·ªáu (do chu·ªói th·ªùi gian c√≥ t√≠nh th·ª© t·ª±)  \n",
    "‚úÖ `callbacks = callbacks_list` ‚Üí Danh s√°ch callback h·ªó tr·ª£ hu·∫•n luy·ªán  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 6: ƒê·∫£m b·∫£o tr·∫°ng th√°i kh√¥ng b·ªã ·∫£nh h∆∞·ªüng khi hu·∫•n luy·ªán nhi·ªÅu l·∫ßn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor layer in model.layers:\\n    if isinstance(layer, SimpleRNN):\\n        layer.reset_states()\\n'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, SimpleRNN):\n",
    "        layer.reset_states()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 7: Tr·∫£ v·ªÅ k·∫øt qu·∫£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nreturn model, history.history['train_loss'], history.history['val_loss']\\n\""
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "return model, history.history['train_loss'], history.history['val_loss']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Grid Search**: T√¨m ki·∫øm si√™u tham s·ªë t·ªëi ∆∞u "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'timesteps': [30, 40, 50],\n",
    "    'hl': [[40, 35]],\n",
    "    'lr': [1e-3, 1e-4],\n",
    "    'batch_size': [32, 64],\n",
    "    'num_epochs': [200, 250]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product # T√≠ch ƒë·ªÅ-c√°c\n",
    "import pandas as pd\n",
    "\n",
    "# H√†m Grid Search\n",
    "def grid_search_rnn(train, val, test, param_grid):\n",
    "# Kh·ªüi t·∫°o danh s√°ch l∆∞u k·∫øt qu·∫£\n",
    "    results = []\n",
    "    best_score = float('inf') # Ban ƒë·∫ßu ƒë∆∞·ª£c ƒë·∫∑t l√† v√¥ c√πng l·ªõn\n",
    "    best_params = None # L∆∞u b·ªô si√™u tham s·ªë c√≥ hi·ªáu su·∫•t t·ªët nh·∫•t\n",
    "# T·∫°o t·∫•t c·∫£ c√°c t·ªï h·ª£p tham s·ªë\n",
    "    all_combinations = list(product(*param_grid.values()))\n",
    "    param_names = list(param_grid.keys())\n",
    "# L·∫∑p qua t·ª´ng t·ªï h·ª£p tham s·ªë\n",
    "    for combination in all_combinations:\n",
    "        params = dict(zip(param_names, combination))\n",
    "        timesteps = params['timesteps']\n",
    "        hl = params['hl']\n",
    "        lr = params['lr']\n",
    "        batch_size = params['batch_size']\n",
    "        num_epochs = params['num_epochs']\n",
    "\n",
    "    print(f'Training with param: {params}')\n",
    "# Hu·∫•n luy·ªán v·ªõi fit.model()\n",
    "    model, train_loss, val_loss = fit_model_2(train, val, timesteps, hl, lr,  batch_size, num_epochs)\n",
    "# ƒê√°nh gi√° m√¥ h√¨nh v·ªõi evaluate_model()\n",
    "    mse, rmse, mape, r2, _, _ = evaluate_model_2(model, test, timesteps)\n",
    "# L∆∞u k·∫øt qu·∫£\n",
    "    results.append({\n",
    "        'timesteps': timesteps,\n",
    "        'hl': hl,\n",
    "        'lr': lr,\n",
    "        'batch_size': batch_size,\n",
    "        'num_epochs': num_epochs,\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAPE': mape,\n",
    "        'R¬≤': r2\n",
    "    })\n",
    "# C·∫≠p nh·∫≠t b·ªô si√™u tham s·ªë t·ªët nh·∫•t n·∫øu RMSE c·∫£i thi·ªán\n",
    "    if rmse < best_score:\n",
    "        best_score = rmse\n",
    "        best_params = params\n",
    "\n",
    "# Tr·∫£ v·ªÅ k·∫øt qu·∫£\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return best_params, best_score, results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H√†m **Evaluate_model_2()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# ƒê·ªãnh nghƒ©a h√†m mean_absolute_percentage_error() (MAPE)\\ndef mean_absolute_percentage_error(y_true, y_pred):\\n    y_true, y_pred = np.array(y_true), np.array(y_pred)\\n    return np.mean(np.abs((y_true - y_pred) / y_true))*100\"\\n    '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# ƒê·ªãnh nghƒ©a h√†m mean_absolute_percentage_error() (MAPE)\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true))*100\"\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_2(model, test, timesteps):\n",
    "    np.random.seed(1)\n",
    "    tf.random.set_seed(2)\n",
    "    rn.seed(3)\n",
    "\n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "\n",
    "    # Loop for testing data\n",
    "    for i in range(timesteps, test.shape[0]):\n",
    "        X_test.append(test[i-timesteps:i])\n",
    "        Y_test.append(test[i][0])\n",
    "    X_test, Y_test = np.array(X_test), np.array(Y_test)\n",
    "\n",
    "    # Evaluate model\n",
    "    Y_hat = model.predict(X_test)                         #ch·ª©a d·ª± ƒëo√°n c·ªßa model d·ª±a tr√™n ƒë·∫ßu v√†o x_test\n",
    "    mse = mean_squared_error(Y_test, Y_hat)\n",
    "    rmse = sqrt(mse)\n",
    "    mape = mean_absolute_percentage_error(Y_test, Y_hat)\n",
    "    r2 = r2_score(Y_test, Y_hat)\n",
    "    return mse, rmse, mape, r2, Y_test, Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot chart (v·∫Ω bi·ªÉu ƒë·ªì)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the predictions\n",
    "def plot_data_2(Y_test,Y_hat):\n",
    "    plt.plot(Y_test, c = 'r')\n",
    "    plt.plot(Y_hat, c = 'y')\n",
    "    plt.xlabel('Day')\n",
    "    plt.ylabel('Price')\n",
    "    plt.title(\"Stock Price Prediction using Multivatiate-RNN\")\n",
    "    plt.legend(['Actual','Predicted'], loc = 'lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training errors: tr·ª±c quan loss qua c√°c epoch -> th·∫•y qtr h·ªçc m√¥ h√¨nh, xem c√≥ overfitting ko\n",
    "def plot_error(train_loss, val_loss):\n",
    "    plt.plot(train_loss, c = 'r')\n",
    "    plt.plot(val_loss, c = 'b')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.title('Train Loss and Validation Loss Curve')\n",
    "    plt.legend(['train', 'val'], loc = 'upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model building**: X√¢y d·ª±ng m√¥ h√¨nh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 1: Tr√≠ch xu·∫•t v√† tr·ª±c quan h√≥a d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3991, 10)\n",
      "              close     CMA10     SMA10      SMA50        EMA12        RSI  \\\n",
      "time                                                                         \n",
      "2025-03-10  1391.07  1381.379  1369.467  1336.1956  1368.821086  81.914388   \n",
      "2025-03-11  1393.57  1384.753  1372.768  1337.6066  1372.628611  80.831758   \n",
      "2025-03-12  1392.39  1386.820  1375.951  1338.9890  1375.668825  78.354464   \n",
      "2025-03-13  1387.30  1388.192  1378.319  1339.8854  1377.458236  71.932575   \n",
      "2025-03-14  1387.03  1388.125  1381.379  1340.7724  1378.930815  67.050447   \n",
      "\n",
      "                   CCI         %K         %D       MACD  \n",
      "time                                                     \n",
      "2025-03-10  186.248408  86.033606  94.302545  14.085059  \n",
      "2025-03-11  143.873911  88.007313  90.655860  15.015993  \n",
      "2025-03-12  152.955891  84.673892  86.238270  15.480105  \n",
      "2025-03-13  121.672693  73.658150  82.113118  15.261273  \n",
      "2025-03-14   97.327890  73.123391  77.151811  14.894368  \n"
     ]
    }
   ],
   "source": [
    "# Extracting the series\n",
    "series = df[['close', 'CMA10', 'SMA10', 'SMA50', 'EMA12', 'RSI', 'CCI', '%K', '%D', 'MACD']]\n",
    "# Drop rows with NaN values\n",
    "series = series.dropna()\n",
    "\n",
    "# Display the shape and the tail of the cleaned series\n",
    "print(series.shape)\n",
    "print(series.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>CMA10</th>\n",
       "      <th>SMA10</th>\n",
       "      <th>SMA50</th>\n",
       "      <th>EMA12</th>\n",
       "      <th>RSI</th>\n",
       "      <th>CCI</th>\n",
       "      <th>%K</th>\n",
       "      <th>%D</th>\n",
       "      <th>MACD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3991.000000</td>\n",
       "      <td>3991.000000</td>\n",
       "      <td>3991.00000</td>\n",
       "      <td>3991.000000</td>\n",
       "      <td>3991.000000</td>\n",
       "      <td>3991.000000</td>\n",
       "      <td>3991.000000</td>\n",
       "      <td>3991.000000</td>\n",
       "      <td>3991.000000</td>\n",
       "      <td>3991.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>814.145743</td>\n",
       "      <td>814.003191</td>\n",
       "      <td>812.86794</td>\n",
       "      <td>807.414210</td>\n",
       "      <td>812.591967</td>\n",
       "      <td>54.032542</td>\n",
       "      <td>17.593878</td>\n",
       "      <td>57.339202</td>\n",
       "      <td>57.342557</td>\n",
       "      <td>1.921103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>321.109578</td>\n",
       "      <td>320.781191</td>\n",
       "      <td>320.75575</td>\n",
       "      <td>319.492655</td>\n",
       "      <td>320.500740</td>\n",
       "      <td>18.541935</td>\n",
       "      <td>110.243542</td>\n",
       "      <td>32.341940</td>\n",
       "      <td>30.423710</td>\n",
       "      <td>13.176084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>254.820000</td>\n",
       "      <td>261.983000</td>\n",
       "      <td>251.40000</td>\n",
       "      <td>265.341000</td>\n",
       "      <td>252.982926</td>\n",
       "      <td>4.401922</td>\n",
       "      <td>-339.712898</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-60.553670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>552.290000</td>\n",
       "      <td>551.247500</td>\n",
       "      <td>550.82900</td>\n",
       "      <td>547.833600</td>\n",
       "      <td>549.926815</td>\n",
       "      <td>40.285656</td>\n",
       "      <td>-70.241516</td>\n",
       "      <td>28.668093</td>\n",
       "      <td>28.989233</td>\n",
       "      <td>-4.653577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>673.530000</td>\n",
       "      <td>672.566000</td>\n",
       "      <td>671.65700</td>\n",
       "      <td>656.734400</td>\n",
       "      <td>669.526149</td>\n",
       "      <td>54.411293</td>\n",
       "      <td>32.984730</td>\n",
       "      <td>62.714014</td>\n",
       "      <td>62.491154</td>\n",
       "      <td>1.775423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1056.925000</td>\n",
       "      <td>1058.503000</td>\n",
       "      <td>1057.24600</td>\n",
       "      <td>1057.393900</td>\n",
       "      <td>1058.727874</td>\n",
       "      <td>67.773165</td>\n",
       "      <td>106.763138</td>\n",
       "      <td>87.016932</td>\n",
       "      <td>85.932690</td>\n",
       "      <td>9.299267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1572.460000</td>\n",
       "      <td>1543.976000</td>\n",
       "      <td>1543.97600</td>\n",
       "      <td>1524.849800</td>\n",
       "      <td>1541.505267</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>314.013206</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>49.008428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             close        CMA10       SMA10        SMA50        EMA12  \\\n",
       "count  3991.000000  3991.000000  3991.00000  3991.000000  3991.000000   \n",
       "mean    814.145743   814.003191   812.86794   807.414210   812.591967   \n",
       "std     321.109578   320.781191   320.75575   319.492655   320.500740   \n",
       "min     254.820000   261.983000   251.40000   265.341000   252.982926   \n",
       "25%     552.290000   551.247500   550.82900   547.833600   549.926815   \n",
       "50%     673.530000   672.566000   671.65700   656.734400   669.526149   \n",
       "75%    1056.925000  1058.503000  1057.24600  1057.393900  1058.727874   \n",
       "max    1572.460000  1543.976000  1543.97600  1524.849800  1541.505267   \n",
       "\n",
       "               RSI          CCI           %K           %D         MACD  \n",
       "count  3991.000000  3991.000000  3991.000000  3991.000000  3991.000000  \n",
       "mean     54.032542    17.593878    57.339202    57.342557     1.921103  \n",
       "std      18.541935   110.243542    32.341940    30.423710    13.176084  \n",
       "min       4.401922  -339.712898     0.000000     0.000000   -60.553670  \n",
       "25%      40.285656   -70.241516    28.668093    28.989233    -4.653577  \n",
       "50%      54.411293    32.984730    62.714014    62.491154     1.775423  \n",
       "75%      67.773165   106.763138    87.016932    85.932690     9.299267  \n",
       "max     100.000000   314.013206   100.000000   100.000000    49.008428  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 2: Chia d·ªØ li·ªáu th√†nh c√°c t·∫≠p Train, Validation, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3991, 10)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2795, 10) (598, 10) (598, 10)\n"
     ]
    }
   ],
   "source": [
    "n = series.shape[0]\n",
    "val_size =  test_size = int(n * 0.15)\n",
    "train_size = n - val_size - test_size # ƒê·ªÉ tr√°nh sai s·ªë l√†m m·∫•t d·ªØ li·ªáu\n",
    "\n",
    "# Chia t·∫≠p d·ªØ li·ªáu theo th·ª© t·ª± th·ªùi gian\n",
    "train_data = series.iloc[:train_size].values\n",
    "val_data = series.iloc[train_size:train_size + val_size].values\n",
    "test_data = series.iloc[(train_size + val_size):].values\n",
    "# Ki·ªÉm tra k√≠ch th∆∞·ªõc c·ªßa t·ª´ng t·∫≠p\n",
    "print(train_data.shape, val_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 3: Chu·∫©n h√≥a d·ªØ li·ªáu b·∫±ng MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2795, 10) (598, 10) (598, 10)\n"
     ]
    }
   ],
   "source": [
    "# Normalisation\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(2)\n",
    "rn.seed(3)\n",
    "\n",
    "# Chu·∫©n h√≥a\n",
    "sc = MinMaxScaler() # T·∫°o b·ªô chu·∫©n h√≥a MinMaxScaler\n",
    "train = sc.fit_transform(train_data)\n",
    "val = sc.transform(val_data)\n",
    "test = sc.transform(test_data)\n",
    "\n",
    "print(train.shape, val.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 4: T√¨m si√™u tham s·ªë t·ªët nh·∫•t b·∫±ng Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with param: {'timesteps': 50, 'hl': [40, 35], 'lr': 0.0001, 'batch_size': 64, 'num_epochs': 250}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 0.39085, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_loss improved from 0.39085 to 0.10623, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: val_loss improved from 0.10623 to 0.02741, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: val_loss improved from 0.02741 to 0.01941, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5: val_loss improved from 0.01941 to 0.01768, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6: val_loss improved from 0.01768 to 0.01545, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7: val_loss improved from 0.01545 to 0.01335, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8: val_loss improved from 0.01335 to 0.01125, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9: val_loss improved from 0.01125 to 0.00910, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10: val_loss improved from 0.00910 to 0.00679, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11: val_loss improved from 0.00679 to 0.00505, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12: val_loss improved from 0.00505 to 0.00417, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13: val_loss improved from 0.00417 to 0.00364, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14: val_loss improved from 0.00364 to 0.00332, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15: val_loss improved from 0.00332 to 0.00307, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16: val_loss improved from 0.00307 to 0.00289, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17: val_loss improved from 0.00289 to 0.00280, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18: val_loss improved from 0.00280 to 0.00276, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 74: val_loss improved from 0.00276 to 0.00276, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 75: val_loss improved from 0.00276 to 0.00270, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 76: val_loss improved from 0.00270 to 0.00266, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 77: val_loss improved from 0.00266 to 0.00261, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 78: val_loss improved from 0.00261 to 0.00258, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 79: val_loss improved from 0.00258 to 0.00254, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 80: val_loss improved from 0.00254 to 0.00252, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 81: val_loss improved from 0.00252 to 0.00249, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 82: val_loss improved from 0.00249 to 0.00245, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 83: val_loss improved from 0.00245 to 0.00242, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 84: val_loss improved from 0.00242 to 0.00240, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 85: val_loss improved from 0.00240 to 0.00237, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 86: val_loss improved from 0.00237 to 0.00236, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 87: val_loss improved from 0.00236 to 0.00233, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 88: val_loss improved from 0.00233 to 0.00232, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 89: val_loss improved from 0.00232 to 0.00229, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 90: val_loss improved from 0.00229 to 0.00227, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 91: val_loss improved from 0.00227 to 0.00225, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 92: val_loss improved from 0.00225 to 0.00223, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 93: val_loss improved from 0.00223 to 0.00220, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 94: val_loss improved from 0.00220 to 0.00219, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 95: val_loss improved from 0.00219 to 0.00216, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 96: val_loss improved from 0.00216 to 0.00214, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 97: val_loss improved from 0.00214 to 0.00212, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 98: val_loss improved from 0.00212 to 0.00208, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 99: val_loss improved from 0.00208 to 0.00205, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 100: val_loss improved from 0.00205 to 0.00202, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 101: val_loss improved from 0.00202 to 0.00200, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 102: val_loss improved from 0.00200 to 0.00196, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 103: val_loss did not improve from 0.00196\n",
      "\n",
      "Epoch 104: val_loss improved from 0.00196 to 0.00191, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 105: val_loss did not improve from 0.00191\n",
      "\n",
      "Epoch 106: val_loss improved from 0.00191 to 0.00187, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 107: val_loss did not improve from 0.00187\n",
      "\n",
      "Epoch 108: val_loss improved from 0.00187 to 0.00183, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 109: val_loss did not improve from 0.00183\n",
      "\n",
      "Epoch 110: val_loss improved from 0.00183 to 0.00179, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 111: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 112: val_loss improved from 0.00179 to 0.00175, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 113: val_loss did not improve from 0.00175\n",
      "\n",
      "Epoch 114: val_loss improved from 0.00175 to 0.00172, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 115: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 116: val_loss improved from 0.00172 to 0.00169, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 117: val_loss improved from 0.00169 to 0.00169, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 118: val_loss improved from 0.00169 to 0.00166, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 119: val_loss did not improve from 0.00166\n",
      "\n",
      "Epoch 120: val_loss improved from 0.00166 to 0.00163, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 121: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 122: val_loss improved from 0.00163 to 0.00162, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 123: val_loss improved from 0.00162 to 0.00162, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 124: val_loss improved from 0.00162 to 0.00160, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 125: val_loss did not improve from 0.00160\n",
      "\n",
      "Epoch 126: val_loss improved from 0.00160 to 0.00158, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 127: val_loss did not improve from 0.00158\n",
      "\n",
      "Epoch 128: val_loss improved from 0.00158 to 0.00157, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 129: val_loss improved from 0.00157 to 0.00156, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 130: val_loss improved from 0.00156 to 0.00154, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 131: val_loss improved from 0.00154 to 0.00153, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 132: val_loss improved from 0.00153 to 0.00150, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 133: val_loss improved from 0.00150 to 0.00147, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 134: val_loss improved from 0.00147 to 0.00146, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 135: val_loss improved from 0.00146 to 0.00143, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 136: val_loss improved from 0.00143 to 0.00141, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 137: val_loss improved from 0.00141 to 0.00137, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 138: val_loss improved from 0.00137 to 0.00135, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 139: val_loss improved from 0.00135 to 0.00134, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 140: val_loss improved from 0.00134 to 0.00130, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 141: val_loss improved from 0.00130 to 0.00129, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 142: val_loss improved from 0.00129 to 0.00126, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 143: val_loss improved from 0.00126 to 0.00124, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 144: val_loss improved from 0.00124 to 0.00122, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 145: val_loss improved from 0.00122 to 0.00119, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 146: val_loss improved from 0.00119 to 0.00118, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 147: val_loss improved from 0.00118 to 0.00118, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 148: val_loss did not improve from 0.00118\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.00118\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.00118\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.00118\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.00118\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.00118\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.00118\n",
      "\n",
      "Epoch 155: val_loss improved from 0.00118 to 0.00104, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 156: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 198: val_loss improved from 0.00104 to 0.00092, saving model to 10Var-vn30-rnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 199: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.00092\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step\n",
      "   timesteps        hl      lr  batch_size  num_epochs       MSE     RMSE  \\\n",
      "0         50  [40, 35]  0.0001          64         250  0.000737  0.02714   \n",
      "\n",
      "       MAPE        R¬≤  \n",
      "0  0.020182  0.944431  \n",
      "Best parameters: {'timesteps': 50, 'hl': [40, 35], 'lr': 0.0001, 'batch_size': 64, 'num_epochs': 250}\n",
      "Best RMSE score: 0.027140470245666452\n"
     ]
    }
   ],
   "source": [
    "best_params, best_score, results_df = grid_search_rnn(train, val, test, param_grid)\n",
    "\n",
    "print(results_df)\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best RMSE score:\", best_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 5: Hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi b·ªô tham s·ªë t·ªëi ∆∞u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.00092\n"
     ]
    }
   ],
   "source": [
    "timesteps = 50\n",
    "hl = [40, 35]\n",
    "lr = 0.0001\n",
    "batch_size = 64\n",
    "num_epochs = 250\n",
    "\n",
    "model, train_error, val_error = fit_model_2(train, val, timesteps, hl, lr, batch_size, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 6: ƒê√°nh gi√° m√¥ h√¨nh v√† tr·ª±c quan h√≥a k·∫øt qu·∫£"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1. V·∫Ω bi·ªÉu ƒë·ªì train_loss v√† val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAHMCAYAAAA067dyAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAWcZJREFUeJzt3QmcU+W5x/EnszHs+47sixugVuFesArighQVrAuirRVBKy5dtYpaqRVaavXSFuz1amtLcQGpiKzaUlyh7guIioCUXRh2Bhhmyf383+GEJGRgBjI5Z8jv+/lkknNycubkzUny5Hmf855QOBwOGwAAQBrL8HsDAAAA/EZABAAA0h4BEQAASHsERAAAIO0REAEAgLRHQAQAANIeAREAAEh7BEQAACDtERABAIC0R0CEKiUUClmfPn383gyUQ9u2bd0lKEaPHu32n1dfffWY9ik9Xo/R+vzYXgCVg4AIFaIP6Ipc/vKXv1hV4n3ZEXSlzr333uva/K677jrisjfddJNb9n/+53+sqtN7o6q9R7wgrbKDwVQpKSmxadOm2be//W074YQTLDc312rWrGknnXSS29feeustvzcRKZSVyn+Gqu+BBx44ZN748eNtx44d9oMf/MDq1asXc99pp52W1P//2WefWY0aNZK6Tvhr+PDh9qtf/comTZpkY8aMsezs7ITL5efn23PPPWfVqlWz66+//rjfp2677TYbMmSItW7d2u9NOS5t3LjRrrjiChf01K5d2y644ALr0KGD6fSeX375pT377LP2xBNP2B/+8Af3WuD4R0CECkn0y1C/cBUQ/fCHP6z0LpITTzyxUteP1GvXrp2df/759o9//MNmzpxpl19+ecLlFAzt2rXLhg4dag0aNDju96lGjRq5C5Jvz5491r9/f/v4449d0PnYY49Z/fr1Y5bZuXOn/fa3v3WfbUgPdJmh0qjbSen1/fv324MPPmhdunRxv+6/973vufv1QfPwww/beeedZ61atbKcnBxr3LixXXrppbZo0aKE60zUnRVda6H0d48ePdwvfn1p6sNu3bp1lfYcN2zYYLfeeqsLBL3t1xf6+++/f8iyaoff//73dsYZZ7gPX22jHnfZZZfZP//5z5hl33jjDbvkkktcu6jNmjVrZv/1X/9lv/jFL8q1XfpfEyZMsAEDBlibNm3cOtQeCjzmzp172JofZWLuvPNOl5nQ4zp27Gjjxo1zv5zjaZ7+zymnnOK6G1q2bOl+TVf0S0TdE6Jf5GXx7vOWXbBggbt98sknW506dax69ep26qmnujbat29fuf93WV2kX3/9td14443WtGlTt25lO//617+WuR695sqSdu/e3bW12qNTp072k5/8xLZt2xazrP7fDTfc4G7rOrqbedWqVUesIZo/f777Qtf/0WvUuXNnu/vuuxO2u/c+LCoqsrFjx7pt0mPURfSzn/3M7SuVRW2i7qgmTZq4/6l9ceTIke59k6i9f/rTn7rPCXVbKdus2/q8WLlyZcw+p9ehV69e7v2mdtZzueiii2zKlCnl2i51uSoY6t27tz399NOHBEOifUqfW9omj7Yl+jUqT23Z4T4Hf/3rX7v7fve73yXczvXr11tWVpadeeaZMfP1WiqI02eCtlOfJaeffrp7L6obEEeHDBEqnT4Q3333Xbv44ott0KBB7sPR66pQ/cg555xj3/rWt9yH0urVq+2ll15yX9rKFuhDv7z0AaHHKqA699xz7e2333YfkPrg++ijj9yHUDJ99dVXdvbZZ7sPLQV111xzja1Zs8aef/55mz17tv3973+3gQMHRpbXB6DS8PrS/u53v+u+ZPXYN9980+bNm+eCFdFttYc+6PRcFGRs3brVtZeeY6Juy3haXl/O+tJQV4C+OPQlpDZVkKTgQl1V8QoLC90Xi7ZLr5c+jF988UX3ZasgI/5/KyuoIK958+YuOFF314wZM1zb6wtAQWJ5KCjUfvHKK6+4fSC+m2jJkiVunfri12srCtI+//xz9xzVXto+dX/oC0lfTgoyMzMz7Wjk5eW59eqLWK+xLmq/73//+3bhhRcmfIzadPr06W779Frqi0kBwaOPPur2Z22/uma8fUFf+GorPfforuX4bud4jz/+uN1yyy0uaLjyyitdu+n5qj30+qoNEq1DmTUF2npdtW/NmTPHfvOb39imTZvsqaeesmSbNWuWe+8rgFHXlIIhtccf//hH97y13ys76GVsFJysWLHC7a/6MaDH/ec//3HL6vHt27d3y+ozQ12seuxVV11ldevWda+NPmP03rv66quPuG3/93//567vv/9+y8g4fF4gWZ8biT4H9YNNz0fdxXq/xps8ebIVFxdHfkR671G1z8svv+yCK72uCgr1A+H22293+9nf/va3pGxz2gkDx6hNmzZKHYS/+uqrmPnnnnuum9+1a9fw5s2bD3nc9u3bE85fs2ZNuHnz5uETTzzxkPu0Pq032gMPPODm165dO/zJJ5/E3HfNNde4+6ZMmVKu57JgwYKE/yORCy+80C370EMPxcx/6623wpmZmeEGDRqEd+3aFXmuoVAo/I1vfCNcVFR0yLry8vIity+//HK33o8++uiQ5RK1VyL79u1z7RhP23HKKaeE69evH96zZ0/C1/Hiiy+Oue/rr78O161b1132798f8zy1fIcOHcJbtmyJzN+7d2/4v/7rv9x9Wmd53XXXXe4xej3j3XHHHe6+hx9+ODJvxYoV4ZKSkkOWve+++9yyzz33XML9RK9xtESv94gRI9z8H/7whzHz33333XBWVlbC7Vy1alXC1/bJJ590y//617+Omf/UU0+5+bpOJNH26n/k5OS4ff2zzz6LWf6WW25xy2vbE70PzzjjjJjXaffu3e61y8jICG/YsCHhNpS1TYleo2ja77X/a92vv/56zH1qB63jggsuiMx76aWXEra3FBQUhHfu3BmZ1npbtmwZzs/PP6r3x+rVq93/0uuofbUirr/++oSfddGfHfFtc6TPQe9zZPHixYfcd/LJJ7vXO/rzwXsNbrvttpj9TbeHDRvm7nvxxRcr9LxQii4zVLpf/vKXCWsh9Msu0Xx1E+kXoX79K1tQXnfccYd17do1Zt6IESPc9TvvvGPJtHbtWpfNUCYj/ugoZRaULVKW5oUXXnDzlBbXd69+bSb6RdqwYcND5imDFK+8NSX6P2rHRG0+bNgw14WjX6uJKOMT/b/1S1ZZDHXHfPHFF5H5XlZBv3Cja3r0a1W/4CtKr5XaSeuNTvsXFBS4X8rKNkX/UlbGQMvH+9GPfuSu9Qv6aOgXuLpRlM2J7/5Q18W1116b8HHKgCTKSKm9lZE52u2JpnZQ5k3dkvG1TypI1zYrO6A2i6cMUvTrpAyTnova+r333rNkUlZH+7+yNd/85jdj7lMXorpmVTMW//5OtM/rdfcyax5lIhO1dXneH153nd5z2lf9/hz0DhCI747Va7J06VKX/fQ+H/RaqchbXejq9otuA91+5JFH3HtC+y8qjoAIlU41PWVRel9pb9UA6Evcq6PQm14qUv8T388uWq/E13Acqw8//NBd68M+0VFR6kKLXk5fiEpzL1y40HWPqJZAKW51FcTzvnB79uzpumjU7acArKI+/fRTF0AocNAXjde2+kIqq20VMKlmqDzt+MEHH7hrrwsrmrqYKtpdpf/bt29f9yUZHTyo61FfrupmiP5CUa2TamLOOusst90KNPX8vC+Po60dUyCu10Wvk9Ybr6whGRRIqYZDz12Bh56/tkfbpQLdZNSyeW3u7V/R1OWsOhJ1Heo5+Pn+ONx2qhtW3eTR7w/tQ+oaVk2NuskVlKt7Td1Fid4fquFR7dg999zjupirQuFzWZ+DgwcPdvuZgpjo5+sFSNE/ApYtW+beCwoQH3roIRewR190xK/e6+peR8VRQ4RKp18ziajeQpkg/UrzDnnVr1Z9gagm4rXXXkv4S7csieom9OEriT5Yj4X3AazamUS8+du3b4/MU2CjX+nPPPNMpBZHz11toKNZVLwrKspW/YV+7f35z392NSPyjW98w2Ve1FZH8u9//9t9Gan4sl+/fq4WSUGZ2lb1VPoFn6hty6pfSdSOXht42x2//NEcIaUs0b/+9S978sknXa2F6HZ0MbUXfOj5KfOnmixlIlQn5QWnKqyuyL4T7XDP63D7s7ZB+7QCUGXUtJxXf6IvqqPdnmPd76rC+0P7pvZZvS9UB+gFxNqHVIR93333RV5bZUbUxsokKoDSRc9DtXF6zyQK6BP97y1btrjgMVVZorL2GwUw+lGoGjRlnbXfKwuoekPt0977wNtm0bAAhzvAYvfu3ZXwDI5/BESodIm6NbyCRqXDlRrWQGjRbr75ZhcQBZWXOdBYJodLy0dnGPTB5/2SU/H166+/7oYsUDeIfvGq4NWjNLkuyoKoSFIBkopRVaStX9X6dXw4+vW4d+9el4WKz2goqFJAdKy856ajg7yCV48CMRUmJ+q2OxwFg/oSVHGw1qvD7BUc60suOtug7VcwpF/P8QXBavvyHo13pOeVSKLXXPuwgiHvKD4v0PC6OVS8nOz9Tkf2lWe/qyrvD+0rf/rTn1zXsrqKFBhPnDjRZVPVhupyEmXeVMyviwrCVZytIRlUUK2sqC6HK4RWVkxd3cpE6j1YVpF8Il53t/bveImC0PJ8DnrdZgqIlBVSAKSDMhT8qNA6OgPttZeySl53PJKHLjP4Zvny5e6LPT4Y0oefPuSCTF0Tou1M9OGoQER0iH1ZH8pK/euXsL7stR7v1180ZcwUCOhIpVGjRrlfjmUdNh/ftuq2SdS9k6xA03tuidan53M0WQcFyPpyUAZIXw7eF6SOiIv+QtHzk0RjFh3r81Ntjg5jViYtUVdMosPgve1RJi46GBIFbgpO43ldihVpJ2+/S7QN+kLWNivjEf+eSrXDbafeL17wn+j9oddZwZ6OmFKdkehIx0RU36Z9YOrUqe59oqPUdETikXjZRv1wONJh6tGZPe/wfP2giXcsdVg6wk7DISjQ1z7ndZfFD0CqfVOZPmXT9B5BchEQwTcqrFTqV4d4e/TlpwyKfiEGmX7NqutKmR11h0RTRkfdYvrw1C852bx5sy1evPiQ9SgDpPS2vkS9Q9T1qzVRkOVlLMozqrLaVrUGn3zyScx8BRjJKO6Nrm1QMa/+l0fdEKrtOFpeIbx+MSuDpl/I0XUU4g0AGv+Fq8PkNbbOsdD/U7Cq7FR8UbW+9BIVrJa1PcpgaJyqRLxap4ocOHDddde57VONnReERWdcVaukZZI9xERFqd5LAbm6ffTlHU3vFw1ZoWyaN7yCsjqJMnLx+7yCk0Sn01Bw4O2D5Xl/qPBe40UpMNMQGImyO3pfKtOo7uz4OqD48bL03i5rLKHyUvCj946G1tCQCN26dYsElh59TihQVIZNB5EkCrR1X9A/P4OKLjP4Rh9KKhrWm15jdOiDXh92ejOrAFndJn5RUWr8l7BHH+JK4//v//6v+2WnQQzV96+iVW8cIqXW1ZXjHR2jglo9Tx0Fpw86ZYj05aWuMHUr6MPNW1a3tbzW7Q34qAJTdSHoSCaNXXIk6k5Q4KMCX2+sFn2ZK3OjmiUNYHmstH36cNaXs+p4tF5vHCIFg2XVjxyJxlZR0a0CQ9G+EV/Po/1DmTVlzvRlpLZVYKH2VFdjRYKMRFSsrcEP9eWtdvPGIVIdmGpVVOcSTYXdag91Y+goQy2vL3Nl8/R8WrRoccj/+O///m/35a3/oeygV2OiNi2ry0v7g5ZXkKXsil5b1ZkoK6bBTJVBUJ1aZVPGJtHghKIuKI2No/o3jZOkgmld632j/VjvFT1XrzZOlAnS+0htorGmlPnRgQTal/Re0n2iAEBtq9deNXV6PyiI0ONVSKwMXXmyY2p3FWNrn1WAq8+a6FN3KNjU66/3qArlPaoNUyZHgZ62Twc+aF/zxpNSpupofec737Gf//znro5KAV5Zp6dR4Kux1fT5o+1WZkwF6Qq+9QNTn6H6kXKkbnUkcODwe6DSxiE6HI3B0r1793CNGjXCDRs2DA8aNMiNJVSRMWPKWla0TbpP44eUhzeWyOEu2l7P2rVrw9///vfDrVu3DmdnZ7vncNlll4XfeeedmPVu27Yt/Itf/CLct2/fcIsWLdzYIs2aNXPP5ZlnnokZT0djJg0ZMiTcsWPHcM2aNd2YMxo7aNSoUeFNmzaFy2vmzJnhnj17hmvVquXGENK4L6+99lqZ49/odSxr3KCy2ljb/Yc//MGNGaXnpPGjRo4c6cY7Otz6jmTy5MmR9n755ZfLHE9m6NChrj1zc3PdmC3jxo0LFxYWVmg/KWvcKY3Nc8MNN4QbNWrk1q/XXW1W1ngzGuNHYwHpOVerVi3cvn378D333OPGyymrLebOnevGbNLr7D1f7310uP1abaLXs169eq7dNZ7QnXfe6fazeId7Hx5pLKR43jYd7vKDH/wgsrzeB3pPqw31/jjhhBPc+2XdunUx6126dGn4Rz/6kRunS8vqOam9vv3tb7vxrjwaB0uvcf/+/d261M5aXvv5H//4RzdmUUUUFxeHp06dGh48eLAb20jrq169erhLly7hG2+8MeZ/R+93V111lRvLS/vFmWeeGf773/9+xHGIyqNfv36RMZI2btxY5nJ6302aNCl83nnnue1Q2+p90Lt37/CYMWPcNqLiQvqTKFACAABIF9QQAQCAtEdABAAA0h4BEQAASHsERAAAIO0REAEAgLRHQAQAANIeAREAAEh7BEQAACDtceqOCti2bVvCc0wdKw29r3NdoXLRzqlDW6cG7Zw6tHXVbGed/807Ke8Rl03af00DCoaSfYZh7wzeWjeDhlce2jl1aOvUoJ1Th7ZOj3amywwAAKQ9AiIAAJD2AtllNm/ePJs5c6Zt377d2rRpY8OGDbOOHTse8XFvvfWW/e53v7MzzzzT7rrrrsh8pd6mTp1q8+fPt/z8fDvxxBNt+PDh1rx580p+JgAAoCoIXIZo4cKFNmnSJLviiits3LhxLiAaM2aM7dix47CP27Rpk/3tb3+zk0466ZD7ZsyYYXPnzrURI0bY2LFjrVq1am6d+/fvr8RnAgAAqorAZYhmzZpl/fr1s759+7ppBTEffPCBLViwwAYNGpTwMSUlJfaHP/zBrrrqKvvss89cFig6OzRnzhy7/PLL7ayzznLzbrvtNrfed99913r37p2iZwYAwKH0naVCYq+oOJ3t3bu3wsmKGjVquKPJjquASDvEypUrYwKfjIwM69q1qy1btqzMx02bNs3q1Klj5513nguI4jNH6nrr1q1bTOOpC07rTBQQ6Uiy6KPJtJNWr149cjuZvPXxRqhctHPq0NapQTsfH229b98+t966desmfd1VUXZ2doWO5lZCZNeuXVazZk332OMmINq5c6d7cvXq1YuZr+n169cnfMznn39u//rXv+w3v/lNwvsVDEn8zqZp775406dPd0GWp127dq77TuMjVJZmzZpV2rpxEO2cOrR1atDOVbutv/rqK/eDnsD2oIoGNlpemaVjrQsOVEBUUWoAdZXdfPPNbodKlsGDB9vAgQMj096OqsGikj0wo9atN9nGjRsZ36IS0c6pQ1unBu18fLR1QUGBq2vF0WWIojNtGzZsOGS+utLKm8wIVECkoEZdZPGZG03HZ43k66+/dkGKsjceb2cdMmSIjR8/PvI4FWVHj1ap6bZt25b5gpQVoVbWB4/Wy4da5aOdU4e2Tg3aOXVo62A71tcmUAGRIrn27dvbkiVLrEePHm6eutA03b9//0OWb9Gihf32t7+Nmffcc8+5SPF73/ueNWrUyDIzM11QtHjx4kgAtGfPHlu+fLldeOGFKXpmAAAgyAIVEIm6qiZOnOgCIxU+6wgxpRT79Onj7p8wYYI1aNDAhg4dajk5Oda6deuYx6uwSqLnDxgwwF544QXXv9ikSRMXNClb5B11BgAAUq9nz55uXEAd+e23wAVEvXr1csXVGkhRXWXK6owaNSrS9ZWXl1fh4rPLLrvMBVWPP/64yw5pYEatUwEVAAAoP40TePLJJ9uDDz5ox0pJDx35HQSBC4hE3WOJushk9OjRh33srbfeesg8BVBXX321uwTJzp0h27Ur047xSEEAAAJVy1NcXFyusYEaNmxoQRG4karTyV//WtN69Ghid9/t95YAAHBkP/zhD23RokX2pz/9yVq2bOkuU6ZMcdcaAkfJDA1V884779iqVavshhtusO7du1unTp1c+crrr79+SJfZE088EZlWWcszzzxjN954o3Xo0MGNFfjKK69Y2maI0kXGgXC0pMTvLQEA+C4cttDevf786+rV1Z1yxOXUTaYBlFV68tOf/tTN++KLL9y1To3185//3NXwaqw/jR+oAZN/9rOfuRIVje+nAElBkQKosjz66KN23333uctTTz3lzi7x9ttvxxwpXhkIiHzk7XsERAAABUPNO3Xy5X9v+PJLC5ejlkfD4yi4yc3Nddkc0VHbcuedd9o555wTWVYBzCmnnBKZ1knXdfJ2ZXwUGJVFp+Hyzlhx9913u2zURx99FDmlV2UhIPJRRkbpmAkMawEAqOq6RZ0iyztH2yOPPGLz5893p9HSwMYaFmfdunWHXU/0SdpVcF27dm13QFVlIyDyERkiAEB0t5UyNX7972MVf7SYutfeeOMNu//++90R48oq3XTTTUc8eWv8wMg6MEpjElY2AiIfUUMEAIgIhcrVbeW37OzscgUo7733nl155ZV28cUXRzJGa9eutaAiIPIRAREAoKo54YQT7MMPP7Q1a9a4wZDLCo50tNncuXPtggsucFmehx9+OCWZnqPFYfcBqCEK8P4BAEAMnVBd5x3VGSS6du1aZk3QAw884I420+DIOp2Wt3xQkSHyETVEAICqpkOHDjZz5syYeYkGPlYm6fnnn4+Zp8Aomg6nj6bi6/iz3X/22WeWCmSIfESXGQAAwUBA5CMCIgAAgoGAyEd0mQEAEAwERAHIEDEwIwAA/iIg8hFHmQEAEAwERD6iywwAgGAgIPIRRdUAAAQDAZGPCIgAAAgGAiIfERABABAMBEQ+CoUoqgYApJeePXvaE088YUFDQOQjMkQAAAQDAZGPCIgAAAgGAiIfcdg9AKAqmTx5sp1xxhlWEvfFdcMNN9iPf/xjW7VqlbvdvXt369Spkw0YMMBef/11qwoIiHzESNUAAI++C/bsCflyCZfze2jgwIG2bds2e+uttyLzNP3qq6/a4MGDLT8/38477zybMmWKvfzyy9anTx8XIK1bt86CLsvvDUhnjFQNAPDs3RuyTp2a+/K/v/xyg9WoceSoqF69eta3b1978cUX7Zvf/KabN3v2bGvQoIH17t3bMjIy7JRTToksf9ddd9m8efPslVdecYFRkJEh8hFdZgCAqmbw4ME2Z84cKygocNPTp0+3Sy+91AVDyhA9+OCDdu6559pJJ53kus2+/PJLMkQ4PIqqAQCe6tXDLlPj1/8urwsuuMDC4bDNnz/f1Qq9/fbbNnr0aHefgqE33njD7r//fmvbtq3l5ubaTTfdZPv377egIyDyEQERACC616A83VZ+y83NtYsvvthlhlRE3aFDB+vatau777333rMrr7zS3S/KGK1du9aqAgIiHxEQAQCqarfZ9773Pfviiy/s8ssvj8xv166dzZ0712WRQqGQPfzww4cckRZU1BD5iJGqAQBV0dlnn+0KrFesWOGCI88DDzxgdevWtcsuu8wFTDrKzMseBR0ZIh+RIQIAVEUZGRn2wQcfHDL/hBNOsOeffz5mngKjaKo5CiIyRD7iKDMAAIKBgMhHZIgAAAiGQHaZaRCnmTNn2vbt261NmzY2bNgw69ixY8JllXpTpfvGjRutuLjYmjVrZpdccomdc845kWUmTpxor732WszjdKjgvffea35ipGoAAIIhcAHRwoULbdKkSTZixAg3oJNGwBwzZoyNHz/eFWrFq1Wrlqtwb9GihWVlZbk+zccee8zq1Kljp512WmQ53R45cmRkWsv6jQwRAADBELgus1mzZlm/fv3c0OCtWrVygVFOTo4tWLAg4fIaIrxHjx5uWWWHdCI5ZZU+//zzmOUUAKki3rsokPIbR5kBABAM/qdJohQVFdnKlStt0KBBMZXsOmRv2bJlR3y8Rs5csmSJrV+/3q699tqY+5YuXWrDhw+3mjVr2qmnnmpDhgyx2rVrJ1xPYWGhu3g0lkL16tUjt5MlMzMUCYiSuV4cymtf2rny0dapQTsfP22t7y5ex6PnjXN0rG0YqIBo586d7okpgxNN0wpyyrJnzx67+eabXUClAOrGG2+0bt26xXSX9ezZ05o0aeJqjZ599lkbO3as64rT8vFUkzRt2rSYgabGjRtnjRs3tmRq0qT0Wq+lsluofLRz6tDWqUE7V+221o9t1cvqxzpKZWdnW3kpZlAMoJ4hL3FxXARExzKMuEbD3Ldvny1evNjVIDVt2jRyxl2dgdfTunVr13C33367ffrppwkHjNIgUwMHDoxMe1Hn5s2bXdCVLFu3qvkbu4BIgZp+JaBy6DXUhxntXPlo69SgnY+ftlaPRF5eHlkiM1ciU9HzntWoUcMFlbrEU7lMeZMZgQqIVAitjE38k9J0fNYomh7jRe46mZzOqvviiy9GAqJ4CpbUXaadO1FApOi0rAg1mW+G6BoirZcPtcpHO6cObZ0atHPVb2uyQ6UUEDZv3tw2bNhQ4XZOxusSqKJqRXLt27d3dUDR6TBNd+7cudzr0WOia4DibdmyxXbv3m3169c3PzEwIwAAwRCoDJGoq0rjBikw0thDc+bMsYKCAnc+FJkwYYI1aNDAhg4dGqn30Zl2lfVREPThhx/aG2+84QqoRd1oGkZcNUTKMn399dc2efJkl1HSWER+IiACACAYAhcQ9erVyxVXT5061XWVqQts1KhRkS6z+H5WBUtPPvmky/qo77Fly5auPkjr8brTVq9e7QZmzM/Pd8GUCq6vvvrqChVuVQYGZgQAIBhCYTqfy01F1YfriquoZcuyrG/fJtaokdknn1S8zxSp6ZtGxdDWqUE7pw5tXXXbWYmP8hZVB6qGKN0wUjUAAMFAQOQjRqoGACAYCIh8RIYIAIBgICDyEQERAADBQEDkIwIiAACCgYDIRwREAAAEAwGRryiqBgAgCAiIfESGCACAYCAg8hEjVQMAEAwERD4iQwQAQDAQEAUkQ0SWCAAA/xAQBWCkaiEgAgDAPwREAcgQCd1mAAD4h4DIRwREAAAEAwGRjwiIAAAIBgIiHxEQAQAQDAREPgqFDt4Oh6MmAABAShEQ+YijzAAACAYCIh/RZQYAQDAQEPmIgAgAgGAgIPIRAREAAMFAQOQjiqoBAAgGAiKfAyKvsJoMEQAA/iEg8hlnvAcAwH8ERD4jIAIAwH8ERAGpIyIgAgDAPwREAckQUVQNAIB/CIh85hVVM1I1AAD+ISDyGTVEAAD4j4DIZwREAAD4j4DIZwREAAD4j4DIZwREAAD4L8sCaN68eTZz5kzbvn27tWnTxoYNG2YdO3ZMuOzbb79t06dPt40bN1pxcbE1a9bMLrnkEjvnnHMiy4TDYZs6darNnz/f8vPz7cQTT7Thw4db8+bNLThF1RxlBgCAXwIXEC1cuNAmTZpkI0aMsE6dOtns2bNtzJgxNn78eKtbt+4hy9eqVcsuv/xya9GihWVlZdkHH3xgjz32mNWpU8dOO+00t8yMGTNs7ty5duutt1qTJk1sypQpbp2PPvqo5eTkmJ/IEAEA4L/AdZnNmjXL+vXrZ3379rVWrVq5wEhBy4IFCxIuf8opp1iPHj3cssoODRgwwGWVPv/880h2aM6cOS5oOuuss9x9t912m23bts3effdd8xsBEQAA/gtUhqioqMhWrlxpgwYNiszLyMiwrl272rJly474eAU/S5YssfXr19u1117r5m3atMl1vXXr1i2yXI0aNVwXnNbZu3fvQ9ZTWFjoLp5QKGTVq1eP3E4mb3XqMkv2unGQ17a0ceWjrVODdk4d2jo92jlQAdHOnTutpKTE6tWrFzNf0wpyyrJnzx67+eabXUClAOrGG2+MBEAKhiS+u03T3n3xVJM0bdq0yHS7du1s3Lhx1rhxY0u2rAOvQMOGjSwAJU3HPWURkRq0dWrQzqlDWx/f7RyogOho5ebm2sMPP2z79u2zxYsXuxqkpk2buu60ozF48GAbOHBgZNqLVjdv3uyCrmQKh5uYWaZt2pRnGzYczEohufQa6k2m4ntlElF5aOvUoJ1Th7auuu2s2uLyJjMCFRCpEFoZnvjMjabjs0bR9Bgvomzbtq2tW7fOXnzxRRcQeY/bsWOH1a9fP/IYTWvZRLKzs90lkWS/GTIywpEaIt5olU9tTDunBm2dGrRz6tDWx3c7B6qoWpFc+/btXR2QR11omu7cuXO516PHeDVAOqpMQZEyR9FdbMuXL6/QOisLRdUAAPgvUBkiUVfVxIkTXWCkwmcdIVZQUGB9+vRx90+YMMEaNGhgQ4cOjdT7dOjQwXWRKQj68MMP7Y033nDjDHkpOB159sILL7hxhxQgPffccy5bpKPO/EZABACA/wIXEPXq1csVV2sgRXWVqVtr1KhRka6vvLy8mAp0BUtPPvmkbdmyxR2e37JlS7v99tvdejyXXXaZW+7xxx932SENzKh1+j0GkRAQAQDgv1CYDtFyU1F19OH4yXDOOU1sxYosmz49z3r02J/UdeMgBdHKEG7YsIEagEpGW6cG7Zw6tHXVbWfVA5e3qDpQNUTp6GBRNeNbAADgFwIin9FlBgCA/wiIfOaVQxEQAQDgHwKigGSI6JYGAMA/BEQ+I0MEAID/CIgCkyGiqBoAAL8QEPks+tQdAADAHwREPuMoMwAA/EdA5DMCIgAA/EdA5DOKqgEA8B8Bkc8oqgYAwH8ERD4LhSiqBgDAbwREPqOGCAAA/xEQ+YyRqgEA8B8Bkc8oqgYAwH8ERD4jQwQAgP8IiAIzUjVHmQEA4BcCIp9RVA0AgP8IiHxGQAQAgP8IiHxGUTUAAP4jIPIZRdUAAPiPgCgwGSKKqgEA8AsBUUCOMiNDBACAfwiIfEZRNQAA/iMg8hlF1QAA+I+AyGdkiAAA8B8Bkc84ygwAAP8REPmMU3cAAOA/AiKf0WUGAID/CIh8RlE1AAD+IyDyGQERAAD+IyDyGUXVAAD4L8sCaN68eTZz5kzbvn27tWnTxoYNG2YdO3ZMuOw///lPe/31123NmjVuun379nbNNdfELD9x4kR77bXXYh7XvXt3u/feey04ARFF1QAA+CVwAdHChQtt0qRJNmLECOvUqZPNnj3bxowZY+PHj7e6desesvzSpUutd+/e1qVLF8vOzrYZM2bYQw89ZI8++qg1aNAgstxpp51mI0eOjExnZWUF7Cgzv7cEAID0Fbgus1mzZlm/fv2sb9++1qpVKxcY5eTk2IIFCxIuf8cdd9hFF11kbdu2tZYtW9r3v/99C4fDtnjx4pjlFADVq1cvcqlVq5YFAUeZAQDgv2CkSQ4oKiqylStX2qBBgyLzMjIyrGvXrrZs2bJyraOgoMCtJz7gUSZp+PDhVrNmTTv11FNtyJAhVrt27YTrKCwsdBdPKBSy6tWrR25XVpdZsteNg7y2pY0rH22dGrRz6tDW6dHOgQqIdu7caSUlJS6DE03T69evL9c6nn76addVpiAqurusZ8+e1qRJE9u4caM9++yzNnbsWNcVp4Ar3vTp023atGmR6Xbt2tm4ceOscePGlmxe3FajRi1r3jwYWavjWbNmzfzehLRBW6cG7Zw6tPXx3c6BCoiO1YsvvmhvvfWWjR492nWzeVRj5GndurUr1L799tvt008/jQmcPIMHD7aBAwdGpr1odfPmzS77lEx799Yxs5q2c+du27BhV1LXDYt5DfUmU0CsLlVUHto6NWjn1KGtq247q1ymvMmMQAVEderUcRkbHV0WTdPxWaN4L730kguI7r//fhfwHE7Tpk1dd5kaPVFApOJsXRJJ9pshuqiaN1rlUxvTzqlBW6cG7Zw6tPXx3c6BKqpWJKfD5pcsWRKZpy40TXfu3LnMx+nIsr///e82atQo69ChwxH/z5YtW2z37t1Wv3598xsDMwIA4L9AZYhEXVUaN0iBkcYSmjNnjiuU7tOnj7t/woQJrkZo6NChblpZoalTp7qjzVQj5GWXcnNz3WXfvn32/PPPuxoiZZm+/vprmzx5skvLaSwivxEQAQDgv8AFRL169XLF1QpyFNzocHplfrwus7y8vJgK9H/84x+urkfjDkW74oor7KqrrnJdcKtXr3YDM+bn57tgqlu3bnb11VeX2S2WSoxUDQCA/wIXEEn//v3dJREVTEdTNulwVFwdhBGpy0JABACA/wJVQ5SODg7MyPgWAAD4hYDIZ5y6AwAA/xEQ+YyiagAA/EdA5DNqiAAA8B8Bkc84uSsAAP4jIPIZRdUAAPiPgMhnoRBF1QAA+I2AyGcUVQMA4D8CIp9RVA0AgP8IiHxGQAQAgP8IiHzGUWYAAPiPgCgwI1VzlBkAAH4hIPIZRdUAAPiPgMhndJkBAOA/AiKfUVQNAID/CIh8RoYIAAD/ERAFpoaIomoAAPxCQBSQo8zoMgMAwD8ERD7jKDMAAPxHQOQziqoBAPAfAZHPKKoGAMB/BEQ+IyACAMB/BEQ+49QdAAD4j4DIZxRVAwDgPwIin1FUDQCA/7KO5cF5eXnucuKJJ0bmrVq1ymbNmmWFhYXWu3dv69GjRzK287hFDREAAFU8Q/TnP//Znn/++cj09u3b7Re/+IW9/fbb9tlnn9kjjzzibqNsdJkBAFDFA6IVK1ZY165dI9Ovv/667d+/3x5++GH73//9X3ffzJkzk7GdadBlRlE1AABVMiDavXu31a1bNzL9/vvv28knn2zNmjWzjIwM1122bt26ZGzncSsU8o4y83tLAABIX8cUENWpU8c2b97sbufn59uXX35p3bt3j9xfUlLiLigbRdUAAFTxomp1ic2dO9dq1Khhn376qYXD4Zgi6rVr11rDhg2TsZ3HLYqqAQCo4gHR0KFDbcOGDfa3v/3NsrKy7Dvf+Y41adLE3aejzBYtWuSONEPZCIgAAKjiAVG9evXsl7/8pe3Zs8dycnJcUORRtuj++++3Ro0aVXi98+bNc8XYOmqtTZs2NmzYMOvYsWPCZf/5z3+6Yu41a9a46fbt29s111wTs7y2ZerUqTZ//nzXtadhAoYPH27Nmze34AREFFUDAFClB2ZUl1l0MCQKkNq2bWu1atWq0LoWLlxokyZNsiuuuMLGjRvnAqIxY8bYjh07Ei6/dOlSl4V64IEH7KGHHnJddLreunVrZJkZM2a4rr0RI0bY2LFjrVq1am6dOiLObxRVAwBQxQOixYsX20svvRQz71//+pfdcsstLvj4y1/+UuGiag3q2K9fP+vbt6+1atXKrUfB1YIFCxIuf8cdd9hFF13kgq+WLVva97//fZcR0raJbs+ZM8cuv/xyO+uss1yAddttt9m2bdvs3XffNb9RVA0AQBXvMtOgjNFdYqtXr7YnnnjCWrdu7Q69V1ZG3WqDBg0q1/qKiops5cqVMcvr8H0Vby9btqxc6ygoKHDr8TJTmzZtcl1v3bp1i8loqUtN60xU46T6J108oVDIqlevHrmdTBkZpetT3JjsdeMgr21p48pHW6cG7Zw6tHV6tPMxBUQaY6hnz56RadXyKHB48MEHXbfU//3f/7l55Q2Idu7c6TJKCqKiaXr9+vXlWsfTTz9tDRo0iAwYqWBIosdL8qa9++JNnz7dpk2bFplu166d675r3LixJZsXT2ZkZAeipul4p0AdqUFbpwbtnDq09fHdzscUEO3bty+SOZGPPvrITjvtNBcMibIwb7zxhqXKiy++aG+99ZaNHj3adbMdrcGDB9vAgQMj0160qjGXlH1Kpm3btJ0NrbCwyDZsKB3TCcmn11Bvso0bN7puVFQe2jo1aOfUoa2rbjurvrm8yYxjCojUXabTd5x33nnuCehIr+hAQiNZZ2dnV2igR3WRxWduNB2fNYqnWiYFRDqyTXVCHu9xKsquX79+ZL6mVXeUiLa5rO1O9pshI+NgUTVvtMqnNqadU4O2Tg3aOXVo6+O7nY8pIDr77LNd15KO6NIgjDVr1nSFyx7VA1WkG0iRnA6bX7JkSWSAR3Whabp///5lPk5Hkb3wwgt27733WocOHWLu07hICopUZO0FQBomYPny5XbhhRea37yuUt5jAAD455gCIh25pS6kDz/80GWLRo4c6YIiLzuk0asHDBhQoXUqwzRx4kQXGKnLTUeIqVC6T58+7v4JEya4GiENCinKCmmMIR1tpuDHyy7l5ua6i1Jw2gYFTArOtMxzzz3nskXRwZtfGJgRAIAqHhBlZma6QRB1iaejvHTEWUX16tXLFVcryFFwo6zOqFGjIl1feXl5MRXo//jHP1xQ9uijj8asR+MYXXXVVe72ZZdd5oKqxx9/3GWHNDCj1nksdUbJQkAEAID/QuEkddSpwFrBiihbpOzM8UZF1dGH4yfDxx/n2IABjaxVqyJ7++1NSV03DlIQrQyhTjVDDUDloq1Tg3ZOHdq66raz6oFTUlQtqsXRoe6ff/55ZBBGFUYrC3PdddcdUtODsoqqGd8CAAC/HFNA9OWXX7pD3FUMrSPNNFK0Nz6RDn/X6TR0f1nnIQNF1QAAVPmASMXJKnDWCV7jD4u/8sor3SHwzz77rLvG4QMiaogAAKii5zJThuiCCy5IOEaQ5p1//vluGZSNc5kBAFDFAyIVQBUXF5d5v2qKOPfL4XGUGQAAVTwg6tKli7388svu6Kt4OuLslVdeccXVKBsBEQAAVbyGSOMPqXD6hz/8oRtZ2huVWidife+999zRZonGKMJBoVBpX1k4TCYNAIAqGRDpLPBjx451hdMKgPbv3+/ma8BDneRVhdW1a9dO1rYel8gQAQDgv2Meh6hVq1Z25513unohjTAdfZJWnS5jypQp7oLECIgAADgOAiKPAqAjnZEehyIgAgCgihdV49gREAEA4D8CosCMVE1RNQAAfiEgCshRZmSIAACoQjVEK1euLPeyW7durejq0w4jVQMAUAUDonvuuadytiRNUUMEAEAVDIhuueWWytmSNEVABABAFQyI+vTpUzlbYuneZRZy3Wac+g0AgNSjqDogRdVCHREAAP4gIApIhkjoNgMAwB8ERD4jIAIAwH8ERD4jIAIAwH8ERD6LLqJmtGoAAPxBQOQziqoBAPAfAZHP6DIDAMB/BEQ+IyACAMB/BEQ+IyACAMB/BEQ+IyACAMB/BEQ+4ygzAAD8R0AUgIDIC4rIEAEA4A8CogDgjPcAAPiLgCgAyBABAOAvAqIAIEMEAIC/sixg5s2bZzNnzrTt27dbmzZtbNiwYdaxY8eEy65Zs8amTJliX331lW3evNmuv/56+9a3vhWzzNSpU23atGkx81q0aGHjx4+34B1pRlE1AACW7gHRwoULbdKkSTZixAjr1KmTzZ4928aMGeOCl7p16x6yfEFBgTVt2tT++7//2/7617+Wud4TTjjB7r///sh0RvSx7gFAhggAAH8FKjKYNWuW9evXz/r27WutWrVygVFOTo4tWLAg4fLKHH3nO9+x3r17W3Z2dpnrVQBUr169yKVOnToWJAREAAD4KzAZoqKiIlu5cqUNGjQoJpDp2rWrLVu27JjWvXHjRrv55ptd0NS5c2cbOnSoNWrUqMzlCwsL3cUTCoWsevXqkdvJpPV5AZHGIUr2+lHKa1fat/LR1qlBO6cObZ0e7RyYgGjnzp1WUlLiMjjRNL1+/fqjXq+63kaOHOnqhrZt2+bqiX7+85/bI488Egly4k2fPj2m7qhdu3Y2btw4a9y4sVUGLyBq1KiJNW9eKf8CBzRr1szvTUgbtHVq0M6pQ1sf3+0cmICospx++umR2yrS9gKkRYsW2XnnnZfwMYMHD7aBAwdGpr1oVYXbymQlP0NU+uJv3LjJatcuTur6cbCd9SZTtjAcDvu9Occ12jo1aOfUoa2rbjtnZWWVO5kRmIBIdT3qItPRZdE0HZ81OhY1a9Z02SI1eFnUtVZWTVJlvBmia4h4s1UutS9tnBq0dWrQzqlDWx/f7RyYompFce3bt7clS5ZE5qkLTdOq+0mWffv2uWAomUHWsaKoGgAAfwUmQyTqppo4caILjHQE2Zw5c9yh9X369HH3T5gwwRo0aOCKokXdV2vXro3c3rp1q61atcpyc3MjfZA6jP/MM890RdSqIdK4RMpEnX322RYUjFQNAIC/AhUQ9erVyxVXK2hRV1nbtm1t1KhRkWxOXl5eTPW5AqC77rorMq0BHXU5+eSTbfTo0ZFlfve739muXbtct9yJJ57oxjYK0qH3B48y83tLAABIT6EwHaLlpqLq6MPxk0EB3llnNbd168xefnmTnXpqcou2cbCdmzdvbhs2bKAGoJLR1qlBO6cObV1121n1wOUtqg5MDVE6O1hDxBgXAAD4gYAoACiqBgDAXwREAUBABACAvwiIAoCACAAAfxEQBUD0ucwAAEDqERAFABkiAAD8RUAUAAzMCACAvwiIAoAMEQAA/iIgCgBGqgYAwF8ERAFAhggAAH8REAUAR5kBAOAvAqIAIEMEAIC/CIgCgIAIAAB/ERAFAAERAAD+IiAKAI4yAwDAXwREgcoQUVQNAIAfCIgCgJGqAQDwFwFRAFBDBACAvwiIAoAaIgAA/EVAFABkiAAA8BcBUQAwUjUAAP4iIAoAMkQAAPiLgCgACIgAAPAXAVEAEBABAOAvAqIA4CgzAAD8RUAUAIxUDQCAvwiIAoCRqgEA8BcBUQDQZQYAgL8IiAKAomoAAPxFQBQABEQAAPiLgMhHNSZPtkbnnmsZSxe7aYqqAQDwBwGRjzJ27LDsL7+07H273HRhod9bBABAesqygJk3b57NnDnTtm/fbm3atLFhw4ZZx44dEy67Zs0amzJlin311Ve2efNmu/766+1b3/rWMa0zlcI5Oe66WrjAXe/fT4YIAABL9wzRwoULbdKkSXbFFVfYuHHjXPAyZswY27FjR8LlCwoKrGnTpjZ06FCrV69eUtbpS0BkBEQAAPgpUAHRrFmzrF+/fta3b19r1aqVjRgxwnJycmzBggUJl1eW5zvf+Y717t3bsrOzk7LOVApXq+auq4X3uev9+33eIAAA0lRgusyKiops5cqVNmjQoMi8jIwM69q1qy1btiyl6ywsLHQXTygUsurVq0duJ80hAVEouetHhNeutG/lo61Tg3ZOHdo6Pdo5MAHRzp07raSk5JCuL02vX78+peucPn26TZs2LTLdrl07193WuHFjS6qmTd1VTklpQJSVVcuaN6+V3P+BGM2aNfN7E9IGbZ0atHPq0NbHdzsHJiAKksGDB9vAgQMj0160qsJtZZ2SpVp+vjXQdcleN719+x7bsMH/2qbjkV5Dvck2btxoYYYEr1S0dWrQzqlDW1fdds7Kyip3MiMwAVGdOnVcd5aOBIum6bIKpitrnapHKqsmKZlvhkhRdcked11QkNz141BqX9o4NWjr1KCdU4e2Pr7bOTBF1Yri2rdvb0uWLInMU3eXpjt37hyYdSZTJCAqLs0QcZQZAAD+CEyGSNRNNXHiRBfE6AiyOXPmuEPr+/Tp4+6fMGGCNWjQwB1mL+q+Wrt2beT21q1bbdWqVZabmxvpgzzSOoMREJVmiDjKDAAAfwQqIOrVq5crhJ46darr1mrbtq2NGjUq0r2Vl5cXU32uAOiuu+6KTGvwRV1OPvlkGz16dLnWGYjD7ovy3TUZIgAA/BGogEj69+/vLol4QY6nSZMmLtA5lnX66kCGKKfIqyEiIAIAwA+BqSFKR5EuswMZIs5lBgCAPwiIghAQFe5213SZAQDgDwIiP3k1RAcOuycgAgDAHwREQSiqPnByV2qIAADwBwFRALrMcqz0eHsOuwcAwB8ERH46MBq2lyGiywwAAH8QEPkpFHLdZgREAAD4i4AoAN1mBwMiv7cGAID0REAUqICIDBEAAH4gIPJbVEBUVBSykhK/NwgAgPRDQOQz1RB5R5lJQWlsBAAAUoiAKEBdZkK3GQAAqUdA5LecnJgMEQERAACpR0AUgC4zhUA5WcVumiPNAABIPQKioIxWHQmIyBABAJBqBER+8854n0lABACAXwiIgpIhyixy1wREAACkHgFRQM547wVEHHYPAEDqERAFpsuMDBEAAH4hIApIl1m1jEJ3XVhIQAQAQKoREAWly+xAQESXGQAAqUdAFJQMUag0IKLLDACA1CMg8pt3lBkBEQAAviEgCkyGqHSIakaqBgAg9QiIAlJDVC1UWjxUUECGCACAVCMg8puXITpwgleOMgMAIPUIiILSZWalGSJqiAAASD0CoqB0mYX3uWsOuwcAIPUIiIJylFmYDBEAAH4hIApKl9mBDBEBEQAAqUdAFLiAyOcNAgAgDREQ+c2rISrZ66457B4AgNTLsgCaN2+ezZw507Zv325t2rSxYcOGWceOHctcftGiRTZlyhTbvHmzNWvWzK699lo744wzIvdPnDjRXnvttZjHdO/e3e69914LTIboQEDEYfcAAKRe4AKihQsX2qRJk2zEiBHWqVMnmz17to0ZM8bGjx9vdevWPWT5L774wn73u9/Z0KFDXRD05ptv2sMPP2zjxo2z1q1bR5Y77bTTbOTIkZHprKxgPHUvIMopLg2I6DIDACD1AtdlNmvWLOvXr5/17dvXWrVq5QKjnJwcW7BgQcLl58yZ44KdSy+91C0/ZMgQa9++vcsyRVMAVK9evcilVq1aFqQus9ziPe6aomoAAFIvGGmSA4qKimzlypU2aNCgyLyMjAzr2rWrLVu2LOFjNH/gwIGHdIe9++67MfOWLl1qw4cPt5o1a9qpp57qAqfatWsnXGdhYaG7eEKhkFWvXj1yu1JqiKICoqT/D0TalLatfLR1atDOqUNbp0c7Byog2rlzp5WUlLgMTjRNr1+/PuFjVGcU35Wmac33KIPUs2dPa9KkiW3cuNGeffZZGzt2rOuKU8AVb/r06TZt2rTIdLt27VwXXOPGjS3pdu1yV9UPBERmuda8efPk/x84qjFDatDWqUE7pw5tfXy3c6ACosrSu3fvyG3VFalQ+/bbb7dPP/3UZZ/iDR48OCbr5EWrKtpWFiuZMnfssCaqISrKd9O7dhXYhg1bk/o/UPoa6k2mgDgcDvu9Occ12jo1aOfUoa2rbjurXKa8yYxABUR16tRxGZvo7I5oOj5r5NH8HTt2xMzTdFnLS9OmTV13mRo9UUCUnZ3tLokk+83gFVXnFu2OdJnxhqs8alvaNzVo69SgnVOHtj6+2zlQRdWK5FQQvWTJksg8daFpunPnzgkfo/mLFy+OmffJJ5+4I9TKsmXLFtu9e7fVr1/fAnOU2YGz3VNUDQCApXdAJOqqmj9/vr366qu2du1ae/LJJ62goMD69Onj7p8wYYI988wzkeUHDBhgH3/8sRu3aN26dTZ16lRbsWKF9e/f392/b98++9vf/uaKrzdt2uSCp9/85jcuLafi6+Cd7d7nDQIAIA0FqstMevXq5YqrFdioq6xt27Y2atSoSBdYXl5eTAV6ly5d7I477rDnnnvOFUurIPnOO++MjEGkLrjVq1e7gRnz8/OtQYMG1q1bN7v66qvL7BZLKe8os0hARIYIAIBUC4XpEC03FVVHH46fDArump9wgr1f3N3OtPetWbNie//9r5P6P3CgnZs3tw0bNlADUMlo69SgnVOHtq667azER3mLqgPXZZaWqlWjywwAAB8REAUsIOJcZgAApB4BURBUq8ZRZgAA+IiAKGAZooICjUPk9wYBAJBeCIgCFhBJkuu2AQDAERAQBUFubkxARLcZAACpRUAUwAwRAREAAKlFQBQE1apZppVYZkaJmyw4GBsBAIAUICAKggOjVedkFrtrDr0HACC1CIiCFBBllQZEdJkBAJBaBERB4J3PLLPIXdNlBgBAahEQBUHNmu4qJ6M0ICJDBABAahEQBUGTJu6qZsZed71tGy8LAACpxDdvEDRt6q6+UecLd/322zk+bxAAAOmFgChAAdF51Re66zffLK0pAgAAqUFAFKAus34l/3DXH3+cbdu3U0cEAECqEBAFKEN0ws7PrGPHQguHQ7ZoEVkiAABShYAoQAFRRl6effPs0mPu6TYDACB1CIiCoHFjdxUqLrZvnrbF3X7ttWpWXDpOIwAAqGQEREGQk2Ml9eq5m2d3WGPVqoXtq6+y7Hvfa2A7d1JLBABAZSMgCoiSRo3cdf29G+z3v99mublh+9e/cu3b325kW7cSFAEAUJkIiAKi+EC3meqIBg7cZ9On51mTJsW2dGm2DR3a0HbsICgCAKCyEBAFLEOUmZfnrrt1K7QpU7ZYw4bFtnhxjl17bUPbtYugCACAykBAFLCAKGPz5si8zp2L7Lnntli9eiX24Yc59t3vNrD8fIIiAACSjYAoIEqiusyinXxyaVBUp06JvfNONbv++ga2Zw9BEQAAyURAFBDFcV1m0bp2LbSnn95itWuXuAEbFRTt3UtQBABAshAQBa3LLEFAJGecUWiTJ2+xmjVLbOHCau6Q/L17U7yRAAAcpwiIgtZlFlVDFO/MM5Up2uqCIo1kPWxYAysoHdgaAAAcAwKiIGaIwuEylzvrrP02efJWq1GjxF5/PdfuuKM+I1oDAHCMCIiCliHat89C+fmHXbZHj/32pz9ttezssM2aVd3uvruuFRWlaEMBADgOERAFRLhGDSupUeOI3Waec87Z70a0DoXC9swzNe266xrali28nAAAHA2+QQOk5MBZ7xvcdJPlzp5tWcuX2+GKhC69dJ898cQ21332xhvVrEePpvbjH9ezl1/OtW3bOAoNAIDyyrIAmjdvns2cOdO2b99ubdq0sWHDhlnHjh3LXH7RokU2ZcoU27x5szVr1syuvfZaO+OMMyL3h8Nhmzp1qs2fP9/y8/PtxBNPtOHDh1vz5s0tSHbefbfV+9nPLHvpUhcUSTgUsuIWLay4TRsratu29LpNm8j1xRebvfhinguElizJsSlTariLNGhQbG3a6FJkTZuWWN26pZfatcOWkVFaqlRSYq4GqagoZIWFZvv36zpk+/ebu1ZXnDfv4P2l9x28DllGRtgyM82tV5fMzNL/oXne7cTT0ctp3pEfp+0+eAm55+A9l7Lu0+2aNc127qzt9oeSklBkufjHxd8n3rW3Pd62h0Ledh9+Ovq56ZKVFbasrOjr0uWzs0uv4+/zbldkGf0fpA+9H/X6a58DUHGhsL4dAmThwoU2YcIEGzFihHXq1Mlmz55t//73v238+PFWt27dQ5b/4osv7IEHHrChQ4e6IOjNN9+0GTNm2Lhx46x169ZumRdffNFdbr31VmvSpIkLnlavXm2PPvqo5eTklHvbFHAV6lMniUKhkAvMNmzY4L6oM7ZutVrjx1u1f//bMletsowj1BOV1KvnAqOi1m3srZxz7dl159mbq9rYlxvrJ3U7UTVFB5jqXo0OMPXFGXt/ounSeV6Q5y0TO30wWPWma9SoZoWF+xLeHx0cRgeQ0QFj4unkLOMFqt7z1P2x0wfbKnraC2y9towOdONvR687/j79L+//ld4++P9j58cGNxp7TJnf7dszbPPmTFu5Msu+/rquLV68z1asyLK1azOtefNidy7ESy7Za6efXmgrV2ba+vWZ1qVLkTVpciCqLyfvB4F+FBUXl/440o+nxLdDbvs0kr4uu3dn2NatGbZtmy4hd3vHjgx30mqNvN+6dbF16FDkLtrmatXCVq1a4iA+9sdK7G2zQ38QRf8Iin9c/H3adv0YbNiwxOrXL4m0t5bLy8uIXLZuzbSGDevbvn1bLTe3xGrUCFv16trmg/vWwdcunPA1TDRtFj7s/Ymm9Zj4+4/8mOTZt8/cvrZiRba9+26OffRRti1bluV+LKu+9fzz99m3v73X7e86F+cJJxSXe9+L/z5MhuzsbGt8oEa3ygVEo0aNsg4dOtiNN97opktKSuyWW26xiy++2AYNGnTI8v/zP/9jBQUFdvfdd0fm3XvvvS6zdNNNN7lGvfnmm23gwIF26aWXuvv37NnjAq6RI0da7969AxUQxVCAtGWLZX71lWX95z+WtWqVZer6P/+xzNWrLfMwtUa7rJattPa2wjq4yyZrYtutnm2z+rbD6pqF9C4OlX4hZYQtJ7PYsjOLLCezxN1WtiEns8iyM4rddE6G7iu2rAP3Z2dpfollH1i2JJRlxaEsK7EMd10cyrTisC4ZpfN0OTAdmafblmElkeVCkfsjjw2XziuJzM+IfCmL+zKzuA8j70NA86K+bHKr5dj+wv2ReaVfRAfuD0Wtx60jXPr4A+v3/p+2Rxmk0m3S/ultY6j0uiTDfdhGTxeXlN4uOfAYzSs6ML+wONOKtL6SDCssLp1fFHMdipvWdeiQaa0bxydvH9V+VxE1s/ZZflHuwens/VazRpFlZB7YH91+V3op3VdDVqIgx9tfK/j/kkGfL3quLniJBDSp2Y4aOYXWqu4Oq5FVYF/mNbL8wmp2vO5LofigKsF0KHTo4wr2Z7rPqSOpnZlvhSVZti9c2oat6261atUzLJyV7f6TPlUPvsal/13TNwzdbg8+0sy3gChQXWZFRUW2cuXKmMAnIyPDunbtasuWLUv4GM1XsBOte/fu9u6777rbmzZtcl1v3bp1i9xfo0YN1wWnxyYKiBT0RAc+ClqqV68euZ1M3voSrjcUsnDjxlakS48eh96dn18aGK1aZVkHrjPXrbOMnTstd/duO2X3djt192uWsXu2hdQHFk37GofrHzcUTBZZlrsUWra7KOgsDUYzI7ePNF1Zy1b1/6Pb0ZdE8w53X/gYyjW9bIbo1W1gW62R5VlHW25d7AvrbMvcdTv7yt63b9hUu8peskttd1Fty7ECO8HWuB9H+YU5lr+j/Bnxw8k8sLfp2ZXudUVWw/ZYLdttNS3fatsua2hb3HbqWpf6ts0KrJpttsa23DraF26ru9h2O5jNVpBfGUIHXkl99XrXuuidouegbdizP9uWbW4U8xhtfxPbZI2t9MfnHvcsa1i+1XTX+yzXrcnbT6Jve9OlP6n0kev/j5aD+9KxfY/VtN1u/+th71gvW2hdbbGb/0873/5kN9qXxZ3ddEPLs63WwFbvaGC248jr3fPaJ2bWLOnfs+UVqIBo586dLiNUr169mPmaXr9+fcLHKNiJ70rTtOZ793vzylom3vTp023atGmR6Xbt2rkuuPJGmUdDtU9H5TC1VTGU31ZQpCLt8l6UT/aKjA53KWuZQ4t6Dr2UZ5nyLlfeZaJF/wqp6O0APV4ftTnhsOUkdVt0KapybRHEx2vSBUfhA1+S3u4Y9SUamfZ+OR/4MvVu1wqVBhqhnGwzdfXrkh19u6O1qhO2yxr8w/bW+cQ+L+5kXbqY1cgtsfzP/2obPttu+as2W0n+XsssKbSMkiJ37V2UFY0O4zTtBTuZoZLIbS+gKDc9ofg+wQO3w6EMKwzlWEEo1wVLuoRLwpZRtN9CxUUWUka3uLD0th6q/+2yuLoOHZwu49q1nFLGEt2PpGIr/cg9cNmXU8fWhk6w/5ScYDsz69tJTbdax3p5lrV3l9muXWa7d8de9uyJfY6x6ZTD3na7hp57JCwrbQd334HbB4OoY1gmJhg7+Bg3L3RgH/Ruxz8+bAn/X7XQfquducfqZuw6+NS8G9Wq2TeafWp3tv69vVOzr9VuXstObrHdti9db0vezreS/6yx0OZNFgqXWKikuPTi3T6wBS3OucTMLjz678PjKSAKisGDB8dknbxoVV1mymIlk9atF3/jxo1JSxGWiz5IdalVy9KBb+2chmjr5NMnkKoJ8yvQzvpK0Y9y98P8Ev2qL72Uh5dA1iWVg+HH51BSmchW25wcNb35ON2nI3HMUT5+74HL4bQ5cL1Rf84163hLObftwHdtMts5KyuranaZ1alTx3WRxWduNB2fNfJo/o4dsbk4TXvLe9eaV7/+wdSsptu2bVtmn6MuiVTWm0HrrepvtKqAdk4d2jo1aOfUoa2P73b2v1MzLpJr3769LVmyJDJPXWia7ty5tE8ynuYvXlzaf+n55JNP3BFqoqPKFBRFL6Oi6uXLl5e5TgAAkF4CFRCJuqo0XtCrr75qa9eutSeffNIdRdanTx93vw7Jf+aZZyLLDxgwwD7++GM3btG6devceEMrVqyw/v37R1JwWuaFF16w9957zx1ur3UoW3TWWWf59jwBAEBwBKrLTHr16uWKqxXYqKtM3Vo6FN/r+srLy4upQO/SpYvdcccd9txzz9mzzz7rDmG/8847I2MQyWWXXeaCqscff9xlhzQwo9ZZkTGIAADA8Stw4xAFWcrHIULS0M6pQ1unBu2cOrR1avg9MGPguswAAABSjYAIAACkPQIiAACQ9giIAABA2iMgAgAAaY+ACAAApD0CIgAAkPYIiAAAQNojIAIAAGkvcKfuCDKdfLYqrhsH0c6pQ1unBu2cOrR11WvniqyLU3cAAIC0R5eZz/bu3Ws/+9nP3DUqD+2cOrR1atDOqUNbp0c7ExD5TAm6r776ihMGVjLaOXVo69SgnVOHtk6PdiYgAgAAaY+ACAAApD0CIp9lZ2fbFVdc4a5ReWjn1KGtU4N2Th3aOj3amaPMAABA2iNDBAAA0h4BEQAASHsERAAAIO0REAEAgLTHiVl8NG/ePJs5c6Zt377d2rRpY8OGDbOOHTv6vVlV1tSpU23atGkx81q0aGHjx493t/fv32+TJk2yhQsXWmFhoXXv3t2GDx9u9erV82mLq46lS5faSy+95AZN27Ztm/30pz+1Hj16RO7XsRlq//nz51t+fr6deOKJrm2bN28eWWb37t325z//2d5//30LhULWs2dPu+GGGyw3N9enZ1U123rixIn22muvxTxG+/K9994bmaatj2z69On2zjvv2Lp16ywnJ8c6d+5s1113nfvM8JTnMyMvL8+eeOIJ+/TTT137nnvuuTZ06FDLzMz06ZlVvXYePXq02++jnX/++XbTTTeltJ0JiHyiN5jeaCNGjLBOnTrZ7NmzbcyYMe7Lu27dun5vXpV1wgkn2P333x+Zzsg4mAT961//ah988IH9+Mc/tho1atif/vQne+SRR+yXv/ylT1tbdRQUFFjbtm3tvPPOs9/+9reH3D9jxgybO3eu3XrrrdakSRObMmWK258fffRR9yEov//9790X/H333WfFxcX22GOP2eOPP24/+MEPfHhGVbet5bTTTrORI0eWeQJL2vrI9AV80UUXWYcOHVwbPfvss/bQQw+5fdYLHI/0mVFSUmK/+tWvXICkx6rNJ0yY4L6k9WUNK1c7S79+/ezqq6+OTHufGyltZx12j9S75557wk8++WRkuri4OHzTTTeFp0+f7ut2VWVTpkwJ//SnP014X35+fnjIkCHhRYsWReatXbs2fOWVV4a/+OKLFG5l1ac2e/vttyPTJSUl4REjRoRnzJgR095Dhw4Nv/nmm256zZo17nHLly+PLPPhhx+Gr7rqqvCWLVtS/AyqblvLhAkTwuPGjSvzMbT10dmxY4drt08//bTcnxkffPCBa9dt27ZFlnn55ZfD3/3ud8OFhYU+PIuq187ywAMPhJ966qlwWVLVztQQ+aCoqMhWrlxpXbt2jclkaHrZsmW+bltVt3HjRrv55pvttttuc7+SlWYVtbd+nUS3ecuWLa1Ro0a0+THatGmT6/bt1q1bZJ5+Tav712tbXdesWdP9SvTotVB3zvLly33Z7qr+q1tdN8r4qBth165dkfto66OzZ88ed12rVq1yf2bounXr1jFdaMre6eSka9asSflzqIrt7HnjjTfsxhtvtJ/85Cf2zDPPuEypJ1XtTJeZD3bu3OlSgPG1K5pev369b9tV1anrUd0I6ptWSlX1RD//+c9diltf2OpW0BdFNHVP6j4cPa/94rt6o9tW13Xq1Im5X+lufSjS/hWjLwLVBKlrUj8A1AUxduxY10WpH1a0dcXp8/gvf/mLdenSxX3xSnk+M3Qd/znuvQ9o6/K1s5x99tku0GzQoIH95z//saefftp9F6p+LpXtTECE48bpp58eua0idS9AWrRoUUx/NFCV9e7dO3JbXyra12+//XZXbBqdzUD5qTZImYYHH3zQ701Jy3Y+//zzY/bp+vXru2UU8Ddr1ixl20eXmQ/06837JRctURSMo6dfdsoW6U2ldlVXpY6AirZjxw7a/Bh57ae2LKttda3MaDR1R+hoKNr/2DRt2tRq167t9nOhrSv+Ja3C6QceeMAaNmwYmV+ezwxdx3+Oe+8D2rp87ZyId7R19D6dinYmIPKB0rDt27e3JUuWxKQSNa1DEpEc+/btiwRDam91GyxevDhyv1KyqjGizY+Num7UxtFtqzoB1at4batrfbGoLsOj/V2H6zPUxLHZsmWLC3b0q1po6/JRe+hLWoeEq2td+3G08nxm6Hr16tUxPwY++eQTq169urVq1SqFz6bqtnMiq1atctfR+3Qq2pkuM58MHDjQjSeiN50+pObMmeOKyPr06eP3plVZGsbgzDPPdH3RqiHSuDjKxKl/WkW+OoxZy6iWQtMap0VvNAKi8geX0YXU+tBSW6q9BwwYYC+88IIbd0gfeM8995z7MDvrrLPc8vrQUu2LDv3WUBP65a3279Wrl6sbQPnaWpfnn3/e1RApCP36669t8uTJrltBY+QIbV0++pJ+88037a677nJfrF4GQp8N6mIvz2eG2lztrUPAr732WrcO7fs6zNyvM7ZXtXbeuHGju/+MM85w7azAR8MdnHTSSa47OJXtzNnufR6YUQOw6cXVuCMaOE11Lzg6GsPps88+c0fcqFtSgwMOGTIk0gftDbL21ltvuS8JBmYsP9Wn/OIXvzhkvgZH09hD3sCM//znP112SG2vI0aiB19TFkMfjtGDBWowUgYLLH9bK8B5+OGH3aCNygIpwNHRfRq/JXo/pq2P7Kqrrko4X3WH3g/T8nxmbN682Z588kn3ulWrVs29TvrSZmDG8rWzMm5/+MMfXG2RkgLqTtNApJdffrkLmlLZzgREAAAg7VFDBAAA0h4BEQAASHsERAAAIO0REAEAgLRHQAQAANIeAREAAEh7BEQAACDtERABQAW8+uqrbrC5FStW+L0pAJKIU3cACGTQ8dhjj5V5/0MPPcQpVwAkFQERgMBSJibRySC907EAQLIQEAEIrNNPP906dOjg92YASAMERACqJJ0F/rbbbrPrrrvOMjIybM6cObZjxw7r2LGjO7Fs69atY5ZfsmSJOwGtToyqE0KefPLJNnToUHcW7Whbt261KVOm2EcffeROFFy/fn139nidfDkr6+BHZmFhoTsr9+uvv+5OAqqTrN58883uxMIe1RnprNwrV650Z7HXSUFPOeUUd2JLAMFCQAQgsPbs2WM7d+6Mmaezt9euXTsyrYBk7969dtFFF7kgRYHRgw8+aL/97W8jZyX/5JNP7Fe/+pXrfrvyyitdADN37ly7//77bdy4cZFuOQVD99xzj/u//fr1s5YtW7p5//73v92ZuKMDoqeeespq1qzp1qfgTP9XZ5j/0Y9+5O5XcKZaJwVIl112mVtWZ+x+++23U9R6ACqCgAhAYP3yl788ZF52drY9/fTTkemNGzfa73//e2vQoIGbVjZn1KhRNmPGDLv++uvdvMmTJ1utWrVszJgx7lrOOussu+uuu1zWSJkmeeaZZ2z79u02duzYmK66q6++2sLhcMx2aD333XefC9BE9yvIUjBVo0YN++KLLyw/P98tE72uIUOGJLmVACQDARGAwFLXV/PmzWPmqXssmgIbLxgSdZl16tTJPvzwQxcQbdu2zVatWmWXXnppJBiSNm3auG4uLSclJSX27rvv2je+8Y2EdUte4OM5//zzY+addNJJNnv2bJcF0rqVEZL333/fTUdnlwAED+9QAIGl4OZIRdXxAZM3b9GiRe62AhRp0aLFIcupS+zjjz929T26qOstvvaoLI0aNYqZ9gIgZYVENUo9e/a0adOmuUBJtUMK3s4++2yX5QIQLAzMCABHIT5T5fG61pQ9+slPfuLqiPr37+9qkf74xz/a3Xff7YIvAMFCQASgStuwYUPCeY0bN3a3vev169cfspzmqUA7NzfXFT9Xr17dVq9endTt0wCS11xzjf3617+2O+64w9asWWNvvfVWUv8HgGNHQASgSlPdj7IvnuXLl9uXX37piqtFh823bdvWXnvttUh3lijwUXeZxjryMj7q0lLNT6LTcsQXVR/J7t27D3mMtkN0NByAYKGGCEBgqeB53bp1h8zv0qVLpKBZo1br8PkLL7wwcti9sj461N2jsYp02L2O+Orbt6877H7evHnuaDCNhu3RuEQ6RH/06NHusHuNUaSibB12r0P5vTqh8lAA9sorr7ggS9uo+qT58+e7LNQZZ5xxzG0DILkIiAAElg6JT0QDG6poWc455xyX3VHhssYsUiH2sGHDXGbIo6PJdCi+1qeLNzDjtddeG3NqEB2tpkPuNZjim2++6YIYzVO2qVq1ahXadq1f2aqFCxe6MYkUfKlAXN1miU5HAsBfoXBF88AAELCRqnVIPQAcC2qIAABA2iMgAgAAaY+ACAAApD1qiAAAQNojQwQAANIeAREAAEh7BEQAACDtERABAIC0R0AEAADSHgERAABIewREAAAg7REQAQCAtEdABAAALN39PyrOUijjwcF1AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_error(train_error, val_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2. ƒê√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step\n"
     ]
    }
   ],
   "source": [
    "mse, rmse, mape, r2, true, predicted = evaluate_model_2(model, test, timesteps) #ƒë∆∞·ª£c t√≠nh d·ª±a tr√™n b·ªô d·ªØ li·ªáu ƒë√£ chu·∫©n ho√°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.000736605125155906\n",
      "RMSE = 0.027140470245666452\n",
      "MAPE = 0.020181852416107097\n",
      "R-Squared Score = 0.9444313668653268\n"
     ]
    }
   ],
   "source": [
    "print('MSE = {}'.format(mse))\n",
    "print('RMSE = {}'.format(rmse))\n",
    "print('MAPE = {}'.format(mape))\n",
    "print('R-Squared Score = {}'.format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3. V·∫Ω ƒë·ªì th·ªã d·ª± ƒëo√°n vs th·ª±c t·∫ø"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data_2(true, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a model\n",
    "model.save('10VAR-VN30index-rnn.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Multivariate-3-GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0' # ƒë·∫£m b·∫£o r·∫±ng c√°c gi√° tr·ªã bƒÉm c·ªßa ƒë·ªëi t∆∞·ª£ng b·∫•t bi·∫øn (dict, set, chu·ªói, tuple...) lu√¥n gi·ªëng nhau gi·ªØa c√°c l·∫ßn ch·∫°y\n",
    "\n",
    "import random as rn\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "rn.seed(3)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, GRU, Dropout\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from math import sqrt\n",
    "import datetime as dt\n",
    "import time\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H√†m callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=80,  verbose=1, mode='min')  \n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=\"10Var-Vn30-gru.h5\",   # T√™n file l∆∞u m√¥ h√¨nh\n",
    "    monitor=\"val_loss\",         # Theo d√µi val_loss\n",
    "    save_best_only=True,        # Ch·ªâ l∆∞u khi t·ªët h∆°n m√¥ h√¨nh tr∆∞·ªõc ƒë√≥\n",
    "    mode=\"min\",                 # Gi·∫£m min c·ªßa val_loss l√† t·ªët nh·∫•t\n",
    "    verbose=1\n",
    ")\n",
    "callbacks_list = [earlystop, checkpoint] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ƒê·ªçc d·ªØ li·ªáu t·ª´ file csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = r\"VN30 Index.csv\"\n",
    "df = pd.read_csv(url, parse_dates= True, index_col= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009-01-05</th>\n",
       "      <td>311.23</td>\n",
       "      <td>311.23</td>\n",
       "      <td>311.23</td>\n",
       "      <td>311.23</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-06</th>\n",
       "      <td>314.21</td>\n",
       "      <td>314.21</td>\n",
       "      <td>314.21</td>\n",
       "      <td>314.21</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-07</th>\n",
       "      <td>320.53</td>\n",
       "      <td>320.53</td>\n",
       "      <td>320.53</td>\n",
       "      <td>320.53</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-08</th>\n",
       "      <td>314.14</td>\n",
       "      <td>314.14</td>\n",
       "      <td>314.14</td>\n",
       "      <td>314.14</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-09</th>\n",
       "      <td>312.90</td>\n",
       "      <td>312.90</td>\n",
       "      <td>312.90</td>\n",
       "      <td>312.90</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              open    high     low   close  volume\n",
       "time                                              \n",
       "2009-01-05  311.23  311.23  311.23  311.23     NaN\n",
       "2009-01-06  314.21  314.21  314.21  314.21     NaN\n",
       "2009-01-07  320.53  320.53  320.53  320.53     NaN\n",
       "2009-01-08  314.14  314.14  314.14  314.14     NaN\n",
       "2009-01-09  312.90  312.90  312.90  312.90     NaN"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "open        0\n",
       "high        0\n",
       "low         0\n",
       "close       0\n",
       "volume    859\n",
       "dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ti·ªÅn x·ª≠ l√≠ d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X√≥a c√°c d√≤ng c√≥ gi√° tr·ªã Volume b·∫±ng 0\n",
    "df.drop(df[df['volume']==0].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "open      0.999562\n",
       "high      0.999783\n",
       "low       0.999789\n",
       "close     1.000000\n",
       "volume    0.169492\n",
       "Name: close, dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ma tr·∫≠n t∆∞∆°ng quan (·ªü ƒë√¢y l√† Pearson t∆∞∆°ng quan tuy·∫øn t√≠nh)\n",
    "df.corr()['close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    3.185000e+03\n",
      "mean     3.279171e+06\n",
      "std      2.776577e+07\n",
      "min      4.540000e+03\n",
      "25%      3.809000e+04\n",
      "50%      6.036000e+04\n",
      "75%      1.560100e+05\n",
      "max      3.744900e+08\n",
      "Name: volume, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df.describe().volume) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['volume'].hist(bins= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B·ªï sung c√°c ch·ªâ b√°o kƒ© thu·∫≠t\n",
    "\n",
    "# T√≠nh CMA10\n",
    "df['CMA10'] = df['close'].rolling(window=10, center=True).mean()\n",
    "# T√≠nh SMA10\n",
    "df['SMA10'] = df['close'].rolling(window=10).mean()\n",
    "# T√≠nh SMA50\n",
    "df['SMA50'] = df['close'].rolling(window=50).mean()\n",
    "# T√≠nh EMA12 v√† EMA26\n",
    "df['EMA12'] = df['close'].ewm(span=12, adjust=False).mean()\n",
    "df['EMA26'] = df['close'].ewm(span=26, adjust=False).mean()\n",
    "# T√≠nh MACD\n",
    "df['MACD'] = df['EMA12'] - df['EMA26']\n",
    "#T√≠nh RSI\n",
    "# T√≠nh gi√° tƒÉng/gi·∫£m\n",
    "delta = df['close'].diff()\n",
    "\n",
    "# T√≠nh gi√° tƒÉng\n",
    "gain = delta.where(delta > 0, 0)\n",
    "\n",
    "# T√≠nh gi√° gi·∫£m\n",
    "loss = -delta.where(delta < 0, 0)\n",
    "\n",
    "# T√≠nh trung b√¨nh ƒë·ªông\n",
    "avg_gain = gain.rolling(window=14).mean()\n",
    "avg_loss = loss.rolling(window=14).mean()\n",
    "\n",
    "# T√≠nh RS v√† RSI\n",
    "rs = avg_gain / avg_loss\n",
    "df['RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "#T√≠nh CCI\n",
    "# T√≠nh gi√° trung b√¨nh\n",
    "typical_price = (df['high'] + df['low'] + df['close']) / 3\n",
    "\n",
    "# T√≠nh SMA c·ªßa gi√° trung b√¨nh\n",
    "sma_typical_price = typical_price.rolling(window=20).mean()\n",
    "\n",
    "# T√≠nh ƒë·ªô l·ªách chu·∫©n\n",
    "mean_deviation = typical_price.rolling(window=20).apply(lambda x: np.mean(np.abs(x - x.mean())))\n",
    "\n",
    "# T√≠nh CCI\n",
    "df['CCI'] = (typical_price - sma_typical_price) / (0.015 * mean_deviation)\n",
    "# T√≠nh %K v√† %D\n",
    "low_min = df['low'].rolling(window=14).min()\n",
    "high_max = df['high'].rolling(window=14).max()\n",
    "\n",
    "df['%K'] = 100 * (df['close'] - low_min) / (high_max - low_min)\n",
    "df['%D'] = df['%K'].rolling(window=3).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              open    high     low   close  volume  CMA10  SMA10  SMA50  \\\n",
      "time                                                                      \n",
      "2009-01-05  311.23  311.23  311.23  311.23     NaN    NaN    NaN    NaN   \n",
      "2009-01-06  314.21  314.21  314.21  314.21     NaN    NaN    NaN    NaN   \n",
      "2009-01-07  320.53  320.53  320.53  320.53     NaN    NaN    NaN    NaN   \n",
      "2009-01-08  314.14  314.14  314.14  314.14     NaN    NaN    NaN    NaN   \n",
      "2009-01-09  312.90  312.90  312.90  312.90     NaN    NaN    NaN    NaN   \n",
      "\n",
      "                 EMA12       EMA26      MACD  RSI  CCI  %K  %D  \n",
      "time                                                            \n",
      "2009-01-05  311.230000  311.230000  0.000000  NaN  NaN NaN NaN  \n",
      "2009-01-06  311.688462  311.450741  0.237721  NaN  NaN NaN NaN  \n",
      "2009-01-07  313.048698  312.123278  0.925420  NaN  NaN NaN NaN  \n",
      "2009-01-08  313.216591  312.272665  0.943926  NaN  NaN NaN NaN  \n",
      "2009-01-09  313.167885  312.319134  0.848750  NaN  NaN NaN NaN  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4044, 15)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.head())\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model / H√†m **fit_model_3()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_3(train, val, timesteps, hl, lr, batch, epochs):\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    tf.random.set_seed(2)\n",
    "    rn.seed(3)\n",
    "    \"\"\"\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    X_val = []\n",
    "    Y_val = []\n",
    "\n",
    "    for i in range(timesteps, train.shape[0]):\n",
    "        X_train.append(train[i-timesteps:i])\n",
    "        Y_train.append(train[i][0])\n",
    "    X_train, Y_train = np.array(X_train), np.array(Y_train)\n",
    "\n",
    "    for i in range(timesteps, val.shape[0]):\n",
    "        X_val.append(val[i-timesteps:i])\n",
    "        Y_val.append(val[i][0])\n",
    "    X_val, Y_val = np.array(X_val), np.array(Y_val)\n",
    "\n",
    "    # Th√™m c√°c l·ªõp v√†o m√¥ h√¨nh\n",
    "    model = Sequential()\n",
    "    model.add(GRU(X_train.shape[2], input_shape= (X_train.shape[1], X_train.shape[2]), activation= 'relu', return_sequences= True))\n",
    "    for i in range(len(hl)-1):\n",
    "        model.add(GRU(hl[i], activation='relu', return_sequences= True))\n",
    "    model.add(GRU(hl[-1], activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Bi√™n d·ªãch\n",
    "    model.compile(optimizer= optimizers.Adam(learning_rate= lr), loss= 'mean_squared_error')\n",
    "\n",
    "    # Hu·∫•n luy·ªán d·ªØ li·ªáu\n",
    "    history = model.fit(X_train, Y_train, epochs= epochs, batch_size= batch, validation_data= (X_val, Y_val), verbose= 0, shuffle= False, callbacks= callbacks_list)\n",
    "    \n",
    "    # ƒê·∫∑t l·∫°i tr·∫°ng th√°i\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, GRU):\n",
    "            layer.reset_states()\n",
    "    \n",
    "    return model, history.history['loss'], history.history['val_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H√†m **Evaluate_model_3()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_3(model, test, timesteps):\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    tf.random.set_seed(2)\n",
    "    \"\"\"\n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "\n",
    "    for i in range(timesteps, test.shape[0]):\n",
    "        X_test.append(test[i-timesteps:i])\n",
    "        Y_test.append(test[i][0])\n",
    "    X_test, Y_test= np.array(X_test), np.array(Y_test)\n",
    "\n",
    "    # C√°c ch·ªâ s·ªë ƒë√°nh gi√°\n",
    "    Y_hat = model.predict(X_test)\n",
    "    mse = mean_squared_error(Y_test, Y_hat)\n",
    "    rmse = sqrt(mse)\n",
    "    mape = mean_absolute_percentage_error(Y_test, Y_hat)\n",
    "    r2 = r2_score(Y_test, Y_hat)\n",
    "\n",
    "    return mse, rmse, mape, r2, Y_test, Y_hat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Grid Search**: T√¨m ki·∫øm si√™u tham s·ªë t·ªëi ∆∞u "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'timesteps': [30, 40, 50],\n",
    "    'hl': [[40, 35]],\n",
    "    'lr': [1e-4, 1e-3],\n",
    "    'batch_size': [32, 64],\n",
    "    'num_epochs': [200, 250]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H√†m grid search\n",
    "def grid_search_rnn(train, val, test, param_grid):\n",
    "    results = []\n",
    "    best_score = float('inf')\n",
    "    best_params = None\n",
    "\n",
    "    # T·∫°o t·∫•t c·∫£ c√°c t·ªï h·ª£p tham s·ªë\n",
    "    all_combinations = list(product(*(param_grid.values())))\n",
    "    param_names = param_grid.keys()\n",
    "\n",
    "    for combination in all_combinations:\n",
    "        # G√°n gi√° tr·ªã tham s·ªë hi·ªán t·∫°i\n",
    "        params = dict(zip(param_names, combination))\n",
    "        hl = params['hl']\n",
    "        lr = params['lr']\n",
    "        batch_size = params['batch_size']\n",
    "        num_epochs = params['num_epochs']\n",
    "\n",
    "        print(f'Training with params: {params}')\n",
    "\n",
    "        # Hu·∫•n luy·ªán m√¥ h√¨nh\n",
    "        model, train_loss, val_loss = fit_model_3(train, val, timesteps, hl, lr, batch_size, num_epochs)\n",
    "\n",
    "        # ƒê√°nh gi√° m√¥ h√¨nh\n",
    "        mse, rmse, mape, r2, _, _ = evaluate_model_3(model, test, timesteps)\n",
    "\n",
    "        # L∆∞u k·∫øt qu·∫£\n",
    "        results.append({\n",
    "            'timesteps': timesteps,\n",
    "            'hl': hl,\n",
    "            'lr': lr,\n",
    "            'batch_size': batch_size,\n",
    "            'num_epochs': num_epochs,\n",
    "            'mse': mse,\n",
    "            'rmse': rmse,\n",
    "            'mape': mape,\n",
    "            'r2': r2\n",
    "        })\n",
    "\n",
    "        # C·∫≠p nh·∫≠t tham s·ªë t·ªët nh·∫•t n·∫øu RMSE c·∫£i thi·ªán\n",
    "        if rmse < best_score:\n",
    "            best_score = rmse\n",
    "            best_params = params\n",
    "\n",
    "        # Tr·∫£ v·ªÅ k·∫øt qu·∫£\n",
    "        results_df = pd.DataFrame(results)\n",
    "        return best_params, best_score, results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot chart (v·∫Ω bi·ªÉu ƒë·ªì)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the predictions\n",
    "def plot_data_3(Y_test, Y_hat):\n",
    "    plt.plot(Y_test, c = 'r')\n",
    "    plt.plot(Y_hat, c = 'y')\n",
    "    plt.xlabel('Day')\n",
    "    plt.ylabel('Price')\n",
    "    plt.title(\"Stock Price Prediction using Multivariate-GRU\")\n",
    "    plt.legend(['Actual','Predicted'], loc = 'lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training errors: tr·ª±c quan loss qua c√°c epoch -> th·∫•y qtr h·ªçc m√¥ h√¨nh, xem c√≥ overfitting ko\n",
    "def plot_error(train_loss, val_loss):\n",
    "    plt.plot(train_loss, c = 'r')\n",
    "    plt.plot(val_loss, c = 'b')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.title('Train Loss and Validation Loss Curve')\n",
    "    plt.legend(['train', 'val'], loc = 'upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model building**: X√¢y d·ª±ng m√¥ h√¨nh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 1: Tr√≠ch xu·∫•t v√† tr·ª±c quan h√≥a d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3991, 10)\n",
      "              close     CMA10     SMA10      SMA50        EMA12        RSI  \\\n",
      "time                                                                         \n",
      "2025-03-10  1391.07  1381.379  1369.467  1336.1956  1368.821086  81.914388   \n",
      "2025-03-11  1393.57  1384.753  1372.768  1337.6066  1372.628611  80.831758   \n",
      "2025-03-12  1392.39  1386.820  1375.951  1338.9890  1375.668825  78.354464   \n",
      "2025-03-13  1387.30  1388.192  1378.319  1339.8854  1377.458236  71.932575   \n",
      "2025-03-14  1387.03  1388.125  1381.379  1340.7724  1378.930815  67.050447   \n",
      "\n",
      "                   CCI         %K         %D       MACD  \n",
      "time                                                     \n",
      "2025-03-10  186.248408  86.033606  94.302545  14.085059  \n",
      "2025-03-11  143.873911  88.007313  90.655860  15.015993  \n",
      "2025-03-12  152.955891  84.673892  86.238270  15.480105  \n",
      "2025-03-13  121.672693  73.658150  82.113118  15.261273  \n",
      "2025-03-14   97.327890  73.123391  77.151811  14.894368  \n"
     ]
    }
   ],
   "source": [
    "# Extracting the series\n",
    "series = df[['close', 'CMA10', 'SMA10', 'SMA50', 'EMA12', 'RSI', 'CCI', '%K', '%D', 'MACD']]\n",
    "# Drop rows with NaN values\n",
    "series = series.dropna()\n",
    "\n",
    "# Display the shape and the tail of the cleaned series\n",
    "print(series.shape)\n",
    "print(series.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 2: Chia d·ªØ li·ªáu th√†nh c√°c t·∫≠p Train, Validation, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3991, 10)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2795, 10) (598, 10) (598, 10)\n"
     ]
    }
   ],
   "source": [
    "n = series.shape[0]\n",
    "val_size =  test_size = int(n * 0.15)\n",
    "train_size = n - val_size - test_size # ƒê·ªÉ tr√°nh sai s·ªë l√†m m·∫•t d·ªØ li·ªáu\n",
    "\n",
    "# Chia t·∫≠p d·ªØ li·ªáu theo th·ª© t·ª± th·ªùi gian\n",
    "train_data = series.iloc[:train_size].values\n",
    "val_data = series.iloc[train_size:train_size + val_size].values\n",
    "test_data = series.iloc[(train_size + val_size):].values\n",
    "# Ki·ªÉm tra k√≠ch th∆∞·ªõc c·ªßa t·ª´ng t·∫≠p\n",
    "print(train_data.shape, val_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 3: Chu·∫©n h√≥a d·ªØ li·ªáu b·∫±ng MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2795, 10) (598, 10) (598, 10)\n"
     ]
    }
   ],
   "source": [
    "# Normalisation\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(2)\n",
    "\n",
    "sc = MinMaxScaler()\n",
    "train = sc.fit_transform(train_data)\n",
    "val = sc.transform(val_data)\n",
    "test = sc.transform(test_data)\n",
    "\n",
    "print(train.shape, val.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 4: T√¨m si√™u tham s·ªë t·ªët nh·∫•t b·∫±ng Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with params: {'timesteps': 30, 'hl': [40, 35], 'lr': 0.0001, 'batch_size': 32, 'num_epochs': 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.03914, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_loss improved from 1.03914 to 0.71168, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: val_loss improved from 0.71168 to 0.36018, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: val_loss improved from 0.36018 to 0.22008, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5: val_loss improved from 0.22008 to 0.18130, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6: val_loss improved from 0.18130 to 0.16611, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7: val_loss improved from 0.16611 to 0.14674, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8: val_loss improved from 0.14674 to 0.13171, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9: val_loss improved from 0.13171 to 0.11219, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10: val_loss improved from 0.11219 to 0.09802, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11: val_loss improved from 0.09802 to 0.08010, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12: val_loss improved from 0.08010 to 0.05970, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13: val_loss improved from 0.05970 to 0.04471, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14: val_loss improved from 0.04471 to 0.03505, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15: val_loss improved from 0.03505 to 0.02858, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16: val_loss improved from 0.02858 to 0.02410, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17: val_loss improved from 0.02410 to 0.02100, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18: val_loss improved from 0.02100 to 0.01910, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19: val_loss improved from 0.01910 to 0.01770, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20: val_loss improved from 0.01770 to 0.01674, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21: val_loss improved from 0.01674 to 0.01602, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22: val_loss improved from 0.01602 to 0.01545, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23: val_loss improved from 0.01545 to 0.01504, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24: val_loss improved from 0.01504 to 0.01466, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25: val_loss improved from 0.01466 to 0.01433, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26: val_loss improved from 0.01433 to 0.01404, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27: val_loss improved from 0.01404 to 0.01379, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28: val_loss improved from 0.01379 to 0.01356, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29: val_loss improved from 0.01356 to 0.01332, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30: val_loss improved from 0.01332 to 0.01314, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31: val_loss improved from 0.01314 to 0.01301, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32: val_loss improved from 0.01301 to 0.01274, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33: val_loss improved from 0.01274 to 0.01249, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34: val_loss improved from 0.01249 to 0.01224, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35: val_loss improved from 0.01224 to 0.01201, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36: val_loss improved from 0.01201 to 0.01176, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37: val_loss improved from 0.01176 to 0.01158, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38: val_loss improved from 0.01158 to 0.01144, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39: val_loss improved from 0.01144 to 0.01128, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40: val_loss improved from 0.01128 to 0.01116, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 41: val_loss improved from 0.01116 to 0.01104, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 42: val_loss improved from 0.01104 to 0.01091, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 43: val_loss improved from 0.01091 to 0.01081, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44: val_loss improved from 0.01081 to 0.01069, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 45: val_loss improved from 0.01069 to 0.01063, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46: val_loss improved from 0.01063 to 0.01052, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 47: val_loss improved from 0.01052 to 0.01046, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48: val_loss improved from 0.01046 to 0.01036, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 49: val_loss improved from 0.01036 to 0.01031, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 50: val_loss improved from 0.01031 to 0.01021, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 51: val_loss improved from 0.01021 to 0.01015, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 52: val_loss improved from 0.01015 to 0.01006, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 53: val_loss improved from 0.01006 to 0.01000, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 54: val_loss did not improve from 0.01000\n",
      "\n",
      "Epoch 55: val_loss improved from 0.01000 to 0.00980, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 56: val_loss improved from 0.00980 to 0.00973, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 57: val_loss improved from 0.00973 to 0.00965, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 58: val_loss improved from 0.00965 to 0.00957, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 59: val_loss improved from 0.00957 to 0.00948, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 60: val_loss improved from 0.00948 to 0.00942, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 61: val_loss improved from 0.00942 to 0.00933, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 62: val_loss improved from 0.00933 to 0.00927, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 63: val_loss improved from 0.00927 to 0.00918, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 64: val_loss improved from 0.00918 to 0.00913, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 65: val_loss improved from 0.00913 to 0.00905, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 66: val_loss improved from 0.00905 to 0.00898, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 67: val_loss improved from 0.00898 to 0.00890, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 68: val_loss improved from 0.00890 to 0.00883, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 69: val_loss improved from 0.00883 to 0.00875, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 70: val_loss improved from 0.00875 to 0.00867, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 71: val_loss improved from 0.00867 to 0.00859, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 72: val_loss improved from 0.00859 to 0.00851, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 73: val_loss improved from 0.00851 to 0.00843, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 74: val_loss improved from 0.00843 to 0.00834, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 75: val_loss improved from 0.00834 to 0.00826, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 76: val_loss improved from 0.00826 to 0.00819, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 77: val_loss improved from 0.00819 to 0.00813, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 78: val_loss improved from 0.00813 to 0.00808, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 79: val_loss improved from 0.00808 to 0.00798, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 80: val_loss improved from 0.00798 to 0.00791, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 81: val_loss improved from 0.00791 to 0.00783, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 82: val_loss improved from 0.00783 to 0.00774, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 83: val_loss improved from 0.00774 to 0.00766, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 84: val_loss improved from 0.00766 to 0.00756, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 85: val_loss improved from 0.00756 to 0.00747, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 86: val_loss improved from 0.00747 to 0.00738, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 87: val_loss improved from 0.00738 to 0.00729, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 88: val_loss improved from 0.00729 to 0.00720, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 89: val_loss improved from 0.00720 to 0.00710, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 90: val_loss improved from 0.00710 to 0.00701, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 91: val_loss improved from 0.00701 to 0.00692, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 92: val_loss improved from 0.00692 to 0.00682, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 93: val_loss improved from 0.00682 to 0.00672, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 94: val_loss improved from 0.00672 to 0.00664, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 95: val_loss improved from 0.00664 to 0.00654, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 96: val_loss improved from 0.00654 to 0.00646, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 97: val_loss improved from 0.00646 to 0.00636, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 98: val_loss improved from 0.00636 to 0.00628, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 99: val_loss improved from 0.00628 to 0.00619, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 100: val_loss improved from 0.00619 to 0.00610, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 101: val_loss improved from 0.00610 to 0.00601, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 102: val_loss improved from 0.00601 to 0.00592, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 103: val_loss improved from 0.00592 to 0.00583, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 104: val_loss improved from 0.00583 to 0.00575, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 105: val_loss improved from 0.00575 to 0.00566, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 106: val_loss improved from 0.00566 to 0.00558, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 107: val_loss improved from 0.00558 to 0.00549, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 108: val_loss improved from 0.00549 to 0.00541, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 109: val_loss improved from 0.00541 to 0.00532, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 110: val_loss improved from 0.00532 to 0.00523, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 111: val_loss improved from 0.00523 to 0.00514, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 112: val_loss improved from 0.00514 to 0.00505, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 113: val_loss improved from 0.00505 to 0.00496, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 114: val_loss improved from 0.00496 to 0.00486, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 115: val_loss improved from 0.00486 to 0.00478, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 116: val_loss improved from 0.00478 to 0.00469, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 117: val_loss improved from 0.00469 to 0.00461, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 118: val_loss improved from 0.00461 to 0.00452, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 119: val_loss improved from 0.00452 to 0.00445, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 120: val_loss improved from 0.00445 to 0.00436, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 121: val_loss improved from 0.00436 to 0.00429, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 122: val_loss improved from 0.00429 to 0.00422, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 123: val_loss improved from 0.00422 to 0.00415, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 124: val_loss improved from 0.00415 to 0.00408, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 125: val_loss improved from 0.00408 to 0.00401, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 126: val_loss improved from 0.00401 to 0.00394, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 127: val_loss improved from 0.00394 to 0.00388, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 128: val_loss improved from 0.00388 to 0.00382, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 129: val_loss improved from 0.00382 to 0.00375, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 130: val_loss improved from 0.00375 to 0.00369, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 131: val_loss improved from 0.00369 to 0.00363, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 132: val_loss improved from 0.00363 to 0.00357, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 133: val_loss improved from 0.00357 to 0.00351, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 134: val_loss improved from 0.00351 to 0.00345, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 135: val_loss improved from 0.00345 to 0.00339, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 136: val_loss improved from 0.00339 to 0.00333, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 137: val_loss improved from 0.00333 to 0.00327, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 138: val_loss improved from 0.00327 to 0.00321, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 139: val_loss improved from 0.00321 to 0.00315, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 140: val_loss improved from 0.00315 to 0.00309, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 141: val_loss improved from 0.00309 to 0.00302, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 142: val_loss improved from 0.00302 to 0.00296, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 143: val_loss improved from 0.00296 to 0.00290, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 144: val_loss improved from 0.00290 to 0.00284, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 145: val_loss improved from 0.00284 to 0.00278, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 146: val_loss improved from 0.00278 to 0.00273, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 147: val_loss improved from 0.00273 to 0.00267, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 148: val_loss improved from 0.00267 to 0.00261, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 149: val_loss improved from 0.00261 to 0.00255, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 150: val_loss improved from 0.00255 to 0.00250, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 151: val_loss improved from 0.00250 to 0.00245, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 152: val_loss improved from 0.00245 to 0.00240, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 153: val_loss improved from 0.00240 to 0.00236, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 154: val_loss improved from 0.00236 to 0.00231, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 155: val_loss improved from 0.00231 to 0.00227, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 156: val_loss improved from 0.00227 to 0.00223, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 157: val_loss improved from 0.00223 to 0.00219, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 158: val_loss improved from 0.00219 to 0.00215, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 159: val_loss improved from 0.00215 to 0.00211, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 160: val_loss improved from 0.00211 to 0.00208, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 161: val_loss improved from 0.00208 to 0.00204, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 162: val_loss improved from 0.00204 to 0.00200, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 163: val_loss improved from 0.00200 to 0.00197, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 164: val_loss improved from 0.00197 to 0.00193, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 165: val_loss improved from 0.00193 to 0.00190, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 166: val_loss improved from 0.00190 to 0.00187, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 167: val_loss improved from 0.00187 to 0.00183, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 168: val_loss improved from 0.00183 to 0.00180, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 169: val_loss improved from 0.00180 to 0.00177, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 170: val_loss improved from 0.00177 to 0.00174, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 171: val_loss improved from 0.00174 to 0.00170, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 172: val_loss improved from 0.00170 to 0.00166, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 173: val_loss improved from 0.00166 to 0.00163, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 174: val_loss improved from 0.00163 to 0.00160, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 175: val_loss improved from 0.00160 to 0.00156, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 176: val_loss improved from 0.00156 to 0.00153, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 177: val_loss improved from 0.00153 to 0.00150, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 178: val_loss improved from 0.00150 to 0.00146, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 179: val_loss improved from 0.00146 to 0.00142, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 180: val_loss improved from 0.00142 to 0.00139, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 181: val_loss improved from 0.00139 to 0.00136, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 182: val_loss improved from 0.00136 to 0.00133, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 183: val_loss improved from 0.00133 to 0.00130, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 184: val_loss improved from 0.00130 to 0.00127, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 185: val_loss improved from 0.00127 to 0.00124, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 186: val_loss improved from 0.00124 to 0.00121, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 187: val_loss improved from 0.00121 to 0.00119, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 188: val_loss improved from 0.00119 to 0.00116, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 189: val_loss improved from 0.00116 to 0.00114, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 190: val_loss improved from 0.00114 to 0.00112, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 191: val_loss improved from 0.00112 to 0.00110, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 192: val_loss improved from 0.00110 to 0.00109, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 193: val_loss improved from 0.00109 to 0.00107, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 194: val_loss improved from 0.00107 to 0.00106, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 195: val_loss improved from 0.00106 to 0.00104, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 196: val_loss improved from 0.00104 to 0.00103, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 197: val_loss improved from 0.00103 to 0.00102, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 198: val_loss improved from 0.00102 to 0.00100, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 199: val_loss improved from 0.00100 to 0.00100, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 200: val_loss improved from 0.00100 to 0.00099, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step\n",
      "   timesteps        hl      lr  batch_size  num_epochs       mse      rmse  \\\n",
      "0         50  [40, 35]  0.0001          32         200  0.000742  0.027243   \n",
      "\n",
      "       mape        r2  \n",
      "0  0.020988  0.944013  \n",
      "Best parameters: {'timesteps': 30, 'hl': [40, 35], 'lr': 0.0001, 'batch_size': 32, 'num_epochs': 200}\n",
      "Best RMSE score: 0.02724254719812192\n"
     ]
    }
   ],
   "source": [
    "best_params, best_score, results_df = grid_search_rnn(train, val, test, param_grid)\n",
    "\n",
    "print(results_df)\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best RMSE score:\", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 5: Hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi b·ªô tham s·ªë t·ªëi ∆∞u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00099\n",
      "\n",
      "Epoch 59: val_loss improved from 0.00099 to 0.00098, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 60: val_loss improved from 0.00098 to 0.00097, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 61: val_loss improved from 0.00097 to 0.00096, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 62: val_loss improved from 0.00096 to 0.00095, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 63: val_loss improved from 0.00095 to 0.00094, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 64: val_loss improved from 0.00094 to 0.00094, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 65: val_loss improved from 0.00094 to 0.00094, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 66: val_loss improved from 0.00094 to 0.00094, saving model to 10Var-Vn30-gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 67: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.00094\n",
      "Epoch 137: early stopping\n"
     ]
    }
   ],
   "source": [
    "timesteps = 30\n",
    "hl = [40, 35]\n",
    "lr = 1e-4\n",
    "batch_size = 32\n",
    "num_epochs = 200\n",
    "\n",
    "model, train_loss, val_loss = fit_model_3(train, val, timesteps, hl, lr, batch_size, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 6: ƒê√°nh gi√° m√¥ h√¨nh v√† tr·ª±c quan h√≥a k·∫øt qu·∫£"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1. V·∫Ω bi·ªÉu ƒë·ªì train_loss v√† val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_error(train_error, val_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2. ƒê√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step\n"
     ]
    }
   ],
   "source": [
    "mse, rmse, mape, r2, true, predicted = evaluate_model_3(model, test, timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.0007688355691376265\n",
      "RMSE = 0.027727884324946727\n",
      "MAPE = 0.02152507816561476\n",
      "R-Squared Score = 0.9445823227225246\n"
     ]
    }
   ],
   "source": [
    "print('MSE = {}'.format(mse))\n",
    "print('RMSE = {}'.format(rmse))\n",
    "print('MAPE = {}'.format(mape))\n",
    "print('R-Squared Score = {}'.format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3. V·∫Ω ƒë·ªì th·ªã d·ª± ƒëo√°n vs th·ª±c t·∫ø"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data_3(true, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a model\n",
    "model.save('10VAR-VN30index-gru.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: Multivariate-3-LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0' # ƒë·∫£m b·∫£o r·∫±ng c√°c gi√° tr·ªã bƒÉm c·ªßa ƒë·ªëi t∆∞·ª£ng b·∫•t bi·∫øn (dict, set, chu·ªói, tuple...) lu√¥n gi·ªëng nhau gi·ªØa c√°c l·∫ßn ch·∫°y\n",
    "\n",
    "import random as rn\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "rn.seed(3)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from math import sqrt\n",
    "import datetime as dt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H√†m callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=80,  verbose=1, mode='min')  \n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=\"10Var_vn30_lstm.h5\",   # T√™n file l∆∞u m√¥ h√¨nh\n",
    "    monitor=\"val_loss\",         # Theo d√µi val_loss\n",
    "    save_best_only=True,        # Ch·ªâ l∆∞u khi t·ªët h∆°n m√¥ h√¨nh tr∆∞·ªõc ƒë√≥\n",
    "    mode=\"min\",                 # Gi·∫£m min c·ªßa val_loss l√† t·ªët nh·∫•t\n",
    "    verbose=1\n",
    ")\n",
    "callbacks_list = [earlystop, checkpoint] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ƒê·ªçc d·ªØ li·ªáu t·ª´ file csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = r\"VN30 Index.csv\"\n",
    "df = pd.read_csv(url, parse_dates= True, index_col= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009-01-05</th>\n",
       "      <td>311.23</td>\n",
       "      <td>311.23</td>\n",
       "      <td>311.23</td>\n",
       "      <td>311.23</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-06</th>\n",
       "      <td>314.21</td>\n",
       "      <td>314.21</td>\n",
       "      <td>314.21</td>\n",
       "      <td>314.21</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-07</th>\n",
       "      <td>320.53</td>\n",
       "      <td>320.53</td>\n",
       "      <td>320.53</td>\n",
       "      <td>320.53</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-08</th>\n",
       "      <td>314.14</td>\n",
       "      <td>314.14</td>\n",
       "      <td>314.14</td>\n",
       "      <td>314.14</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-09</th>\n",
       "      <td>312.90</td>\n",
       "      <td>312.90</td>\n",
       "      <td>312.90</td>\n",
       "      <td>312.90</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              open    high     low   close  volume\n",
       "time                                              \n",
       "2009-01-05  311.23  311.23  311.23  311.23     NaN\n",
       "2009-01-06  314.21  314.21  314.21  314.21     NaN\n",
       "2009-01-07  320.53  320.53  320.53  320.53     NaN\n",
       "2009-01-08  314.14  314.14  314.14  314.14     NaN\n",
       "2009-01-09  312.90  312.90  312.90  312.90     NaN"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "open        0\n",
       "high        0\n",
       "low         0\n",
       "close       0\n",
       "volume    859\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ti·ªÅn x·ª≠ l√≠ d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X√≥a c√°c d√≤ng c√≥ gi√° tr·ªã Volume b·∫±ng 0\n",
    "df.drop(df[df['volume']==0].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "open      0.999562\n",
       "high      0.999783\n",
       "low       0.999789\n",
       "close     1.000000\n",
       "volume    0.169492\n",
       "Name: close, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ma tr·∫≠n t∆∞∆°ng quan (·ªü ƒë√¢y l√† Pearson t∆∞∆°ng quan tuy·∫øn t√≠nh)\n",
    "df.corr()['close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    3.185000e+03\n",
      "mean     3.279171e+06\n",
      "std      2.776577e+07\n",
      "min      4.540000e+03\n",
      "25%      3.809000e+04\n",
      "50%      6.036000e+04\n",
      "75%      1.560100e+05\n",
      "max      3.744900e+08\n",
      "Name: volume, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(df.describe().volume) \n",
    "df['volume'].hist(bins= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B·ªï sung c√°c ch·ªâ b√°o kƒ© thu·∫≠t\n",
    "\n",
    "# T√≠nh CMA10\n",
    "df['CMA10'] = df['close'].rolling(window=10, center=True).mean()\n",
    "# T√≠nh SMA10\n",
    "df['SMA10'] = df['close'].rolling(window=10).mean()\n",
    "# T√≠nh SMA50\n",
    "df['SMA50'] = df['close'].rolling(window=50).mean()\n",
    "# T√≠nh EMA12 v√† EMA26\n",
    "df['EMA12'] = df['close'].ewm(span=12, adjust=False).mean()\n",
    "df['EMA26'] = df['close'].ewm(span=26, adjust=False).mean()\n",
    "# T√≠nh MACD\n",
    "df['MACD'] = df['EMA12'] - df['EMA26']\n",
    "#T√≠nh RSI\n",
    "# T√≠nh gi√° tƒÉng/gi·∫£m\n",
    "delta = df['close'].diff()\n",
    "\n",
    "# T√≠nh gi√° tƒÉng\n",
    "gain = delta.where(delta > 0, 0)\n",
    "\n",
    "# T√≠nh gi√° gi·∫£m\n",
    "loss = -delta.where(delta < 0, 0)\n",
    "\n",
    "# T√≠nh trung b√¨nh ƒë·ªông\n",
    "avg_gain = gain.rolling(window=14).mean()\n",
    "avg_loss = loss.rolling(window=14).mean()\n",
    "\n",
    "# T√≠nh RS v√† RSI\n",
    "rs = avg_gain / avg_loss\n",
    "df['RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "#T√≠nh CCI\n",
    "# T√≠nh gi√° trung b√¨nh\n",
    "typical_price = (df['high'] + df['low'] + df['close']) / 3\n",
    "\n",
    "# T√≠nh SMA c·ªßa gi√° trung b√¨nh\n",
    "sma_typical_price = typical_price.rolling(window=20).mean()\n",
    "\n",
    "# T√≠nh ƒë·ªô l·ªách chu·∫©n\n",
    "mean_deviation = typical_price.rolling(window=20).apply(lambda x: np.mean(np.abs(x - x.mean())))\n",
    "\n",
    "# T√≠nh CCI\n",
    "df['CCI'] = (typical_price - sma_typical_price) / (0.015 * mean_deviation)\n",
    "# T√≠nh %K v√† %D\n",
    "low_min = df['low'].rolling(window=14).min()\n",
    "high_max = df['high'].rolling(window=14).max()\n",
    "\n",
    "df['%K'] = 100 * (df['close'] - low_min) / (high_max - low_min)\n",
    "df['%D'] = df['%K'].rolling(window=3).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              open    high     low   close  volume  CMA10  SMA10  SMA50  \\\n",
      "time                                                                      \n",
      "2009-01-05  311.23  311.23  311.23  311.23     NaN    NaN    NaN    NaN   \n",
      "2009-01-06  314.21  314.21  314.21  314.21     NaN    NaN    NaN    NaN   \n",
      "2009-01-07  320.53  320.53  320.53  320.53     NaN    NaN    NaN    NaN   \n",
      "2009-01-08  314.14  314.14  314.14  314.14     NaN    NaN    NaN    NaN   \n",
      "2009-01-09  312.90  312.90  312.90  312.90     NaN    NaN    NaN    NaN   \n",
      "\n",
      "                 EMA12       EMA26      MACD  RSI  CCI  %K  %D  \n",
      "time                                                            \n",
      "2009-01-05  311.230000  311.230000  0.000000  NaN  NaN NaN NaN  \n",
      "2009-01-06  311.688462  311.450741  0.237721  NaN  NaN NaN NaN  \n",
      "2009-01-07  313.048698  312.123278  0.925420  NaN  NaN NaN NaN  \n",
      "2009-01-08  313.216591  312.272665  0.943926  NaN  NaN NaN NaN  \n",
      "2009-01-09  313.167885  312.319134  0.848750  NaN  NaN NaN NaN  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4044, 15)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.head())\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model / H√†m **fit_model_4()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_4(train, val, timesteps, hl, lr, batch, epochs):\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    X_val = []\n",
    "    Y_val = []\n",
    "\n",
    "    for i in range(timesteps, train.shape[0]):\n",
    "        X_train.append(train[i-timesteps:i])\n",
    "        Y_train.append(train[i][0])\n",
    "    X_train, Y_train = np.array(X_train), np.array(Y_train)\n",
    "\n",
    "    for i in range(timesteps, val.shape[0]):\n",
    "        X_val.append(val[i-timesteps:i])\n",
    "        Y_val.append(val[i][0])\n",
    "    X_val, Y_val = np.array(X_val), np.array(Y_val)\n",
    "\n",
    "    # X√¢y d·ª±ng m√¥ h√¨nh\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(X_train.shape[2], input_shape= (X_train.shape[1], X_train.shape[2]), activation='relu', return_sequences= True))\n",
    "    for i in range(len(hl)-1):\n",
    "        model.add(LSTM(hl[i], activation='relu', return_sequences= True))\n",
    "    model.add(LSTM(hl[-1], activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Bi√™n d·ªãch\n",
    "    model.compile(optimizer= optimizers.Adam(learning_rate= lr), loss= 'mean_squared_error')\n",
    "\n",
    "    # Hu·∫•n luy·ªán m√¥ h√¨nh\n",
    "    history = model.fit(X_train, Y_train, batch_size= batch, epochs= epochs, validation_data= (X_val, Y_val), verbose= 0, shuffle= False, callbacks= callbacks_list)\n",
    "\n",
    "    # ƒê·∫∑t l·∫°i tr·∫°ng th√°i\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, LSTM):\n",
    "            layer.reset_state()\n",
    "\n",
    "    return model, history.history['loss'], history.history['val_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H√†m **Evaluate_model_4()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_4(model, test, timesteps):\n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "\n",
    "    for i in range(timesteps, test.shape[0]):\n",
    "        X_test.append(test[i-timesteps:i])\n",
    "        Y_test.append(test[i][0])\n",
    "    X_test, Y_test = np.array(X_test), np.array(Y_test)\n",
    "\n",
    "    # C√°c ch·ªâ s·ªë\n",
    "    Y_hat = model.predict(X_test)\n",
    "    mse = mean_squared_error(Y_test, Y_hat)\n",
    "    rmse = sqrt(mse)\n",
    "    mape = mean_absolute_percentage_error(Y_test, Y_hat)\n",
    "    r2 = r2_score(Y_test, Y_hat)\n",
    "\n",
    "    return mse, rmse, mape, r2, Y_test, Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Grid Search**: T√¨m ki·∫øm si√™u tham s·ªë t·ªëi ∆∞u "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'timesteps': [30, 40, 50],  # S·ªë gi√° tr·ªã tr∆∞·ªõc ƒë√≥ ƒë·ªÉ d·ª± ƒëo√°n\n",
    "    'hl': [ [40, 35]], # C·∫•u tr√∫c l·ªõp ·∫©n\n",
    "    'lr': [1e-3, 1e-4],  # T·ªëc ƒë·ªô h·ªçc\n",
    "    'batch_size': [32, 64],  # K√≠ch th∆∞·ªõc batch\n",
    "    'num_epochs': [200, 250],  # S·ªë epoch\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import pandas as pd\n",
    "\n",
    "# H√†m Grid Search\n",
    "def grid_search_rnn(train, val, test, param_grid):\n",
    "    results = []  # L∆∞u k·∫øt qu·∫£ c·ªßa t·ª´ng t·ªï h·ª£p tham s·ªë\n",
    "    best_score = float('inf')  # L∆∞u RMSE t·ªët nh·∫•t\n",
    "    best_params = None  # L∆∞u b·ªô tham s·ªë t·ªët nh·∫•t\n",
    "\n",
    "    # T·∫°o t·∫•t c·∫£ c√°c t·ªï h·ª£p tham s·ªë\n",
    "    all_combinations = list(product(*param_grid.values()))\n",
    "    param_names = list(param_grid.keys())\n",
    "\n",
    "    for combination in all_combinations:\n",
    "        # G√°n gi√° tr·ªã tham s·ªë hi·ªán t·∫°i\n",
    "        params = dict(zip(param_names, combination))\n",
    "        timesteps = params['timesteps']\n",
    "        hl = params['hl']\n",
    "        lr = params['lr']\n",
    "        batch_size = params['batch_size']\n",
    "        num_epochs = params['num_epochs']\n",
    "\n",
    "        print(f\"Training with params: {params}\")\n",
    "\n",
    "        # Hu·∫•n luy·ªán m√¥ h√¨nh\n",
    "        model, train_loss, val_loss = fit_model_4(\n",
    "            train, val, timesteps, hl, lr, batch_size, num_epochs\n",
    "        )\n",
    "\n",
    "        # ƒê√°nh gi√° m√¥ h√¨nh\n",
    "        mse, rmse, mape, r2, _, _ = evaluate_model_4(model, test, timesteps)\n",
    "\n",
    "        # L∆∞u k·∫øt qu·∫£\n",
    "        results.append({\n",
    "            'timesteps': timesteps,\n",
    "            'hl': hl,\n",
    "            'lr': lr,\n",
    "            'batch_size': batch_size,\n",
    "            'num_epochs': num_epochs,\n",
    "            'MSE': mse,\n",
    "            'RMSE': rmse,\n",
    "            'MAPE': mape,\n",
    "            'R¬≤': r2\n",
    "        })\n",
    "\n",
    "        # C·∫≠p nh·∫≠t tham s·ªë t·ªët nh·∫•t n·∫øu RMSE c·∫£i thi·ªán\n",
    "        if rmse < best_score:\n",
    "            best_score = rmse\n",
    "            best_params = params\n",
    "\n",
    "    # Tr·∫£ v·ªÅ k·∫øt qu·∫£\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return best_params, best_score, results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot chart (v·∫Ω bi·ªÉu ƒë·ªì)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the predictions\n",
    "def plot_data_4(Y_test, Y_hat):\n",
    "    plt.plot(Y_test, c = 'r')\n",
    "    plt.plot(Y_hat, c = 'y')\n",
    "    plt.xlabel('Day')\n",
    "    plt.ylabel('Price')\n",
    "    plt.title('Stock Prediction Graph using Multivariate-LSTM model')\n",
    "    plt.legend(['Actual', 'Predicted'], loc = 'lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training errors: tr·ª±c quan loss qua c√°c epoch -> th·∫•y qtr h·ªçc m√¥ h√¨nh, xem c√≥ overfitting ko\n",
    "def plot_error(train_loss, val_loss):\n",
    "    plt.plot(train_loss, c = 'r')\n",
    "    plt.plot(val_loss, c = 'b')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.title('Train Loss and Validation Loss Curve')\n",
    "    plt.legend(['train', 'val'], loc = 'upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model building**: X√¢y d·ª±ng m√¥ h√¨nh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 1: Tr√≠ch xu·∫•t v√† tr·ª±c quan h√≥a d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3991, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>CMA10</th>\n",
       "      <th>SMA10</th>\n",
       "      <th>SMA50</th>\n",
       "      <th>EMA12</th>\n",
       "      <th>RSI</th>\n",
       "      <th>CCI</th>\n",
       "      <th>%K</th>\n",
       "      <th>%D</th>\n",
       "      <th>MACD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3991.000000</td>\n",
       "      <td>3991.000000</td>\n",
       "      <td>3991.00000</td>\n",
       "      <td>3991.000000</td>\n",
       "      <td>3991.000000</td>\n",
       "      <td>3991.000000</td>\n",
       "      <td>3991.000000</td>\n",
       "      <td>3991.000000</td>\n",
       "      <td>3991.000000</td>\n",
       "      <td>3991.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>814.145743</td>\n",
       "      <td>814.003191</td>\n",
       "      <td>812.86794</td>\n",
       "      <td>807.414210</td>\n",
       "      <td>812.591967</td>\n",
       "      <td>54.032542</td>\n",
       "      <td>17.593878</td>\n",
       "      <td>57.339202</td>\n",
       "      <td>57.342557</td>\n",
       "      <td>1.921103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>321.109578</td>\n",
       "      <td>320.781191</td>\n",
       "      <td>320.75575</td>\n",
       "      <td>319.492655</td>\n",
       "      <td>320.500740</td>\n",
       "      <td>18.541935</td>\n",
       "      <td>110.243542</td>\n",
       "      <td>32.341940</td>\n",
       "      <td>30.423710</td>\n",
       "      <td>13.176084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>254.820000</td>\n",
       "      <td>261.983000</td>\n",
       "      <td>251.40000</td>\n",
       "      <td>265.341000</td>\n",
       "      <td>252.982926</td>\n",
       "      <td>4.401922</td>\n",
       "      <td>-339.712898</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-60.553670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>552.290000</td>\n",
       "      <td>551.247500</td>\n",
       "      <td>550.82900</td>\n",
       "      <td>547.833600</td>\n",
       "      <td>549.926815</td>\n",
       "      <td>40.285656</td>\n",
       "      <td>-70.241516</td>\n",
       "      <td>28.668093</td>\n",
       "      <td>28.989233</td>\n",
       "      <td>-4.653577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>673.530000</td>\n",
       "      <td>672.566000</td>\n",
       "      <td>671.65700</td>\n",
       "      <td>656.734400</td>\n",
       "      <td>669.526149</td>\n",
       "      <td>54.411293</td>\n",
       "      <td>32.984730</td>\n",
       "      <td>62.714014</td>\n",
       "      <td>62.491154</td>\n",
       "      <td>1.775423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1056.925000</td>\n",
       "      <td>1058.503000</td>\n",
       "      <td>1057.24600</td>\n",
       "      <td>1057.393900</td>\n",
       "      <td>1058.727874</td>\n",
       "      <td>67.773165</td>\n",
       "      <td>106.763138</td>\n",
       "      <td>87.016932</td>\n",
       "      <td>85.932690</td>\n",
       "      <td>9.299267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1572.460000</td>\n",
       "      <td>1543.976000</td>\n",
       "      <td>1543.97600</td>\n",
       "      <td>1524.849800</td>\n",
       "      <td>1541.505267</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>314.013206</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>49.008428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             close        CMA10       SMA10        SMA50        EMA12  \\\n",
       "count  3991.000000  3991.000000  3991.00000  3991.000000  3991.000000   \n",
       "mean    814.145743   814.003191   812.86794   807.414210   812.591967   \n",
       "std     321.109578   320.781191   320.75575   319.492655   320.500740   \n",
       "min     254.820000   261.983000   251.40000   265.341000   252.982926   \n",
       "25%     552.290000   551.247500   550.82900   547.833600   549.926815   \n",
       "50%     673.530000   672.566000   671.65700   656.734400   669.526149   \n",
       "75%    1056.925000  1058.503000  1057.24600  1057.393900  1058.727874   \n",
       "max    1572.460000  1543.976000  1543.97600  1524.849800  1541.505267   \n",
       "\n",
       "               RSI          CCI           %K           %D         MACD  \n",
       "count  3991.000000  3991.000000  3991.000000  3991.000000  3991.000000  \n",
       "mean     54.032542    17.593878    57.339202    57.342557     1.921103  \n",
       "std      18.541935   110.243542    32.341940    30.423710    13.176084  \n",
       "min       4.401922  -339.712898     0.000000     0.000000   -60.553670  \n",
       "25%      40.285656   -70.241516    28.668093    28.989233    -4.653577  \n",
       "50%      54.411293    32.984730    62.714014    62.491154     1.775423  \n",
       "75%      67.773165   106.763138    87.016932    85.932690     9.299267  \n",
       "max     100.000000   314.013206   100.000000   100.000000    49.008428  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting the series\n",
    "series = df[['close', 'CMA10', 'SMA10', 'SMA50', 'EMA12', 'RSI', 'CCI', '%K', '%D', 'MACD']]\n",
    "# Drop rows with NaN values\n",
    "series = series.dropna()\n",
    "\n",
    "# Display the shape and the tail of the cleaned series\n",
    "print(series.shape)\n",
    "series.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 2: Chia d·ªØ li·ªáu th√†nh c√°c t·∫≠p Train, Validation, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3991, 10)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2795, 10) (598, 10) (598, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n = series.shape[0]\n",
    "val_size =  test_size = int(n * 0.15)\n",
    "train_size = n - val_size - test_size # ƒê·ªÉ tr√°nh sai s·ªë l√†m m·∫•t d·ªØ li·ªáu\n",
    "\n",
    "# Chia t·∫≠p d·ªØ li·ªáu theo th·ª© t·ª± th·ªùi gian\n",
    "train_data = series.iloc[:train_size].values\n",
    "val_data = series.iloc[train_size:train_size + val_size].values\n",
    "test_data = series.iloc[(train_size + val_size):].values\n",
    "# Ki·ªÉm tra k√≠ch th∆∞·ªõc c·ªßa t·ª´ng t·∫≠p\n",
    "print(train_data.shape, val_data.shape, test_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 3: Chu·∫©n h√≥a d·ªØ li·ªáu b·∫±ng MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2795, 10) (598, 10) (598, 10)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "tf.random.set_seed(2)\n",
    "\n",
    "sc = MinMaxScaler()\n",
    "train = sc.fit_transform(train_data)\n",
    "val = sc.transform(val_data)\n",
    "test = sc.transform(test_data)\n",
    "\n",
    "print(train.shape, val.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 4: T√¨m si√™u tham s·ªë t·ªët nh·∫•t b·∫±ng Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with params: {'timesteps': 30, 'hl': [40, 35], 'lr': 0.001, 'batch_size': 32, 'num_epochs': 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 0.10977, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_loss did not improve from 0.10977\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.10977\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.10977\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.10977\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.10977\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.10977\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.10977\n",
      "\n",
      "Epoch 9: val_loss improved from 0.10977 to 0.10802, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10: val_loss did not improve from 0.10802\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.10802\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.10802\n",
      "\n",
      "Epoch 13: val_loss improved from 0.10802 to 0.05420, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14: val_loss did not improve from 0.05420\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.05420\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.05420\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.05420\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.05420\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.05420\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.05420\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.05420\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.05420\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.05420\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.05420\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.05420\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.05420\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.05420\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.05420\n",
      "\n",
      "Epoch 29: val_loss improved from 0.05420 to 0.05267, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30: val_loss improved from 0.05267 to 0.05080, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31: val_loss improved from 0.05080 to 0.04893, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32: val_loss improved from 0.04893 to 0.04716, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33: val_loss improved from 0.04716 to 0.04544, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34: val_loss improved from 0.04544 to 0.04382, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35: val_loss improved from 0.04382 to 0.04202, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36: val_loss improved from 0.04202 to 0.04050, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37: val_loss improved from 0.04050 to 0.03888, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38: val_loss improved from 0.03888 to 0.03735, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39: val_loss improved from 0.03735 to 0.03568, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40: val_loss improved from 0.03568 to 0.03437, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 41: val_loss improved from 0.03437 to 0.03251, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 42: val_loss improved from 0.03251 to 0.03216, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 43: val_loss improved from 0.03216 to 0.03036, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44: val_loss improved from 0.03036 to 0.03005, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 45: val_loss improved from 0.03005 to 0.02786, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46: val_loss improved from 0.02786 to 0.02771, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 47: val_loss improved from 0.02771 to 0.02505, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48: val_loss improved from 0.02505 to 0.02369, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 49: val_loss improved from 0.02369 to 0.02090, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 50: val_loss did not improve from 0.02090\n",
      "\n",
      "Epoch 51: val_loss improved from 0.02090 to 0.01883, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 52: val_loss did not improve from 0.01883\n",
      "\n",
      "Epoch 53: val_loss improved from 0.01883 to 0.01652, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 54: val_loss improved from 0.01652 to 0.01439, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 55: val_loss improved from 0.01439 to 0.01067, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 56: val_loss did not improve from 0.01067\n",
      "\n",
      "Epoch 57: val_loss improved from 0.01067 to 0.00871, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 58: val_loss improved from 0.00871 to 0.00860, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 59: val_loss improved from 0.00860 to 0.00560, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 60: val_loss improved from 0.00560 to 0.00535, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 61: val_loss improved from 0.00535 to 0.00354, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 62: val_loss improved from 0.00354 to 0.00149, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 63: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.00149\n",
      "Epoch 142: early stopping\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step\n",
      "Training with params: {'timesteps': 30, 'hl': [40, 35], 'lr': 0.001, 'batch_size': 32, 'num_epochs': 250}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.00149\n",
      "Epoch 131: early stopping\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step\n",
      "Training with params: {'timesteps': 30, 'hl': [40, 35], 'lr': 0.001, 'batch_size': 64, 'num_epochs': 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 51: val_loss improved from 0.00149 to 0.00100, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 52: val_loss did not improve from 0.00100\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00100\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00100\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00100\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00100\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00100\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00100\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00100\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00100\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00100\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00100\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00100\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00100\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00100\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00100\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00100\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00100\n",
      "\n",
      "Epoch 69: val_loss improved from 0.00100 to 0.00095, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 70: val_loss did not improve from 0.00095\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00095\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00095\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00095\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00095\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00095\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00095\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00095\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00095\n",
      "\n",
      "Epoch 79: val_loss improved from 0.00095 to 0.00093, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 80: val_loss did not improve from 0.00093\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00093\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00093\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00093\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00093\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00093\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00093\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00093\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.00093\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.00093\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.00093\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.00093\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.00093\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.00093\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.00093\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.00093\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.00093\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.00093\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.00093\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.00093\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.00093\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.00093\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.00093\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.00093\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.00093\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.00093\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.00093\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.00093\n",
      "\n",
      "Epoch 108: val_loss improved from 0.00093 to 0.00092, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 109: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.00092\n",
      "Epoch 131: early stopping\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step\n",
      "Training with params: {'timesteps': 30, 'hl': [40, 35], 'lr': 0.001, 'batch_size': 64, 'num_epochs': 250}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.00092\n",
      "Epoch 191: early stopping\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step\n",
      "Training with params: {'timesteps': 30, 'hl': [40, 35], 'lr': 0.0001, 'batch_size': 32, 'num_epochs': 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.00092\n",
      "Epoch 121: early stopping\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step\n",
      "Training with params: {'timesteps': 30, 'hl': [40, 35], 'lr': 0.0001, 'batch_size': 32, 'num_epochs': 250}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00092\n",
      "Epoch 87: early stopping\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step\n",
      "Training with params: {'timesteps': 30, 'hl': [40, 35], 'lr': 0.0001, 'batch_size': 64, 'num_epochs': 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.00092\n",
      "Epoch 177: early stopping\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step\n",
      "Training with params: {'timesteps': 30, 'hl': [40, 35], 'lr': 0.0001, 'batch_size': 64, 'num_epochs': 250}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.00092\n",
      "Epoch 143: early stopping\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step\n",
      "Training with params: {'timesteps': 40, 'hl': [40, 35], 'lr': 0.001, 'batch_size': 32, 'num_epochs': 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.00092\n",
      "Epoch 149: early stopping\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
      "Training with params: {'timesteps': 40, 'hl': [40, 35], 'lr': 0.001, 'batch_size': 32, 'num_epochs': 250}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.00092\n",
      "Epoch 136: early stopping\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step\n",
      "Training with params: {'timesteps': 40, 'hl': [40, 35], 'lr': 0.001, 'batch_size': 64, 'num_epochs': 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.00092\n",
      "Epoch 146: early stopping\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step\n",
      "Training with params: {'timesteps': 40, 'hl': [40, 35], 'lr': 0.001, 'batch_size': 64, 'num_epochs': 250}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 133: val_loss improved from 0.00092 to 0.00069, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 134: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.00069\n",
      "Epoch 213: early stopping\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step\n",
      "Training with params: {'timesteps': 40, 'hl': [40, 35], 'lr': 0.0001, 'batch_size': 32, 'num_epochs': 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.00069\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step\n",
      "Training with params: {'timesteps': 40, 'hl': [40, 35], 'lr': 0.0001, 'batch_size': 32, 'num_epochs': 250}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.00069\n",
      "Epoch 128: early stopping\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 225ms/step\n",
      "Training with params: {'timesteps': 40, 'hl': [40, 35], 'lr': 0.0001, 'batch_size': 64, 'num_epochs': 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.00069\n",
      "Epoch 89: early stopping\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step\n",
      "Training with params: {'timesteps': 40, 'hl': [40, 35], 'lr': 0.0001, 'batch_size': 64, 'num_epochs': 250}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.00069\n",
      "Epoch 190: early stopping\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
      "Training with params: {'timesteps': 50, 'hl': [40, 35], 'lr': 0.001, 'batch_size': 32, 'num_epochs': 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.00069\n",
      "Epoch 105: early stopping\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step\n",
      "Training with params: {'timesteps': 50, 'hl': [40, 35], 'lr': 0.001, 'batch_size': 32, 'num_epochs': 250}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.00069\n",
      "Epoch 102: early stopping\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step\n",
      "Training with params: {'timesteps': 50, 'hl': [40, 35], 'lr': 0.001, 'batch_size': 64, 'num_epochs': 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.00069\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step\n",
      "Training with params: {'timesteps': 50, 'hl': [40, 35], 'lr': 0.001, 'batch_size': 64, 'num_epochs': 250}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.00069\n",
      "Epoch 207: early stopping\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step\n",
      "Training with params: {'timesteps': 50, 'hl': [40, 35], 'lr': 0.0001, 'batch_size': 32, 'num_epochs': 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.00069\n",
      "Epoch 90: early stopping\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step\n",
      "Training with params: {'timesteps': 50, 'hl': [40, 35], 'lr': 0.0001, 'batch_size': 32, 'num_epochs': 250}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.00069\n",
      "Epoch 197: early stopping\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
      "Training with params: {'timesteps': 50, 'hl': [40, 35], 'lr': 0.0001, 'batch_size': 64, 'num_epochs': 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.00069\n",
      "Epoch 132: early stopping\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step\n",
      "Training with params: {'timesteps': 50, 'hl': [40, 35], 'lr': 0.0001, 'batch_size': 64, 'num_epochs': 250}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.00069\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.00069\n",
      "Epoch 193: early stopping\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
      "    timesteps        hl      lr  batch_size  num_epochs       MSE      RMSE  \\\n",
      "0          30  [40, 35]  0.0010          32         200  0.030048  0.173345   \n",
      "1          30  [40, 35]  0.0010          32         250  0.124565  0.352937   \n",
      "2          30  [40, 35]  0.0010          64         200  0.010141  0.100702   \n",
      "3          30  [40, 35]  0.0010          64         250  0.003375  0.058097   \n",
      "4          30  [40, 35]  0.0001          32         200  0.039918  0.199794   \n",
      "5          30  [40, 35]  0.0001          32         250  0.034624  0.186076   \n",
      "6          30  [40, 35]  0.0001          64         200  0.002423  0.049228   \n",
      "7          30  [40, 35]  0.0001          64         250  0.005129  0.071620   \n",
      "8          40  [40, 35]  0.0010          32         200  0.005148  0.071752   \n",
      "9          40  [40, 35]  0.0010          32         250  0.000925  0.030412   \n",
      "10         40  [40, 35]  0.0010          64         200  0.033684  0.183532   \n",
      "11         40  [40, 35]  0.0010          64         250  0.007001  0.083669   \n",
      "12         40  [40, 35]  0.0001          32         200  0.002783  0.052755   \n",
      "13         40  [40, 35]  0.0001          32         250  0.018443  0.135804   \n",
      "14         40  [40, 35]  0.0001          64         200  0.004995  0.070673   \n",
      "15         40  [40, 35]  0.0001          64         250  0.064404  0.253779   \n",
      "16         50  [40, 35]  0.0010          32         200  0.035031  0.187166   \n",
      "17         50  [40, 35]  0.0010          32         250  0.030464  0.174538   \n",
      "18         50  [40, 35]  0.0010          64         200  0.041946  0.204808   \n",
      "19         50  [40, 35]  0.0010          64         250  0.032646  0.180683   \n",
      "20         50  [40, 35]  0.0001          32         200  0.010574  0.102831   \n",
      "21         50  [40, 35]  0.0001          32         250  0.001797  0.042393   \n",
      "22         50  [40, 35]  0.0001          64         200  0.007403  0.086039   \n",
      "23         50  [40, 35]  0.0001          64         250  0.004890  0.069928   \n",
      "\n",
      "        MAPE        R¬≤  \n",
      "0   0.113561 -1.165887  \n",
      "1   0.232811 -7.978632  \n",
      "2   0.082616  0.269050  \n",
      "3   0.047808  0.756708  \n",
      "4   0.089024 -1.877255  \n",
      "5   0.129646 -1.495706  \n",
      "6   0.038537  0.825324  \n",
      "7   0.049520  0.630271  \n",
      "8   0.050005  0.623689  \n",
      "9   0.024743  0.932399  \n",
      "10  0.144588 -1.462057  \n",
      "11  0.055717  0.488308  \n",
      "12  0.044155  0.796579  \n",
      "13  0.107018 -0.348041  \n",
      "14  0.054611  0.634925  \n",
      "15  0.196795 -3.707463  \n",
      "16  0.129088 -1.642694  \n",
      "17  0.149324 -1.298134  \n",
      "18  0.139992 -2.164375  \n",
      "19  0.149018 -1.462790  \n",
      "20  0.060777  0.202291  \n",
      "21  0.031629  0.864421  \n",
      "22  0.066517  0.441550  \n",
      "23  0.045430  0.631115  \n",
      "Best parameters: {'timesteps': 40, 'hl': [40, 35], 'lr': 0.001, 'batch_size': 32, 'num_epochs': 250}\n",
      "Best RMSE score: 0.030411502531739045\n"
     ]
    }
   ],
   "source": [
    "best_params, best_score, results_df = grid_search_rnn(train, val, test, param_grid)\n",
    "\n",
    "print(results_df)\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best RMSE score:\", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 5: Hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi b·ªô tham s·ªë t·ªëi ∆∞u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\84368\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 0.12801, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_loss improved from 0.12801 to 0.08628, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: val_loss did not improve from 0.08628\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.08628\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.08628\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.08628\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.08628\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.08628\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.08628\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.08628\n",
      "\n",
      "Epoch 11: val_loss improved from 0.08628 to 0.07785, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12: val_loss did not improve from 0.07785\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.07785\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.07785\n",
      "\n",
      "Epoch 15: val_loss improved from 0.07785 to 0.03631, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16: val_loss did not improve from 0.03631\n",
      "\n",
      "Epoch 17: val_loss improved from 0.03631 to 0.03254, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18: val_loss did not improve from 0.03254\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.03254\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.03254\n",
      "\n",
      "Epoch 21: val_loss improved from 0.03254 to 0.02539, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22: val_loss improved from 0.02539 to 0.01737, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23: val_loss improved from 0.01737 to 0.01052, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24: val_loss improved from 0.01052 to 0.00935, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25: val_loss improved from 0.00935 to 0.00766, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26: val_loss improved from 0.00766 to 0.00712, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27: val_loss improved from 0.00712 to 0.00600, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28: val_loss improved from 0.00600 to 0.00563, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29: val_loss improved from 0.00563 to 0.00522, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30: val_loss improved from 0.00522 to 0.00502, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31: val_loss improved from 0.00502 to 0.00478, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32: val_loss improved from 0.00478 to 0.00460, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33: val_loss improved from 0.00460 to 0.00439, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34: val_loss improved from 0.00439 to 0.00434, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35: val_loss improved from 0.00434 to 0.00416, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36: val_loss improved from 0.00416 to 0.00406, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37: val_loss improved from 0.00406 to 0.00393, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38: val_loss did not improve from 0.00393\n",
      "\n",
      "Epoch 39: val_loss improved from 0.00393 to 0.00386, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 100: val_loss improved from 0.00386 to 0.00253, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 101: val_loss improved from 0.00253 to 0.00140, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 102: val_loss improved from 0.00140 to 0.00138, saving model to 10Var_vn30_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 103: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.00138\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.00138\n",
      "Epoch 181: early stopping\n"
     ]
    }
   ],
   "source": [
    "timesteps = 40\n",
    "hl = [40, 35]\n",
    "lr = 1e-3\n",
    "batch_size = 32\n",
    "num_epochs = 250\n",
    "\n",
    "model, train_error, val_error = fit_model_4(train, val, timesteps, hl, lr, batch_size, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 6: ƒê√°nh gi√° m√¥ h√¨nh v√† tr·ª±c quan h√≥a k·∫øt qu·∫£"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1. V·∫Ω bi·ªÉu ƒë·ªì train_loss v√† val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_error(train_error, val_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2. ƒê√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step\n"
     ]
    }
   ],
   "source": [
    "mse, rmse, mape, r2, true, predicted = evaluate_model_4(model, test, timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.0011348796054511474\n",
      "RMSE = 0.0336879741963085\n",
      "MAPE = 0.02675516819411632\n",
      "R-Squared Score = 0.9170480625833\n"
     ]
    }
   ],
   "source": [
    "print('MSE = {}'.format(mse))\n",
    "print('RMSE = {}'.format(rmse))\n",
    "print('MAPE = {}'.format(mape))\n",
    "print('R-Squared Score = {}'.format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3. V·∫Ω ƒë·ªì th·ªã d·ª± ƒëo√°n vs th·ª±c t·∫ø"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data_4(true, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a model\n",
    "model.save('10VAR-VN30index-lstm.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
